{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook created by [Sam Dauncey](https://disco.ethz.ch/members/sdauncey) for FS 2025. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Computer Vision\n",
    "\n",
    "\"Creating noise from data is easy; creating data from noise is generative modeling.\"\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\- [Yang Song et al.](https://openreview.net/pdf?id=PxTIG12RRHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DiffusionPipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/IML/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/IML/lib/python3.12/site-packages/torchvision/_meta_registrations.py:25\u001b[39m\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mroi_align\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/IML/lib/python3.12/site-packages/torchvision/_meta_registrations.py:18\u001b[39m, in \u001b[36mregister_meta.<locals>.wrapper\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(fn):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextension\u001b[49m._has_ops():\n\u001b[32m     19\u001b[39m         get_meta_lib().impl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch.ops.torchvision, op_name), overload_name), fn)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# Collapsed cell:imports\n",
    "import tqdm\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Poisson, Uniform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "import peft\n",
    "\n",
    "assert torch.cuda.is_available(), \"This notebook requires a GPU\"\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: helper functions to count the number of parameters in a torch.nn.Module\n",
    "def count_parameters(module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "\n",
    "def parameter_count_string(module):\n",
    "    n_params = count_parameters(module)\n",
    "    if n_params > 10**9:\n",
    "        return f\"{n_params/10**9:.1f}B\"\n",
    "    elif n_params > 10**6:\n",
    "        return f\"{n_params/10**6:.1f}M\"\n",
    "    elif n_params > 10**3:\n",
    "        return f\"{n_params/10**3:.1f}k\"\n",
    "    else:\n",
    "        return f\"{n_params}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The image generation problem\n",
    "\n",
    "Why care about AI image generation? Yes, it lets us create cool images, but generation also provides the strongest possible signal for _unsupervised learning_. To faithfully generate an image of a turtle with a city riding its shell the model must first have an internal world model where it understands what turtles and cities are. This differs from supervised learning: unless your dataset has sufficient counter-examples your model can acheive zero loss by learning spurious correlations such as \"wolf-labels appear in snowy backgrounds and dog-labels appear in grassy backgrounds\".\n",
    "\n",
    "![turtle with a city riding on its shell](https://polybox.ethz.ch/index.php/s/WDQPJr8etriqjyS/download/turtle_city_stable_diffusion3.png)\n",
    "\n",
    "A turtle with a city riding on its shell generated by [Stable Diffusion v3.5](https://stability.ai/stable-image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup: transposed convolutions\n",
    "\n",
    "A common theme in image generation is wanting to take some small grid and upsample it into a larger grid. The most common way to do this is using a transposed convolution operator, implemented by the `torch.nn.ConvTranspose2d` class and `torch.nn.functional.conv_transpose2d` function. This figure gives a nice depiction of a transposed convolution  with `kernel_size=3, stride=2, padding=1`:\n",
    "\n",
    "![Conv_transpose_example](https://polybox.ethz.ch/index.php/s/om9xqyE96p99pRX/download/conv_transpose_example.gif)\n",
    "\n",
    "Figure from [Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning](https://github.com/vdumoulin/conv_arithmetic/tree/master). \n",
    "\n",
    "The `stride = 2` parameter here acts to dialate the input, placing a `0` cell inbetween each pair of spatially adjacent input cells.\n",
    "\n",
    "See the below example for this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: how does a convolutional transpose behave if we fix its weights?\n",
    "\n",
    "my_conv_transpose = nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1, bias=False)\n",
    "filter = torch.arange(9, dtype=torch.float32).reshape(1, 1, 3, 3)\n",
    "my_conv_transpose.weight.data = filter\n",
    "\n",
    "x = (10**torch.arange(4, dtype=torch.float32)).reshape(1, 1, 2, 2)\n",
    "print(f\"x = \\n{x}\\nfilter = \\n{filter}\\nmy_conv_transpose(x) = \\n{my_conv_transpose(x)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 1:** complete the implementation of the subclass `MyUpsampler`, which should compute a convolutional transpose operation with `kernel_size=3, stride=2, padding=1` (as in the figure). \n",
    "\n",
    "Your solution can _not_ use `torch.nn.ConvTranspose2d` or `torch.nn.functional.conv_transpose2d`.\n",
    "\n",
    "Hint: Your solution _can_ use  [numpy-style striding](https://numpy.org/doc/stable/user/basics.indexing.html#slicing-and-striding) (which can be applied to torch tensors) as well as `torch.nn.functional.conv2d`([docs](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)) correctly composing these should do the trick.\n",
    "\n",
    "Hint: Technically transposed convolution flips the filter in the spatial dimensions whereas standard convolution does not, we handle this for you already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "class MyUpsampler(nn.Module):\n",
    "    def __init__(self, weight, bias):\n",
    "        super(MyUpsampler, self).__init__()\n",
    "        self.weight = weight.flip(2, 3) # Transposed convolution flips the filter in the spatial dimensions, whereas standard convolution does not.\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, w, h = x.shape\n",
    "        out = torch.zeros(b, c, 2*w - 1, 2*h- 1)\n",
    "        out[:, :, ::2, ::2] = x\n",
    "        out = F.conv2d(F.pad(out, (1, 1, 1, 1)), self.weight, bias=self.bias)\n",
    "        return  out\n",
    "\n",
    "# This should be the same as the output of my_conv_transpose(x) above. \n",
    "bias = torch.zeros(1, dtype=torch.float32)\n",
    "us = MyUpsampler(filter, bias)\n",
    "print(f\"us(x) = \\n{us(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a simple generation process\n",
    "\n",
    "In this section, we'll build up to the state-of-the-art generative computer vision models using a simple example dataset.\n",
    "\n",
    "Let's use a training dataset consisting of a grayscale images of a random number of rectangles with random sizes and positions. In order to be able to faithfully generate new images, our model must be able to learn the process that we used to generate them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: Make a simple dataset\n",
    "\n",
    "def generate_rectangle_images(batch_size=32):\n",
    "    # Initialize white background images (batch_size x 1 x 32 x 32)\n",
    "    images = torch.ones(batch_size, 1, 32, 32)\n",
    "    \n",
    "    # Sample number of rectangles per image from Poisson(2) + 1\n",
    "    n_rectangles = Poisson(torch.tensor(3.0)).sample((batch_size,)).int() + 1\n",
    "    \n",
    "    # For each image in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Generate n rectangles\n",
    "        for _ in range(n_rectangles[i].item()):\n",
    "            # Sample rectangle dimensions\n",
    "            width = Uniform(2, 10).sample().int().item()\n",
    "            height = Uniform(2, 10).sample().int().item()\n",
    "            \n",
    "            # Sample position (ensure rectangle fits within image)\n",
    "            x = Uniform(0, 32 - width).sample().int().item()\n",
    "            y = Uniform(0, 32 - height).sample().int().item()\n",
    "            \n",
    "            # Draw black rectangle (value of 0)\n",
    "            images[i, 0, y:y+height, x:x+width] = 0\n",
    "            \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset of 10000 images\n",
    "class UnlabeledTensorDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "rectangle_images = generate_rectangle_images(10000)\n",
    "train_dataset = UnlabeledTensorDataset(rectangle_images[1000:])\n",
    "validation_dataset = UnlabeledTensorDataset(rectangle_images[:1000])\n",
    "\n",
    "# Plot first 4 images in the dataset\n",
    "plt.figure(figsize=(8, 8), dpi=64)  # Set dpi to 64\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(2, 2, i + 1)\n",
    "    ax.imshow(train_dataset[i].squeeze(), cmap='gray')\n",
    "    # Remove ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('equal')  # Ensure aspect ratio is equal\n",
    "plt.tight_layout(pad=0)  # Remove padding between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we take these images $x$ and learn the process which generated them? There is a problem, which we encountered already in the audio notebook: neural networks themselves aren't random, and we cannot simply output logits over all possible images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A NaÃ¯ve approach: decoder image generation\n",
    "\n",
    "To generate images, a naive approach would be to sample a vector of normally distributed random seeds $z$, which we will call the _latent variables_, and then use a decoder neural network $f^{\\theta}$ to map $z$ to an image.\n",
    "\n",
    "$$x \\sim f^{\\theta}(z)$$\n",
    "\n",
    "Then, we could regress latent variables to a dataset of images. Let's try this: in every epoch we'll select a batch of images $x_{1:B}$ and a random set of random seeds $z_{1:B}$ and then try to learn a mapping between the former on the latter using Mean-Square-Error loss \n",
    "\n",
    "$$L(x) = \\left\\lVert x - f^{\\theta}(z) \\right\\rVert^2$$\n",
    "\n",
    "(Note: in generative computer vision we often use the MSE loss, which is equivalent to the log-likelihood of a standard normal distribution. The papers introducing the concepts in this notebook all use this statistical language when formulating their methods, we will avoid it for simplicity)\n",
    "\n",
    "![decoder-only](https://polybox.ethz.ch/index.php/s/3Gq9rQaeeKc2je9/download/decoder-only-figure.png)\n",
    "\n",
    "*The information flow (left to right) of the decoder at train time. $x' = f^{\\theta}(z)$ is the reconstructed image.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: define the decoder architecture mapping (batch_size, latent_dim) -> (batch_size, 1, 32, 32)\n",
    "\n",
    "decoder = nn.Sequential(\n",
    "    nn.Linear(latent_dim, 64 * 8 * 8),\n",
    "    nn.Unflatten(1, (64, 8, 8)),\n",
    "    nn.ConvTranspose2d(64, 64, 3, stride=2, padding=1, output_padding=1), # We use output padding to exactly double the size of the spatial dimensions \n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(64, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 1, 3, stride=1, padding=1),\n",
    "    nn.Sigmoid()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"decoder has {parameter_count_string(decoder)} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1:** Fill out the `compute_loss_decoder` function to decode and compute the loss described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_loss_decoder(z, decoder, batch):\n",
    "    \"\"\"Returns a torch scalar loss of the mean-squared error between the batch and the decoder reconstruction of the latent variables\"\"\"\n",
    "    # Decode the latent variables to images\n",
    "    recon_batch = decoder(z)\n",
    "    # Compute the MSE loss\n",
    "    loss = F.mse_loss(recon_batch, batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: test your solution to exercise 2\n",
    "\n",
    "# Test: your solution should give zero loss if the batch is zeros and the decoder returns zeros\n",
    "def dummy_decoder(z):\n",
    "    return torch.zeros(z.shape[0], 1, 32, 32)\n",
    "\n",
    "assert torch.allclose(compute_loss_decoder(torch.randn(5, 64), dummy_decoder, torch.zeros(5, 1, 32, 32)), torch.tensor(0., device=device))\n",
    "\n",
    "# Test: your solution should give zero loss if the perfect decoder predicts the batch exactly\n",
    "z = torch.randn(5, latent_dim, device=device)\n",
    "assert torch.allclose(compute_loss_decoder(z, decoder, decoder(z)), torch.tensor(0., device=device))\n",
    "\n",
    "# Test: your solution should give the same as the deviation from the \"perfect\" decoder\n",
    "z = torch.randn(5, latent_dim, device=device)\n",
    "noise = torch.randn(5, 1, 32, 32, device=device)\n",
    "assert torch.allclose(compute_loss_decoder(z, decoder, decoder(z) + noise), (noise**2).mean())\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's wrap our decoder in a training loop. You should observe the loss almost instantly converging to $\\approx 0.095$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "decoder.to(device)\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "\n",
    "# Assuming train_dataset is already defined\n",
    "dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Generate random latent variables\n",
    "        z = torch.randn(batch.size(0), latent_dim).to(device)\n",
    "\n",
    "        loss = compute_loss_decoder(z, decoder, batch)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try and generate some images.\n",
    "\n",
    "**Exercise 2.2**: fill out the `generate_images_decoder` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_images_decoder(num_samples, decoder):\n",
    "    \"\"\"Returns a tensor of shape (num_samples, 1, 32, 32) representing images \"\"\"\n",
    "    # Generate a batch of num_samples latent vectors\n",
    "    z = torch.randn(num_samples, latent_dim).to(device)\n",
    "    # Decode these latent vectors into images\n",
    "    images = decoder(z)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " If you get an image with a grey blur in the middle on all your generations, don't worry, this is expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random latent variables\n",
    "num_samples = 20\n",
    "\n",
    "# Generate images\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    generated_images = generate_images_decoder(20, decoder)\n",
    "\n",
    "# Plot the generated images\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(generated_images[i].cpu().squeeze().clip(0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders\n",
    "\n",
    "So, our approach didn't work. Why? The problem is that neural networks are continuous (they tend to map similar latent variables to similar images), but our training process doesn't account for this. If we sample two similar training images, we would like for the corresponding latent variables to also be similar.\n",
    "\n",
    "To solve this, let's use another neural network $g^{\\phi}$ to learn the inverse mapping from images to latent variables, which should map similar images to similar latents:\n",
    "\n",
    "$$z = g^{\\phi}(x)$$\n",
    "\n",
    "After considering this pair of neural networks you should notice this looks alot like the autoencoder we covered in the Computer Vision & Audio notebook, in fact it's the same concept! Let's try and use it for generating images. What we'll do is choose a much smaller latent space and then minimize the difference between the original image $x$ and the reconstructed image $f^{\\theta}(g^{\\phi}(x))$:\n",
    "\n",
    "$$\\mathcal{L}(x) = \\left\\lVert x - f^{\\theta}(g^{\\phi}(x)) \\right\\rVert^2 $$\n",
    "\n",
    "Hopefully, we should then be able to sample new images by decoding $z$ values that we sample randomly.\n",
    "\n",
    "![autoencoder](https://polybox.ethz.ch/index.php/s/SMX7wGAC3BGJGW4/download/autoencoder-figure.png)\n",
    "\n",
    "*The information flow (left to right) of an autoencoder at train time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: define the Autoencoder architecture with .encode and .decode methods mapping (batch_size, 1, 32, 32) -> (batch_size, latent_dim) -> (batch_size, 1, 32, 32)\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=64):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_backbone = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size\n",
    "        self.fc_z = nn.Linear(64 * 4 * 4, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64 * 4 * 4),\n",
    "            nn.Unflatten(1, (64, 4, 4)),\n",
    "            nn.ConvTranspose2d(64, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        return self.fc_z(self.encoder_backbone(x))\n",
    "        \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder().to(device)\n",
    "print(f\"autoencoder has {parameter_count_string(autoencoder)} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1:** fill out the `compute_loss_autoencoder` function to compute the loss as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_loss_autoencoder(autoencoder, batch):\n",
    "    \"\"\"Returns a torch scalar loss of the mean-squared error between the batch and the decoder reconstruction of the latent variables\"\"\"\n",
    "    # Encode the images as latent variables\n",
    "    z = autoencoder.encode(batch)\n",
    "    # Decode the latent variables to images\n",
    "    recon_batch = autoencoder.decode(z)\n",
    "    # Compute the MSE loss\n",
    "    loss = F.mse_loss(recon_batch, batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: test your solution to exercise 3.1\n",
    "\n",
    "class DummyAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DummyAutoencoder, self).__init__()\n",
    "\n",
    "    def encode(self, x):\n",
    "        return x\n",
    "\n",
    "    def decode(self, z):\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "# Test: your solution should give zero loss if the batch is zeros and the autoencoder returns zeros\n",
    "dummy_autoencoder = DummyAutoencoder()\n",
    "\n",
    "test_batch = torch.randn(5, 1, 32, 32)\n",
    "assert torch.allclose(compute_loss_autoencoder(dummy_autoencoder, test_batch), torch.tensor(0., device=device))\n",
    "\n",
    "# Test: your solution should give zero loss when given the perfect autoencoder.\n",
    "noise = torch.randn(5, 1, 32, 32)\n",
    "assert torch.allclose(compute_loss_autoencoder(dummy_autoencoder, noise), torch.tensor(0., device=device))\n",
    "\n",
    "# Test: your solution should give the same loss as the deviation from the \"perfect\" autoencoder.\n",
    "class NoisyAutoencoder(nn.Module):\n",
    "    def __init__(self, noise):\n",
    "        self.noise = noise\n",
    "        super(NoisyAutoencoder, self).__init__()\n",
    "\n",
    "    def encode(self, x):\n",
    "        return x\n",
    "\n",
    "    def decode(self, z):\n",
    "        return z + self.noise\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))\n",
    "\n",
    "noise = torch.randn(5, 1, 32, 32)\n",
    "test_batch = torch.randn(5, 1, 32, 32)\n",
    "\n",
    "noisy_autoencoder = NoisyAutoencoder(noise)\n",
    "\n",
    "assert torch.allclose(compute_loss_autoencoder(noisy_autoencoder, test_batch), (noise**2).mean())\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = optim.Adam(autoencoder .parameters(), lr=1e-4) # We need a smaller learning rate to prevent instabilities.\n",
    "dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Total loss\n",
    "        loss = compute_loss_autoencoder(autoencoder, batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Calculate validation loss\n",
    "        autoencoder.eval()\n",
    "        val_loss = 0\n",
    "        val_dataloader = DataLoader(validation_dataset, batch_size=128, shuffle=True)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "\n",
    "                batch = batch.to(device)\n",
    "                loss = compute_loss_autoencoder(autoencoder, batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreasing is a much more positive sign. Again, let's generate some images by sampling iid standard normal latents.\n",
    "\n",
    "**Exercise 3.2:** fill out the `generate_images_autoencoder` function. \n",
    "\n",
    "Hint: `torch.randn` ([docs](https://docs.pytorch.org/docs/stable/generated/torch.randn.html)) can be used to sample iid standard normal tensors of arbitrary shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_images_autoencoder(num_samples, autoencoder):\n",
    "    \"\"\"Returns a tensor of shape (num_samples, 1, 32, 32) representing images.\"\"\"\n",
    "    # Generate a batch of num_samples latent vectors normally distributed\n",
    "    z = torch.randn(num_samples, latent_dim).to(device)\n",
    "    # Decode these latent vectors into images\n",
    "    images = autoencoder.decode(z)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your solution should generate gray blurs with some deviations in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random latent variables\n",
    "num_samples = 20\n",
    "\n",
    "# Generate images\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    generated_images = generate_images_autoencoder(20, autoencoder)\n",
    "\n",
    "# Plot the generated images\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(generated_images[i].cpu().squeeze().clip(0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So sampling random latent variables didn't allow us to generate images like our dataset. As we saw in the Computer Vision and Audio notebook, standard autoencoders _do_ learn to reconstruct images that are like what they see at train time. With images, we can plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: visualize autoencoder reconstructions of images in the validation set\n",
    "autoencoder.eval()  # Set to evaluation mode\n",
    "\n",
    "# Get a batch of images from the validation set\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size=10, shuffle=True)\n",
    "original_images = next(iter(val_dataloader)).to(device)\n",
    "\n",
    "# Generate reconstructions\n",
    "with torch.no_grad():\n",
    "    reconstructed_images = autoencoder(original_images)\n",
    "\n",
    "# Plot original vs reconstructed images\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot original images\n",
    "for i in range(5):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(original_images[i].cpu().squeeze(), cmap='gray')\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "\n",
    "# Plot reconstructed images\n",
    "for i in range(5):\n",
    "    plt.subplot(2, 5, i + 6)\n",
    "    plt.imshow(reconstructed_images[i].cpu().squeeze(), cmap='gray')\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Variational_ AutoEncoders (VAEs)\n",
    "\n",
    "What we can see with the above autoencoder results is that the decoder from an autoencoder _does_ learn to reconstruct unseen images when given some very specific latent variables. The problem is that we have no clue of knowing which latent variables to choose to elicit these generations!\n",
    "\n",
    "What we can do is _force_ our autoencoder to make its latents have some nice statistical property (such as being normally distributed). This is precisely what _Variational AutoEncoders (VAEs)_ do. To get this to work, they add some noise to the encoder's latent prediction, by making it to output a mean and variance of a distribution of latents.\n",
    "\n",
    "$$\\mu, \\sigma^2 = g^{\\phi}(x)$$\n",
    "\n",
    "$$z \\sim \\mathcal{N}(\\mu, diag(\\sigma^2))$$\n",
    "\n",
    "Now, we can make the latent variables normally distributed by incentivising the encoder to output $\\mu, \\sigma$ closer to $0, I$. The new KL term in the right hand side of the below loss formula does just this (don't worry if you don't know the precise formula for this term). We need to add two tricks before this completely works. \n",
    "\n",
    "$$\\mathcal{L}(x) = \\frac{1}{2 b^2} \\left\\lVert x - f^{\\theta}(\\texttt{sample}(g^{\\phi}(x))) \\right\\rVert^2 + \\ln(b) + KL(g^{\\phi}(x) \\Vert \\mathcal{N}(0,I))$$\n",
    "\n",
    "The additional $b$ is to balance the mean square error term and the KL term, and it represents the model's uncertainty in its reconstruction. We will initialise it to $0.5$ and the let the model learn it's value. \n",
    "\n",
    "This loss formula is called the Evidence Lower BOund (ELBO), in this notebook we won't go into its precise derivation.\n",
    "\n",
    "![VAE](https://polybox.ethz.ch/index.php/s/yNcTpzfcKHks7qa/download/vae-figure.png)\n",
    "\n",
    "*The information flow (left to right) of a VAE at train time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: define the VAE architecture by extending the AutoEncoder class.\n",
    "class VAE(AutoEncoder):\n",
    "    def __init__(self, latent_dim=64):\n",
    "        super(VAE, self).__init__(latent_dim)\n",
    "        self.fc_mu = self.fc_z\n",
    "        self.fc_var = nn.Linear(64 * 4 * 4, latent_dim)\n",
    "        self.b = nn.Parameter(torch.tensor(.1)) # The initial variance of the reconstruction\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder_backbone(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        # In reality, we model the log of the variance, not the variance itself for numerical stability\n",
    "        log_var = self.fc_var(x) \n",
    "        return mu, log_var\n",
    "    \n",
    "    def sample_z(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var) # 1/2 as standard deviation is sqrt of variance\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "        \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.sample_z(mu, log_var)\n",
    "        return self.decode(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VAE to train\n",
    "model = VAE().to(device)\n",
    "print(f\"VAE has {parameter_count_string(model)} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.1:** fill out the `compute_loss_vae` function. Note that the losses will only be balanced if you take the **sum** of the squared errors (rather than the mean, as has been done in previous sections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_loss_vae(model, batch):\n",
    "    # Forward pass\n",
    "    recon_batch, mu, log_var = model(batch)\n",
    "    b = model.b\n",
    "    # Reconstruction loss sum of squared errors\n",
    "    sum_squared_loss = torch.sum((recon_batch - batch) ** 2)\n",
    "    # KL divergence loss (given for free)\n",
    "    kl_loss = 0.5 * torch.sum(log_var.exp() + mu.pow(2) - log_var - 1)\n",
    "    # Total loss\n",
    "    loss = sum_squared_loss / (2*b*b) + kl_loss + torch.log(b)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: test your solution\n",
    "class DummyVAE(nn.Module):\n",
    "    def __init__(self, mu, log_var, b, recon_x):\n",
    "        super(DummyVAE, self).__init__()\n",
    "        self.mu = mu\n",
    "        self.log_var = log_var\n",
    "        self.b = b\n",
    "        self.recon_x = recon_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.recon_x, self.mu, self.log_var\n",
    "\n",
    "# Test: if the image is perfectly reconstructed with b=1., the loss should be the KL divergence\n",
    "\n",
    "test_mu = torch.randn(2, 64).to(device)\n",
    "test_log_var = torch.randn(2, 64).to(device)\n",
    "test_b = torch.tensor(1.).to(device)\n",
    "test_x = torch.randn(2, 1, 32, 32).to(device)\n",
    "\n",
    "dummy_model = DummyVAE(test_mu, test_log_var, test_b, test_x).to(device)\n",
    "\n",
    "kl_term = 0.5 * torch.sum(test_log_var.exp() + test_mu.pow(2) - test_log_var - 1)\n",
    "\n",
    "assert torch.allclose(compute_loss_vae(dummy_model, test_x), kl_term)\n",
    "\n",
    "# Test: if the latent distribution is standard normal and b=1, the loss should be half the _sum_ of the squared error.\n",
    "\n",
    "test_mu = torch.zeros_like(test_mu)\n",
    "test_log_var = torch.zeros_like(test_log_var)\n",
    "test_x_recon = torch.randn(2, 1, 32, 32).to(device)\n",
    "\n",
    "dummy_model = DummyVAE(test_mu, test_log_var, test_b, test_x_recon).to(device)\n",
    "\n",
    "assert torch.allclose(compute_loss_vae(dummy_model, test_x), torch.sum((test_x - test_x_recon)**2) / 2)\n",
    "\n",
    "# Test: extend the above test to check for b!= 1\n",
    "\n",
    "test_b = torch.tensor(0.73).to(device)\n",
    "\n",
    "dummy_model = DummyVAE(test_mu, test_log_var, test_b, test_x_recon).to(device)\n",
    "\n",
    "assert torch.allclose(compute_loss_vae(dummy_model, test_x), torch.sum((test_x - test_x_recon)**2) / (2*test_b**2) + torch.log(test_b))\n",
    "\n",
    "print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While waiting for your VAE to train, you can read the below section *Aside: Generative Adversarial Networks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VAE to train\n",
    "model = VAE().to(device)\n",
    "print(f\"VAE has {parameter_count_string(model)} parameters\")\n",
    "\n",
    "# Training setup\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        loss = compute_loss_vae(model, batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataset)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "\n",
    "        # Calculate validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_dataloader = DataLoader(validation_dataset, batch_size=128, shuffle=True)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "\n",
    "                batch = batch.to(device)\n",
    "                loss = compute_loss_vae(model, batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(validation_dataset)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's generate some images from the VAE. Your results should be much better than the autoencoder, but still have a quite a bit of blurriness.\n",
    "\n",
    "**Exercise 4.2:** write the `generate_images_vae` function.\n",
    "\n",
    "Hint: you may not need to rewrite the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "generate_images_vae = generate_images_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate some samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    samples = generate_images_vae(4, model)\n",
    "    \n",
    "# Plot generated samples\n",
    "plt.figure(figsize=(8, 8), dpi=64)\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(2, 2, i + 1)\n",
    "    ax.imshow(samples[i].cpu().squeeze(), cmap='gray')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('equal')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some reconstructions from the VAE too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random image from the dataset\n",
    "image = train_dataset[torch.randint(0, len(train_dataset), (1,))].to(device)\n",
    "\n",
    "# reconstruct the image with the VAE\n",
    "with torch.no_grad():\n",
    "    reconstructed, _, _ = model(image)\n",
    "\n",
    "# display the original and reconstructed images\n",
    "plt.figure(figsize=(8, 8), dpi=128)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image.cpu().squeeze(), cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reconstructed.cpu().squeeze(), cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Generative Adversarial Networks\n",
    "\n",
    "*Reading this section is optional but encouraged.*\n",
    "\n",
    "Until ~2020, the way to solve the blurriness problem of the VAE was to use a Generative Adversarial Network (GAN). Instead of trying to learn the inverse mapping from images to latent variables, GANs would instead learn a discriminator $\\delta$ which could tell real images from generated images. The discriminator would be trained using labels indicating whether an image is from the training dataset or generated by $f^\\theta$, and the generator would be trained using the gradient of the discriminator backpropagated onto the pixels of the generated images.\n",
    "\n",
    "One problem with GANs is that they are hard to validate: with a VAE you can compare the loss on the train and validation datasets, but with a GAN there exists no such way to pass a dataset into the generator. You _can_ evaluate the most extreme form of overfitting by searching for exact copies of generated images in the train dataset, but this does not account for overfitting to certain motifs in the train dataset. GANs also tend to struggle with very diverse datasets: the generator gets stuck only generating a few of the many classes in the data.\n",
    "\n",
    "![GAN](https://polybox.ethz.ch/index.php/s/6yp84KePe4PGBsc/download/GAN-figure.png)\n",
    "\n",
    "*The information flow (left to right) of a GAN at train time.*\n",
    "\n",
    "### Diffusion Models\n",
    "\n",
    "One way one can understand the bluriness of VAEs is to realise that the model internally may know that the image contains hard edges, but with a single step generation process it must hedge its bet when reconstructing the image and predict a blur around where it believes the edge should be. Additionally, our VAE isn't very sample efficient: it only has ~500k parameters, produces low-fidelity samples and has still slightly overfit to the training data.\n",
    "\n",
    "_Diffusion models_ solve these problems by turning generation into a multi-step process: the model partially reconstructs the image, looks at the image again, reconstructs it a bit more, looks at the image again, and so on. The fact that the model is trained on so many partial reconstructions makes the effective training dataset much larger, meaning that we can use a larger model with more compute without overfitting.\n",
    "\n",
    "Specifically, at train time we pick a random \"timestep\" $t$ in $0, 1, 2 \\dots 1000$, and then combine a training image $x$ with some normally distributed noise $\\epsilon$ to get some noised image $x_t$. For this notebook, we will use a simple linear combination:\n",
    "\n",
    "$$x_t = \\left(1 - \\frac{t}{1000} \\right)x + \\frac{t}{1000} \\epsilon$$\n",
    "\n",
    "Thus, at $t=0$ the input $x_0$ is the original image $x$, whereas at time $t=1000$ the input $x_{1000}$ is pure noise. \n",
    "\n",
    "The idea is that we can train the model to reverse this process: given a noised image $x_t$, can we remove a small amount of noise to get a slightly less noisy image $x_{t-1}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: plotting the flow x_t process at different timesteps\n",
    "plt.figure(figsize=(20, 5))\n",
    "image = train_dataset[5]\n",
    "\n",
    "# Generate noise\n",
    "epsilon = torch.randn_like(image)\n",
    "\n",
    "\n",
    "for i, t in enumerate(range(0, 1100, 100)):\n",
    "    t = min(t, 1000)\n",
    "    # Mix image and noise according to timestep\n",
    "    x_t = (1 - t/1000) * image + (t/1000) * epsilon\n",
    "    \n",
    "    plt.subplot(1, 11, i+1)\n",
    "    plt.imshow(x_t.cpu().squeeze(), cmap='gray')\n",
    "    plt.title(f't={t}')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our formulation does tell us that $x_{t-1}$ differs from $x_t$ by a small step in the direction in pixel space $\\epsilon - x$:\n",
    "\n",
    "$$x_{t-1} = x_t - \\frac{1}{1000}(\\epsilon - x)$$\n",
    "\n",
    "The brilliant thing is that we can use a neural network $f^{\\theta}$ to take in $x_t$ for some sample of $x, t, \\epsilon$ and output its best approximation of the direction $\\epsilon - x$, which we evaluate with MSE.\n",
    "\n",
    "$$\\mathcal{L}(x) = \\left\\lVert (\\epsilon - x) - f^{\\theta} \\left(x_t, \\frac{t}{1000}\\right) \\right\\rVert^2$$\n",
    "\n",
    "If we let $v = \\epsilon - x$ and $v_t' = f^{\\theta}(x_t, \\frac{t}{1000})$, the below figure describes the information flow at train time:\n",
    "\n",
    "![Diffusion](https://polybox.ethz.ch/index.php/s/6KyiNLMMcpEizaG/download/diffusion-figure.png)\n",
    "\n",
    "*The information flow (left to right) of a diffusion model at train time.*\n",
    "\n",
    "Then to generate new samples, we can start with some noise for $x_{1000}'$, get our network's prediction $v_{1000}' = f^{\\theta}(x_{1000}', 1)$ for the model's best guess of the direction $v$, denoise our sample a small amount using the network's prediction to retrieve a slightly less noisy sample, our best approximation of what $x_{999}'$ would have been, and then repeat:\n",
    "\n",
    "$$x_{t-1}' = x_{t}' - \\frac{1}{1000} f^{\\theta} \\left(x_{t}', \\frac{t}{1000}\\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Diffusion model architecture: \n",
    "- It can be much larger than the VAE's without overfitting. \n",
    "- As we don't need latents to be squashed through a bottleneck, we can use the u-net architecture which we covered in the Computer Vision and Audio notebook. (Technically this is _possible_ with VAEs, but it requires adding alot of complexity with a heirachy of latent variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture for our diffusion model. We can make the model much larger than the VAE for the reasons discussed above, and we can also \n",
    "class DiffusionUNet(nn.Module):\n",
    "    def __init__(self, base_dim=32, n_channels=1, n_updown_blocks=4, n_middle_blocks=2):\n",
    "        \"\"\"base_dim is the number of channels after the first convolution, n_channels is the number of channels in the input image\"\"\"\n",
    "        super(DiffusionUNet, self).__init__()\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "\n",
    "        self.conv_in = nn.Sequential(\n",
    "            nn.Conv2d(n_channels+1, base_dim, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(8, base_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        for i in range(n_updown_blocks):\n",
    "            # Encoder layers\n",
    "            in_channels = base_dim * (2**i)\n",
    "            out_channels = base_dim * (2**(i+1))\n",
    "            \n",
    "            self.down_blocks.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "                nn.GroupNorm(8, out_channels),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "            \n",
    "            # Decoder layers\n",
    "            dec_in_channels = out_channels * 2  # We double the number of channels for skip connections\n",
    "            dec_out_channels = in_channels\n",
    "            \n",
    "            self.up_blocks.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(dec_in_channels, dec_out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.GroupNorm(8, dec_out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(dec_out_channels, dec_out_channels, kernel_size=1, stride=1, padding=0), # Add a 1x1 conv to make future adaptations easier\n",
    "                nn.GroupNorm(8, dec_out_channels),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "\n",
    "        mid_block_width = base_dim * 2**(n_updown_blocks)\n",
    "\n",
    "        self.middle = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(mid_block_width, mid_block_width, 3, padding=1),\n",
    "                nn.GroupNorm(8, mid_block_width),\n",
    "                nn.ReLU()\n",
    "            ) \n",
    "        for _ in range(n_middle_blocks)])\n",
    "\n",
    "        self.conv_out = nn.Conv2d(2*base_dim, n_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x_t, t):\n",
    "        # Concatenate timestep as another channel in the image\n",
    "        t = t.expand(-1, 1, x_t.shape[2], x_t.shape[3])\n",
    "        x = torch.cat([x_t, t], dim=1)\n",
    "\n",
    "        x = self.conv_in(x)\n",
    "\n",
    "        x_in = x\n",
    "\n",
    "        # Store the hidden states as we downsample\n",
    "        hidden_states = [x]\n",
    "        for layer in self.down_blocks:\n",
    "            x = layer(x)\n",
    "            hidden_states.append(x)\n",
    "\n",
    "        # Use residual connections in the middle\n",
    "        for layer in self.middle:\n",
    "            x = x + layer(x)\n",
    "\n",
    "        # Use skip connections with the corresponding stored hidden states\n",
    "        for layer, hidden_state in zip(reversed(self.up_blocks), reversed(hidden_states)):\n",
    "            x = torch.cat([x, hidden_state], dim=1)\n",
    "            x = layer(x)\n",
    "\n",
    "        # Concatenate the original image with the stored hidden states\n",
    "        x = torch.cat([x, x_in], dim=1)\n",
    "        x = self.conv_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways of formulating a diffusion processes by combining $(x, \\epsilon, t)$ to get a loss and sampling process, the one we present above is called *rectified flow* or *optimal transport*, it's relatively mathematically simple and was used to train Stable Diffusion 3.\n",
    "\n",
    "**Exercise 5.1:** fill out the `compute_loss_rectified_flow` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_loss_rectified_flow(model, x):\n",
    "    # Sample a batch of shape (batch_size, 1, 1, 1) of random timesteps uniformly from 0 to 1000\n",
    "    batch_size = x.shape[0]\n",
    "    time = torch.randint(0, 1000, (batch_size, 1, 1, 1)).to(device)\n",
    "    \n",
    "    # Create a batch of the same shape as x of normally distributed noise\n",
    "    noise = torch.randn_like(x)\n",
    "\n",
    "    # Create a batch of noised images x_t according to rectified flow process\n",
    "    x_t = (1 - time/1000) * x + (time/1000) * noise\n",
    "\n",
    "    # Predict direction v_t using the model\n",
    "    v_t = model(x_t, time/1000)\n",
    "\n",
    "    # Compute the true direction v\n",
    "    v = noise - x\n",
    "\n",
    "    # Rectified flow loss: MSE between predicted and true direction (this time, you can use the mean)\n",
    "    loss = F.mse_loss(v_t, v)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: test your compute_loss_rectified_flow function\n",
    "class DummyDiffusionUNet(nn.Module):\n",
    "    def __init__(self, true_x, error):\n",
    "        super(DummyDiffusionUNet, self).__init__()\n",
    "        # Memorize the true x and some model error term we will add to the prediction\n",
    "        self.true_x = true_x\n",
    "        self.error = error\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        # Test: the timestep given to the model should contain values in the range [0, 1]\n",
    "        assert torch.all(t >= 0) and torch.all(t <= 1)\n",
    "        \n",
    "        # Compute the true noise using the memorized true x\n",
    "        true_noise = (1/t) * (x_t - self.true_x * (1 - t))\n",
    "\n",
    "        # Test: the true noise should have mean 0 and std 1\n",
    "        # Ideally would use a Goodness-of-Fit (eg. KS test here) to test normality but I doubt students will use the wrong distribution.\n",
    "        standard_error = 1 / true_noise.numel() **.5\n",
    "        assert -4 < true_noise.mean()/standard_error < 4\n",
    "\n",
    "        standard_error = 1 / (2* true_noise.numel()) **.5\n",
    "        assert -4 < (true_noise.std() - 1)/standard_error < 4\n",
    "\n",
    "        # Compute the true direction v using the memorized true x\n",
    "        true_v = (1/t) * (x_t - self.true_x)\n",
    "\n",
    "        # Return the true direction v plus the predetermined model error term for more tests\n",
    "        return true_v + self.error\n",
    "\n",
    "# Test: if the model can compute the true direction v, the loss should be 0\n",
    "\n",
    "test_x = torch.randn(8, 1, 64, 64).to(device)\n",
    "test_error = torch.zeros(8, 1, 64, 64).to(device)\n",
    "\n",
    "my_model = DummyDiffusionUNet(test_x, test_error)\n",
    "\n",
    "assert torch.allclose(compute_loss_rectified_flow(my_model, test_x), torch.tensor(0.))\n",
    "\n",
    "# Test: if there is some error in the model, the loss should be the mean square of this error\n",
    "\n",
    "test_error = torch.randn(8, 1, 64, 64).to(device)\n",
    "\n",
    "my_model = DummyDiffusionUNet(test_x, test_error)\n",
    "\n",
    "assert torch.allclose(compute_loss_rectified_flow(my_model, test_x), torch.mean(test_error**2))\n",
    "\n",
    "print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_rectified_flow(model, val_dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_dataloader:\n",
    "            \n",
    "            val_x = val_batch.to(device)\n",
    "            loss = compute_loss_rectified_flow(model, val_x)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(val_dataloader)\n",
    "    model.train()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# Training loop with rectified flow objective\n",
    "def train_rectified_flow(model, dataloader, val_dataloader, num_epochs=100, lr=1e-4):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get data and move to device\n",
    "            x = batch.to(device)\n",
    "            \n",
    "            loss = compute_loss_rectified_flow(model, x)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        if epoch % 10 == 0:\n",
    "            # Calculate validation loss\n",
    "            val_loss = validate_rectified_flow(model, val_dataloader)\n",
    "            print(f'Epoch {epoch}, Train MSE: {avg_loss:.4f}, Val MSE: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop should take ~5 mins to run. While it's running, you can read the below section on conditional image generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_unet = DiffusionUNet(base_dim=32)\n",
    "\n",
    "print(f\"DiffusionUNet has {parameter_count_string(diffusion_unet)} parameters\")\n",
    "\n",
    "diffusion_unet.to(device)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "train_rectified_flow(diffusion_unet, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the model doesn't overfit, despite having 20x more parameters than the VAE. This means that, with our diffusion model, if we had more time and compute, we could a train larger model for longer and get even better results.\n",
    "\n",
    "Now, let's generate some images from the rectified flow. Your results won't perfectly match the dataset and there will still be some artefacts, but it should be much better.\n",
    "\n",
    "**Exercise 5.2:** fill out the `generate_images_rectified_flow` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images_rectified_flow(model, num_samples):\n",
    "    # Initialise the sample from random noise\n",
    "    x = torch.randn(num_samples, 1, 32, 32).to(device)\n",
    "    \n",
    "    # Sample using rectified flow (reverse process)\n",
    "    # Loop over the timesteps [1000, 999, 998, ..., 1]\n",
    "    for step in range(1000, 0, -1):\n",
    "        # Make a timestep tensor of shape (num_samples, 1, 1, 1)\n",
    "        timestep = torch.full((num_samples, 1, 1, 1), step).to(device)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise = model(x, timestep/1000)\n",
    "        \n",
    "        # Update sample using rectified flow equation\n",
    "        x = x - 1/1000 * noise\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate and visualize samples\n",
    "num_samples = 4\n",
    "with torch.no_grad():\n",
    "    samples = generate_images_rectified_flow(diffusion_unet, num_samples)\n",
    "\n",
    "# Plot generated samples\n",
    "plt.figure(figsize=(8, 8), dpi=64)\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(2, 2, i + 1)\n",
    "    ax.imshow(samples[i].cpu().squeeze().clip(0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('equal')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Diffusion Models\n",
    "\n",
    "Now we know the principles of operation for diffusion models, which are the general class of models which dominate the state-of-the-art as of 2025, let's see how we can actually use them in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Image Generation\n",
    "\n",
    "Say that we have a dataset of different classes of images, or even images with textual captions. To share we don't want to re-train a new set of model weights for each class, so how do we do this? Perhaps suprisingly, a statistically valid way to do this is simply to _input a representation of the class_ into the networks composing our model. \n",
    "\n",
    "Specifically, any of the above models can be made class conditional by taking all of the component networks $h^{\\theta}(x, ...)$ and adding the class information $c$ to get $h^{\\theta}(x, c. ...)$. At train and test time, we pass the class information $c$ into all the components uncorrupted. This works because, at the abstract math level, the class information essentially has the same effect as an additional set of model weights $\\theta_c = (\\theta, c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning to generate MNIST\n",
    "\n",
    "To see how diffusion generalises to real-world datasets, let's train a model to generate MNIST digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: for Google Colab/local machine, you can just set cache_dir to \"./\"\n",
    "import os\n",
    "import getpass\n",
    "user = getpass.getuser()\n",
    "cache_dir = os.path.join(\"/scratch\", user)\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_train = torchvision.datasets.MNIST(root=cache_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = torchvision.datasets.MNIST(root=cache_dir, train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "valid_size = len(mnist_train) // 10\n",
    "\n",
    "mnist_valid = torch.utils.data.Subset(mnist_train, range(valid_size))\n",
    "mnist_train = torch.utils.data.Subset(mnist_train, range(valid_size, len(mnist_train)))\n",
    "\n",
    "mnist_train_dataloader = DataLoader(mnist_train, batch_size=128, shuffle=True)\n",
    "mnist_valid_dataloader = DataLoader(mnist_valid, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some examples of the MNIST dataset\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(mnist_train[i][0].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the class information as input can be done in many ways, for large models conditioned text we often use cross-attention, but for our model we'll use the `nn.Embedding` class ([docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)). What we'll do is wrapping our pre-existing `DiffusionUNet` class to add some extra channels to our image, where these channels are the class embedding copied across the width and height of the image.\n",
    "\n",
    "**Exercise 6:** Finish the `ConditionalDiffusionUNet.forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "class ConditionalDiffusionUNet(DiffusionUNet):\n",
    "    def __init__(self, base_dim=32, n_channels=1, n_updown_blocks=4, n_middle_blocks=2, n_classes=None, class_embedding_dim=16):\n",
    "        # We will use concatenate the class embedding as some extra channels in the image\n",
    "\n",
    "        super(ConditionalDiffusionUNet, self).__init__(base_dim=base_dim, n_channels=n_channels+class_embedding_dim, n_updown_blocks=n_updown_blocks, n_middle_blocks=n_middle_blocks)\n",
    "\n",
    "        self.class_embedding = nn.Embedding(n_classes, class_embedding_dim)\n",
    "\n",
    "        # we need to reset this as it is incorrectly set by the superclass constructor\n",
    "        self.n_channels_img = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, x, t, c):\n",
    "        # Here c is a tensor of shape (batch_size, ) with integer values in the range [0, n_classes)\n",
    "        # Embed the class information\n",
    "        c = self.class_embedding(c)\n",
    "\n",
    "        # Copy the embedding across the width and height of the image\n",
    "        c = c.unsqueeze(2).unsqueeze(3)\n",
    "        c = c.expand(-1, -1, x.shape[2], x.shape[3])\n",
    "\n",
    "        # Concatenate the embedding with the image in the channel dimension\n",
    "        x = torch.cat([x, c], dim=1)\n",
    "\n",
    "        x = super().forward(x, t)\n",
    "\n",
    "        # Remove the extra channels\n",
    "        x = x[:, :self.n_channels_img, :, :]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As MNIST images are 28x28, we can only downsample twice before we get to 7x7, so we use 2 updown blocks to avoid complications\n",
    "mnist_unet = ConditionalDiffusionUNet(base_dim=32, n_classes=10, n_updown_blocks=2, n_middle_blocks=8).to(device)\n",
    "print(\"mnist_unet has\", parameter_count_string(mnist_unet), \"parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: test your solution to Exercise 6\n",
    "test_x = torch.randn(5, 1, 28, 28).to(device)\n",
    "test_c = torch.randint(0, 10, (5,)).to(device)\n",
    "test_t = torch.ones(5, 1, 1, 1).to(device) * 500\n",
    "out = mnist_unet(test_x, test_t, test_c)\n",
    "assert out.shape == (5, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7.1** adapt your solution to Exercise 5.1 to complete the `compute_loss_rectified_flow_conditional` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_loss_rectified_flow_conditional(model, x, c):\n",
    "    # Sample a batch of shape (batch_size, 1, 1, 1) of random timesteps uniformly from 0 to 1000\n",
    "    batch_size = x.shape[0]\n",
    "    time = torch.randint(0, 1000, (batch_size, 1, 1, 1)).to(device)\n",
    "    \n",
    "    # Create a batch of the same shape as x of normally distributed noise\n",
    "    noise = torch.randn_like(x)\n",
    "\n",
    "    # Create a batch of noised images x_t according to rectified flow process\n",
    "    x_t = (1 - time/1000) * x + (time/1000) * noise\n",
    "\n",
    "    # Predict direction v_t using the model\n",
    "    v_t = model(x_t, time/1000, c)\n",
    "\n",
    "    # Compute the true direction v\n",
    "    v = noise - x\n",
    "\n",
    "    # Rectified flow loss: MSE between predicted and true direction (this time, you can use the mean)\n",
    "    loss = F.mse_loss(v_t, v)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our MNIST model for $20$ epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: adapted training loops from above to be class-conditional\n",
    "\n",
    "def validate_rectified_flow_conditional(model, val_dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_dataloader:\n",
    "            val_x, val_c = val_batch\n",
    "            val_x = val_x.to(device)\n",
    "            val_c = val_c.to(device)\n",
    "            \n",
    "            loss = compute_loss_rectified_flow_conditional(model, val_x, val_c)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    val_loss /= len(val_dataloader)\n",
    "    model.train()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# Training loop with rectified flow objective\n",
    "def train_rectified_flow_conditional(model, dataloader, val_dataloader, num_epochs=100, lr=1e-4):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get data and move to device\n",
    "            x, c = batch\n",
    "            x = x.to(device)\n",
    "            c = c.to(device)\n",
    "            \n",
    "            loss = compute_loss_rectified_flow_conditional(model, x, c)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        if epoch % 10 == 0:\n",
    "            # Calculate validation loss\n",
    "            val_loss = validate_rectified_flow_conditional(model, val_dataloader)\n",
    "            print(f'Epoch {epoch}, Train MSE: {avg_loss:.4f}, Val MSE: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rectified_flow_conditional(mnist_unet, mnist_train_dataloader, mnist_valid_dataloader, num_epochs=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7.2** adapt your solution to Exercise 5.2 to complete the `generate_images_rectified_flow_conditional` function. Make sure to initialise your image with spatial dimension $28 \\times 28$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_images_rectified_flow_conditional(model, c, num_samples):\n",
    "    # Initialise the sample from random noise\n",
    "    x = torch.randn(num_samples, 1, 28, 28).to(device)\n",
    "    \n",
    "    # Sample using rectified flow (reverse process)\n",
    "    # Loop over the timesteps [1000, 999, 998, ..., 1]\n",
    "    for step in range(1000, 0, -1):\n",
    "        # Make a timestep tensor of shape (num_samples, 1, 1, 1)\n",
    "        timestep = torch.full((num_samples, 1, 1, 1), step).to(device)\n",
    "        \n",
    "        # Predict noise\n",
    "        noise = model(x, timestep/1000, c)\n",
    "        \n",
    "        # Update sample using rectified flow equation\n",
    "        x = x - 1/1000 * noise\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mnist_samples = 20\n",
    "# Use assending order of classes as the condition, with wrapping around.\n",
    "c_mnist_samples = torch.arange(n_mnist_samples).fmod(10).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples = generate_images_rectified_flow_conditional(mnist_unet, c_mnist_samples, num_samples=n_mnist_samples)\n",
    "\n",
    "# Plot generated samples\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(samples[i].cpu().squeeze().clip(0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's getting there, but still a bit under-baked. Let's load one that has been trained using the above code for 100 epochs, and see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsed cell: loading a pretrained mnist_unet_100_epochs model\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "user = getpass.getuser()\n",
    "cache_dir = os.path.join(\"/scratch\", user)\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "unet_save_path = os.path.join(cache_dir, \"mnist_unet_100_epochs_model.pth\")\n",
    "\n",
    "if not os.path.exists(unet_save_path):\n",
    "    unet_url = \"https://polybox.ethz.ch/index.php/s/wjBkDgWSAMZJj6T/download/mnist_unet_100_epochs_model.pth\"\n",
    "    print(f\"Model not found at {unet_save_path}, downloading from {unet_url}\")\n",
    "    response = requests.get(unet_url)\n",
    "    with open(unet_save_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded model to {unet_save_path}\")\n",
    "\n",
    "# Load the saved model\n",
    "# Note: for Google Colab/local machine you may need to set the weights_only flag to False\n",
    "mnist_unet_100_epochs = torch.load(unet_save_path)\n",
    "mnist_unet_100_epochs.to(device)  # Move model to GPU\n",
    "mnist_unet_100_epochs.eval()  # Set to evaluation mode\n",
    "print(f\"Loaded model from {unet_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    samples = generate_images_rectified_flow_conditional(mnist_unet_100_epochs, c_mnist_samples, num_samples=n_mnist_samples)\n",
    "\n",
    "# Plot generated samples\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(samples[i].cpu().squeeze().clip(0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-of-the art generative computer vision\n",
    "\n",
    "We don't have the time \\& compute to train the latest diffusion models from scratch, so we'll use the ð¤ HuggingFace ð§¨ Diffusers [library](https://huggingface.co/docs/diffusers/index) to load a pre-trained models and play with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU\n",
    "for name in dir():\n",
    "    if not name.startswith('_'):\n",
    "        del globals()[name]\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Move huggingface cache to scratch space to avoid filling up /net-scratch/\n",
    "import os\n",
    "import getpass\n",
    "user = getpass.getuser()\n",
    "cache_dir = os.path.join(\"/scratch\", user)\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "cache_dir = os.path.join(cache_dir, \"hugging_face\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "os.environ[\"HF_HOME\"] = os.path.abspath(cache_dir)\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.abspath(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "import peft\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we'll load is StableDiffusionXL, which was [released](https://arxiv.org/pdf/2307.01952) in 2023. It uses _latent diffusion_: an autoencoder downsamples the image to some latents and then diffusion is performed on these latents rather than the image itself. At generation time, the diffusion process contains another noise-denoise process (called refinement). Instead of conditioning on a class label, the generation is conditioned on a text prompt entered by the user, which is inputted into the diffusion UNet with cross-attention.\n",
    "\n",
    "![StableDiffusionXL](https://polybox.ethz.ch/index.php/s/WCNSocn9LbCnfJC/download/stablediffusionXL-architecture.png)\n",
    "\n",
    "Figure credit: [SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis](https://arxiv.org/pdf/2307.01952), StabilityAI. *ArXiV. 2023*\n",
    "\n",
    "You need $\\approx 6GB$ of VRAM free to run the stable diffusion XL example in the notebook. To fit this on the GPU we're going to use `fp16` precision which uses 2 bytes/parameter (rather than the 4 bytes/parameter of `fp32`) and we're going to configure HuggingFace to load parts of the model in \\& out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "def display_gpu_memory():\n",
    "  total_memory_GB = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
    "  allocated_memory_GB = torch.cuda.memory_allocated(device) / 1024**3\n",
    "  remaining_gpu_memory_GB = (total_memory_GB - allocated_memory_GB) \n",
    "  print(f\"Remaining GPU memory: {remaining_gpu_memory_GB:.2f}/{total_memory_GB:.2f} GB\")\n",
    "\n",
    "display_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "def count_parameters(module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "\n",
    "def parameter_count_string(module):\n",
    "    n_params = count_parameters(module)\n",
    "    if n_params > 10**9:\n",
    "        return f\"{n_params/10**9:.1f}B\"\n",
    "    elif n_params > 10**6:\n",
    "        return f\"{n_params/10**6:.1f}M\"\n",
    "    elif n_params > 10**3:\n",
    "        return f\"{n_params/10**3:.1f}k\"\n",
    "    else:\n",
    "        return f\"{n_params}\"\n",
    "print(f\"pipeline contains a vae, text encoder and unet with {parameter_count_string(pipe.vae)}, {parameter_count_string(pipe.text_encoder)} and {parameter_count_string(pipe.unet)} parameters respectively\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, HuggingFace wraps the model in a nice pipeline abstraction, making inference easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"ETH Zurich main building\"\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt).images[0]\n",
    "    \n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Rank Adaptation (LoRA)\n",
    "\n",
    "We see that our model knows what a rough university campus building looks like, but doesn't know what the ETH Zurich main building looks like. How could we (in theory) change this?\n",
    "\n",
    "We could collect a bunch of images of the ETH Zurich main building and use them to finetune our model. This has problems:\n",
    "\n",
    "- We would need $\\approx 3 \\times$ the GPU RAM as we use in inference to store the gradients and the rest of the optimizer state.\n",
    "- We would have to collect $>100$ images to make sure our model doesn't overfit.\n",
    "\n",
    "What we can do instead is freeze our model parameters train an _adapter_: a small part that we train and add to our model for a specific task. The adapter can have many fewer parameters than the base model, making it less prone to overfit and making storing the gradients much less memory intensive.\n",
    "\n",
    "Low Rank Adapters (LoRAs) are a type of adapters which run in parrallel to each linear map in the base model. Each LoRA adaptation of a given linear map is composed of two smaller linear maps: $A$ which maps the input of the linear map to a small number of entries and $B$ which maps the small number of entries to a vector which we add to the base output. We then freeze the base model and initialize $B = 0$ so that the adapted model behaves the same as the base model until we start to update $A$ \\& $B$ . \n",
    "\n",
    "Figure 1 of the seminal paper gives a good depiction of this process:\n",
    "\n",
    "![LoRA](https://polybox.ethz.ch/index.php/s/GrMoyPRiNqneDKp/download/LoRA_Hu_Fig1.png)\n",
    "\n",
    "Figure credit: [LoRA: Low-Rank Adaptation of Large Language Models](https://openreview.net/forum?id=nZeVKeeFYf9), Hu et al. *ICLR 2022.*\n",
    "\n",
    "The frozen model parameters are blue, the trainable LoRA parameters are orange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA toy problem: regularizing linear regression \n",
    "\n",
    "To see LoRA in action in a toy problem, we're going to use it to train a linear mapping $W$ between two noisy vectors with `512` entries each. Let's generate such a dataset with only `50` examples.\n",
    "\n",
    "(aside: solving this problem may not be actually _too_ far from what LoRA does on large models, there is [some evidence](https://transformer-circuits.pub/2022/toy_model/index.html#motivation-directions) that common large neural networks represent concepts as linear combinations of vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy problem setup:\n",
    "embedding_dim = 512\n",
    "\n",
    "mu_x = torch.randn(1, embedding_dim)\n",
    "mu_y = torch.randn(1, embedding_dim)\n",
    "\n",
    "n_train = 50\n",
    "\n",
    "n_test = 50\n",
    "\n",
    "# Create a batch of 50 training examples.\n",
    "x_train = 0.3 * torch.randn(n_train, embedding_dim) + mu_x\n",
    "y_train = 0.3 * torch.randn(n_train, embedding_dim) + mu_y\n",
    "\n",
    "# Create a batch of 50 test examples.\n",
    "x_test = 0.3 * torch.randn(n_test, embedding_dim) + mu_x\n",
    "y_test = 0.3 * torch.randn(n_test, embedding_dim) + mu_y\n",
    "\n",
    "W = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "print(\"W has\", parameter_count_string(W), \"parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize these data, heres a scatter plot of the first two entries of the train dataset vectors and their means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_toy_problem(W, x_train, y_train, x_test, y_test):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(x_train[:, 0], x_train[:, 1], c='blue', label='Training examples')\n",
    "    plt.scatter(mu_x[0, 0], mu_x[0, 1], c='black', label='Mean of x')\n",
    "    plt.scatter(y_train[:, 0], y_train[:, 1], c='red', label='Training examples')\n",
    "    plt.scatter(mu_y[0, 0], mu_y[0, 1], c='black', label='Mean of y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_toy_problem(W, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we naively finetune on this dataset using SGD we see that our model dramatically overfits and uses the same number of trainable parameters as $W$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_finetune(W, x_train, y_train, x_test, y_test):\n",
    "\n",
    "    print(f\"W has {parameter_count_string(W)} parameters\")\n",
    "    optimizer = torch.optim.Adam(W.parameters(), lr=1e-3)\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_train_pred = W(x_train)\n",
    "\n",
    "        train_loss = F.mse_loss(y_train_pred, y_train)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_test_pred = W(x_test)\n",
    "            test_loss = F.mse_loss(y_test_pred, y_test)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}\")\n",
    "\n",
    "\n",
    "W_copy = deepcopy(W)\n",
    "full_finetune(W_copy, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:** Fill in the `lora_finetune` function by implementing LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "def lora_finetune(W, x_train, y_train, x_test, y_test):\n",
    "\n",
    "    # Initialize A and B as nn.Linear layers\n",
    "    A = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "    B = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "   \n",
    "    # Set B's weights to zero (the default initialization of A is fine)\n",
    "    B.weight.data.zero_()\n",
    "    print(f\"A has {parameter_count_string(A)} parameters, B has {parameter_count_string(B)} parameters\")\n",
    "\n",
    "    # Create a list of the parameters we want to update at each step\n",
    "    optimization_parameters = list(A.parameters()) + list(B.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(optimization_parameters, lr=1e-3)\n",
    "\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Add the LoRA adapter output to the prediction\n",
    "        y_train_pred = W(x_train) + A(x_train) @ B.weight.T\n",
    "\n",
    "        train_loss = F.mse_loss(y_train_pred, y_train)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Add the LoRA adapter output to the prediction\n",
    "            y_test_pred = W(x_test) + A(x_test) @ B.weight.T\n",
    "            test_loss = F.mse_loss(y_test_pred, y_test)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}\")\n",
    "\n",
    "W_copy = deepcopy(W)\n",
    "lora_finetune(W, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your solution to Exercise 8, we will use the  ð¤ HuggingFace Parameter-Efficient Fine-Tuning (PEFT) [library](https://huggingface.co/docs/peft/index), which allows you to easily put LoRA into some pre-specified parts of an `nn.Module`. If your solution works, it should produce very similar results (converged losses within $10^{-3}$) to the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m W_wrapper = \u001b[43mnn\u001b[49m.Sequential(deepcopy(W))\n\u001b[32m      2\u001b[39m W_wrapper\n\u001b[32m      4\u001b[39m lora_config = peft.LoraConfig(\n\u001b[32m      5\u001b[39m     task_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      6\u001b[39m     inference_mode=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     target_modules=\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "W_wrapper = nn.Sequential(deepcopy(W))\n",
    "W_wrapper\n",
    "\n",
    "lora_config = peft.LoraConfig(\n",
    "    task_type=None,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=4,\n",
    "    target_modules=\"0\"\n",
    ")\n",
    "\n",
    "W_lora = peft.get_peft_model(W_wrapper, lora_config)\n",
    "W_lora.print_trainable_parameters()\n",
    "\n",
    "full_finetune(W_lora, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large models, we can train a separate LoRA for each linear map in the model. We can in fact make the adapted model use the exact same amount of compute as the base model by simply adding the matrices $W_{lora} = W_{base} + BA$ at the end of training.\n",
    "\n",
    "One great thing is that HuggingFace contains many LoRA adapters that have been pre-finetuned, and these adapters are relatively lightweight as the $A$ and $B$ matrices are often much smaller than the base model layers. [Here's one](https://huggingface.co/TheLastBen/Papercut_SDXL) which adds a papercut effect to the generation. Let's try generating a papercut-style model of the ETH Zurich main building using the original model, and then compare it with the one generated using the LoRA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"papercut ETH Zurich main building\"\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt).images[0]\n",
    "    \n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.load_lora_weights(\"TheLastBen/Papercut_SDXL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LoRA is trained to be activated when the prompt contains \"papercut\"\n",
    "prompt = \"papercut Gobbledigoober\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = pipe(prompt).images[0]\n",
    "    \n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
