{"cells":[{"cell_type":"markdown","metadata":{"id":"VS_b7lkiuFSm"},"source":["Notebook created by [Peter Belcák](https://disco.ethz.ch/members/pbelcak)\n","\n","<br>\n","\n","Updated:<br>\n","\n","For FS24 by [Andreas Plesner](https://disco.ethz.ch/members/aplesner)\n","\n","For HS24, FS25 by [Till Aczel](https://disco.ethz.ch/members/taczel)\n"]},{"cell_type":"markdown","metadata":{"id":"FPXC6lGgqhJ1"},"source":["# Hand-On Deep Learing - Introduction Session\n","\n","This session will introduce you to the basic concepts of differentiable programming and traning neural networks from scratch. We will be using the popular deep learning framework PyTorch.\n"]},{"cell_type":"markdown","metadata":{"id":"TM0WBd5RpGGa"},"source":["We will first talk about the bread-and-butter data type of deep learning - tensors. Once done, we will give a quick introduction to differentiable programs. On a simple example, we will show how to compute gradients in pytorch, and then lead you towards an implementation of a simple gradient descent training loop.\n","\n","Having got a better feel of how one trains the parameters of a differentiable program, we will introduce neural networks. We will provide you with a full implementation of a training loop, and help you train an approximation of a Boolean function.\n","\n","We will then steer away from illustrative examples and get started with practical, application-oriented deep learning. We will define and train both shallow and deep neural networks for image classification, tune the parameters of training and the sizes of architectures, and observe how the individual properties of our training setup influence the quality of the learning outcomes.\n","\n","We will conclude this session with the introduction of convolutional layers. The challenge of the day will be to use convolutional neural networks to correctly classify greyscale images of fashion articles."]},{"cell_type":"markdown","metadata":{"id":"kWg7Abwlot8z","jp-MarkdownHeadingCollapsed":true},"source":["## Prelude: Tensors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2y1UqizqeUr"},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","metadata":{"id":"SsArlqzXFU1X"},"source":["For training of deep models, `torch` uses a special data type: `torch.tensor`. The `torch.tensor` is `torch`'s way to store tensors which can be seen as multidimensional arrays, with the vector and matrix simply being the 1 and 2 dimensional instances. The values in `torch.tensor` can be learned from data.\n","\n","Before we can move on to do anything more exciting, one has to know a bit about tensors. Anyone familiar with `numpy.ndarray` can skip to the last subsection on trainable tensors."]},{"cell_type":"markdown","metadata":{"id":"ggXYIWqrF5gi"},"source":["#### Tensors are an enhanced, uniform variant of multi-dimensional lists that torch operations can eat."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gI8gFtZwGOHy"},"outputs":[],"source":["A = [[ 0.0, 1.0], [ 1.0 , 0.0]]\n","\n","try:\n","  torch.matmul(A, A) # throws a TypeError\n","except TypeError as error:\n","  print(f\"An error occured in {error}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qkXDlE2WGfCY"},"outputs":[],"source":["A_tensor = torch.tensor(A)\n","\n","torch.matmul(A_tensor, A_tensor)"]},{"cell_type":"markdown","metadata":{"id":"o9veRCZCKlqp"},"source":["Notice that nested lists that are candidates for tensors must be uniform in every dimension"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLXm_LvnLdGC"},"outputs":[],"source":["B = [\n","    [[1, 2, 3], [4, 5, 6]],\n","    [[0, 0], [1, 1]]\n","]\n","\n","\n","try:\n","  B_tensor = torch.tensor(B) # throws a ValueError\n","except ValueError as error:\n","  print(f\"Could not form a tensor: {error}\")"]},{"cell_type":"markdown","metadata":{"id":"sLtbSji2NXIB"},"source":["**Exercise.** Create a tensor `I_tensor`, which is a 3x3 identity matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4a53MyvNnOZ"},"outputs":[],"source":["# Write your code here\n","\n","I = None\n","\n","I_tensor = None"]},{"cell_type":"markdown","metadata":{"id":"8N_bgQbmHt5x"},"source":["#### Tensors have a multi-dimensional `size`, also known as `shape`\n","The shape of a tensor describes the sizes of its individual tensor dimensions (also known as *axes*)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xp9ZCybhH9Fl"},"outputs":[],"source":["A_tensor.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNgPKohDIBZm"},"outputs":[],"source":["A_tensor.size()"]},{"cell_type":"markdown","metadata":{"id":"x2rzIVf0IRDl"},"source":["Notice that `A` consists of two lists containing two elements each.\n","Correspondingly, `A_tensor` has size of `[2, 2]`, meaning that `A_tensor` consists of to sub-tensors, namely `A_tensor[0]` and `A_tensor[1]`, containing two elements each."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9UQIDJMI7IZ"},"outputs":[],"source":["A_tensor[0]"]},{"cell_type":"markdown","metadata":{"id":"HqgN1NydJJfC"},"source":["If we take `B` such that `B` contains two lists of lists, such as in"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAJfXvqlIZra"},"outputs":[],"source":["B = [\n","    [[1, 2, 3], [4, 5, 6]],\n","    [[0, 0, 0], [1, 1, 1]]\n","]"]},{"cell_type":"markdown","metadata":{"id":"Hd4A95w0JSHw"},"source":["Then we can turn it into a `B_tensor` of size `[2,2,3]`,"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6KrSZ3aJkzt"},"outputs":[],"source":["B_tensor = torch.tensor(B)\n","B_tensor.shape"]},{"cell_type":"markdown","metadata":{"id":"6drfln9LJrRW"},"source":["... meaning that `B_tensor` consits of two two-dimensional sub-tensors, `B_tensor[0]`, and `B_tensor[1]`."]},{"cell_type":"markdown","metadata":{"id":"H1IUoaVnNwL1"},"source":["**Exercise.** What should be the shape of `I_tensor`?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBaWraIFN0xF"},"outputs":[],"source":["I_tensor_shape_intended = torch.Size( [  ] ) # modify this line with your guess"]},{"cell_type":"markdown","metadata":{"id":"7-KzK5SbOMG2"},"source":["**Exercise.** Retrieve the shape of `I_tensor`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9khgv9NORPi"},"outputs":[],"source":["I_tensor_shape = None # modify this line with the correct code"]},{"cell_type":"markdown","metadata":{"id":"PujSmcTuOXVE"},"source":["**Exercise.** Are they the same?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kNlLXAAQOgj9"},"outputs":[],"source":["# Run this code block.\n","\n","if I_tensor_shape_intended == I_tensor_shape:\n","  print(\"I_tensor has shape as intended.\")\n","else:\n","  print(\"I_tensor does not have shape as intended.\")"]},{"cell_type":"markdown","metadata":{"id":"h5QiVwQxKJ_R"},"source":["#### You can access arbitrary sub-tensors of every tensor\n","\n","For this, you can use the python's usual slicing notation. For example, to get the second element of each of the deepest lists of `B` in the corresponding tensor, one can simply write"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1PnNsaaL_Ij"},"outputs":[],"source":["B_tensor[:,:,1]"]},{"cell_type":"markdown","metadata":{"id":"9CoaVWTtHjN1"},"source":["#### Tensors can be used in computations element-wise, as long as the dimensions match"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdFQUCmzHijV"},"outputs":[],"source":["B_tensor + B_tensor ** 2 - 0.3 * B_tensor"]},{"cell_type":"markdown","metadata":{"id":"Z5Ndj7QtUitM"},"source":["**Exercise.** With the help of PyTorch documentation online, find the square root of $B^3$. All operations are applied element-wise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WmGpu-sjZrNQ"},"outputs":[],"source":["# your solution\n"]},{"cell_type":"markdown","metadata":{"id":"uaFKUKGsGnmC"},"source":["#### Tensors can be either trainable or non-trainable\n","\n","The trainable tensors are the ones that have `require_gradient` set to `True`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UZrpvNetHHks"},"outputs":[],"source":["A_trainable = torch.tensor(A, requires_grad=True)\n","\n","print(f\"A_tensor is trainable: {A_tensor.requires_grad}\")\n","print(f\"A_trainable is trainable: {A_trainable.requires_grad}\")"]},{"cell_type":"markdown","metadata":{"id":"U6JZJgF0yR5o","jp-MarkdownHeadingCollapsed":true},"source":["## Differentiable Functions and Why They Are So Special\n","\n","The entire world is now interested in a particular sub-class of functions, called *differentiable functions*."]},{"cell_type":"markdown","metadata":{"id":"11getKqlvbJE"},"source":["A differentiable functions is a function $f$ such that $f$ is differentiable with respect to its parameters. Here is an example of a differentiable function $f$ taking $x$ as input and multiplying it by a parameter $p$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"958evPdf2rA7"},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0fCXWtzv6et"},"outputs":[],"source":["p = torch.tensor([ 1.0 ], requires_grad=True)\n","\n","def f(x):\n","  global p\n","  return p * x"]},{"cell_type":"markdown","metadata":{"id":"AEV5JmM5v8IG"},"source":["Apart from forcing software engineers to dust off their high-school calculus knowledge, what are these differentiable programs actually good for? Why has the entire software engineering and data science world gone crazy over them?"]},{"cell_type":"markdown","metadata":{"id":"MWwo3vb00uaO"},"source":["We won't keep you in suspense, here's the \"secret\":\n","\n","> Given input-output data, differentiable functions can be taught, through trial and error, to use the right parameters.\n","\n","So, in the example of `f` above, we could train the function to learn the best value of `p`."]},{"cell_type":"markdown","metadata":{"id":"mfHkDnBA1km6"},"source":["The method enabling this is known as gradient descent training. This has been covered well in many lectures and online resources. If you need a quick refresher, have a look through at the corresponding videos from the Computational Thinking course, available [here](https://disco.ethz.ch/courses/hs24/coti/). The short version is that we can update the parameter `p` using the update:\n","\n","$p_{new} = p - ∇_pf(x)$\n","\n","where $∇_p f(x)$ is the gradient of `f` with respect to `p` for the data `x`."]},{"cell_type":"markdown","metadata":{"id":"S4gmO1GKilew"},"source":["### Gradient Computation in PyTorch\n","At the helm of gradient-descent training in PyTorch is the `autograd` module. `torch.autograd` is PyTorch's automatic differentiation engine that powers gradient-descent training.\n","\n","Suppose you take some trainable tensor $x$ and pass it through $f$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ovOmObj-aCnr"},"outputs":[],"source":["x = torch.tensor([ 5.1 ], requires_grad=True)\n","output = f(x)\n","output"]},{"cell_type":"markdown","metadata":{"id":"QcDXcjV3aTK5"},"source":["Notice that `output` now has an additional field, `grad_fn`, that was set by the `autograd` system to keep track of what operations have been performed on `x` to arrive at output."]},{"cell_type":"markdown","metadata":{"id":"DKlQaJ9Ialbh"},"source":["Now, given some expected output for $f(x)$, say $1$, `autograd` allows you to compute an indication of how `p` needs to be changed in order for $f(x)$ to eventually yield the correct outputs. This is done in a process called *backward pass*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6nNrPt4bLbh"},"outputs":[],"source":["# define the output we expect. I.e. the output of f(x) with the correct p\n","expected_output = torch.tensor([1.0])\n","# compute the loss (how \"bad\" that output is for the current value of p)\n","loss = (output - expected_output) ** 2\n","# compute the gradient of f w.r.t. p\n","loss.backward()"]},{"cell_type":"markdown","metadata":{"id":"M6I-wj_QbV83"},"source":["This indication can then by inspected by asking `p` what its gradient is by reading `p.grad`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YanreQybVVa"},"outputs":[],"source":["p.grad"]},{"cell_type":"markdown","metadata":{"id":"2vNxdfQgcNcq"},"source":["This indication can be interpreted as\n","\n","> Decreasing `p` by some small $\\epsilon$ will decrease the loss by $41.82\\epsilon$."]},{"cell_type":"markdown","metadata":{"id":"xVLVUzhpjQOS"},"source":["And, as you already know, leveraging the negation of gradient as the indication of the direction in which one should modify the parameters in order to descent towards lower values of the loss, gradient descent training is simply the routine under which one iteratively computes and then applies the gradient of the loss function with respect to parameters of the computation to minimise the loss."]},{"cell_type":"markdown","metadata":{"id":"s93DpbnznITx"},"source":["**Exercise.** Fill in the code below to compute the gradients for `p` equal to `0.75`, `0.5`, `0.25`, `0.20` and `0.10`. What do you observe?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZoWVAgtCnlT6"},"outputs":[],"source":["# case p = 0.75\n","p = torch.tensor([ 0.75 ], requires_grad=True)\n","# you want something gradienty here - remember the backwards call :)\n","loss = None\n","\n","print(f\"For p = 0.75, the gradient is {p.grad}\")\n","\n","\n","# case p = 0.50\n","p = torch.tensor([ 0.50 ], requires_grad=True)\n","# also here\n","\n","print(f\"For p = 0.50, the gradient is {p.grad}\")\n","\n","\n","# case p = 0.25\n","p = torch.tensor([ 0.25 ], requires_grad=True)\n","# ...\n","\n","print(f\"For p = 0.25, the gradient is {p.grad}\")\n","\n","\n","# case p = 0.20\n","p = torch.tensor([ 0.20 ], requires_grad=True)\n","# ...\n","\n","print(f\"For p = 0.20, the gradient is {p.grad}\")\n","\n","\n","# case p = 0.10\n","p = torch.tensor([ 0.10 ], requires_grad=True)\n","# last one\n","\n","print(f\"For p = 0.10, the gradient is {p.grad}\")"]},{"cell_type":"markdown","metadata":{"id":"33PSV9YLnxy4"},"source":["### A Basic Training Loop\n","As hinted on by the above example, modifying the parameters of a differentiable program in the direction opposite to its gradient (i.e. in the direction in which the loss decreases most rapidly) generally guides the differentiable programs towards a minimum in the loss.\n","\n","This process can be repeated iteratively, to form what is called a *training loop*. A typical training procedure of a differentiable program looks as follows:\n","\n","\n","1. Initialise the parameters of the differentiable program according to an appropriate scheme.\n","2. Take the inputs provided and perform a *forward pass* -- apply the program to the inputs.\n","3. Compute the loss between the expected outputs and the actual outputs of the program.\n","4. Compute the gradient of the loss with respect to the program's parameters.\n","5. Scale the gradients by the desired pace of descent -- *the learning rate* -- and update the parameters accordingly.\n","6. Repeat step 2 to 5 for a number of rounds or until the loss is sufficently small.\n","\n","\n","You already possess all the basic ingredients necessary to implement such a training procedure yourself. Let's do that."]},{"cell_type":"markdown","metadata":{"id":"1zTTMnCiUOph"},"source":["#### Training loop without PyTorch"]},{"cell_type":"markdown","metadata":{"id":"J3G-phKdX0d0"},"source":["To get a feeling for how we can optimize the parameter `p` to some data we will start without the help PyTorch gives. We will still use PyTorch for tensor operations, but this could also be done using NumPy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcgtJtJIUUHV"},"outputs":[],"source":["def f(x, p):\n","    return x * p\n","\n","def grad_f(x, p):\n","    return x\n","\n","def loss_function(x, p, y):\n","    y_hat = f(x, p)\n","    errors = y_hat - y\n","    loss = torch.mean(errors**2)\n","    return loss\n","\n","def grad_loss_function(x, p, y):\n","    y_hat = f(x, p)\n","    grad_y_hat = grad_f(x, p)\n","    errors = y_hat - y\n","    grad_errors = grad_y_hat\n","    grad_loss = torch.mean( 2*errors*grad_errors )\n","    return grad_loss\n","\n","\n","def training_loop(x, y, learning_rate: float = 0.1, number_of_iterations: int = 100):\n","    p = 0.9\n","\n","    for iteration in range(number_of_iterations):\n","        # compute the gradient\n","        grad_loss = grad_loss_function(x, p, y)\n","\n","        # log the result every 10th iteration\n","        if iteration%10 == 0:\n","            print(f\"iteration: {iteration:3}, p: {p:5.3f}, gradient: {grad_loss}\")\n","\n","        # update p\n","        p = p - learning_rate * grad_loss\n","\n","    return p"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2rqqDO8axUt"},"outputs":[],"source":["p_true = 0.2 # the parameter value of p to be learned\n","datapoint_count = 10 # the number of datapoints to use for training in every iteration of the training loop\n","\n","# Generate some artifical data. (datapoint_count, ) gives a one-dimensional array of length datapoint_count.\n","x = torch.rand((datapoint_count, ), requires_grad=False)\n","y = x * p_true\n","\n","p_found = training_loop(x, y, learning_rate=0.1)\n","print(f\"The true value is {p_true}, the value learned by gradient descent is {p_found}\")"]},{"cell_type":"markdown","metadata":{"id":"z1YSJDPigASP"},"source":["**Exercise.** Play around with the number of iterations and learning rate. What happens if the learning rate is very high (>5) or very small (<1e-6)?\n"]},{"cell_type":"markdown","metadata":{"id":"YO7Q4H3OUUrG"},"source":["#### Training loop with PyTorch"]},{"cell_type":"markdown","metadata":{"id":"_nYngzhdoCRt"},"source":["We need to define the function using PyTorch's syntax.\n","A function in PyTorch is a `class` with a `forward` function.\n","Forward defines what should happen when you say `f(x)` and the building blocks\n","are the functions in `torch.nn`, e.g., `nn.Linear`. The next section will look at this in details"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dn1YONH9lr5r"},"outputs":[],"source":["\n","class f_function(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # Make a linear function (y=ax+b) from input to output.\n","        # bias=False means b=0, so we get y = p*x\n","        self.p = nn.Linear(1, 1, bias=False)\n","\n","        # set p to a fixed value\n","        # the torch.no_grad() removes gradient computations, so p.grad does not change\n","        with torch.no_grad():\n","            self.p.weight[0][0] = 0.9\n","\n","    def forward(self, x):\n","        return self.p(x)\n","\n","def train_f(x, y, learning_rate: float = 0.1, number_of_iterations: int = 100):\n","    # define the function\n","    f = f_function()\n","    # define the loss function as the mean squared loss\n","    loss_fn = nn.MSELoss()\n","    # define the optimizer as classic gradient decent\n","    optimizer = torch.optim.SGD(f.parameters(), lr=learning_rate)\n","\n","    for iteration in range(number_of_iterations):\n","        # perform a \"forward pass\" (apply f to x)\n","        y_hat = f(x)\n","\n","        # compute the loss\n","        loss = loss_fn(y_hat, y)\n","\n","        # zero the gradients computed in the previous step\n","        optimizer.zero_grad()\n","\n","        # calculate the gradients of the parameters of the net\n","        loss.backward()\n","\n","        # use the gradients to update the weights of the network\n","        optimizer.step()\n","\n","    # return p\n","    return f.p.weight[0][0]"]},{"cell_type":"markdown","metadata":{"id":"4mmw4UKFl5Kh"},"source":["**Exercise.** Complete the code below and compare the results to the implementation without PyTorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xA81E0Ll1N5"},"outputs":[],"source":["# pytorch needs the shape to be an array of shape (datapoint_count, 1)\n","x_pytorch = x.reshape( TODO )\n","y_pytorch = y.reshape( TODO )\n","\n","p_found = train_f(x_pytorch, y_pytorch, learning_rate=0.1)\n","print(f\"The true value is {p_true}, the value learned by gradient descent is {p_found}\")"]},{"cell_type":"markdown","metadata":{"id":"3PXg-iPLsNLV"},"source":["## Introducing Neural Networks\n","Neural networks are a particular class of differentiable programs, for which it has been theoretically proven that they can learn to approximate an arbitrary integrable function arbitrarily well, as long as they are given enough *representational power*.\n","\n","*Here is where the deep learning black magic begins.*\n","\n","Classical programs consist of a sequence of specific operations such as addition or conditional value assignment. Neural networks are differentiable programs that consist of a sequence of amenable elementary building blocks, traditionally referred to as *layers*, that can ultimately perform a wide variety of operations. The \"bigger\" these layers are, the more complex behaviour they can learn to exhibit.\n","\n","There exists several popular types of neural network building blocks, including the trainable *linear layer*, or the non-trainable *activation*, *softmax*, and *dropout layers*, to name but a few. The combination of a linear layer and an activation layer is sometimes referred to as *dense layer* and is the basic building block of a *deep neural network*.\n","\n","The amount of *representational power* network has is determined by the sizes of its trainable layers. Linear layers have a \"width\" (the number of constituent neurons). The wider the layer, the more fine-grained operation it is capable of representing. Whether it can learn to represent this operation is, however, an entirely different question."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SlXTgnk52rA_"},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","metadata":{"id":"oSgEwtUzQSEq"},"source":["### Constructing Neural Networks\n","Without further ado, let use these building blocks to form neural networks.\n","\n","Knowing the format of the operation of individual layers, you could go ahead and implement them manually. To avoid uncanny detail, we will instead use the ready-made implementations of these layers from the `torch.nn` module.\n","\n","In general, the [documentation](https://pytorch.org/docs/stable/nn.html) of the `torch.nn` module is what you want to turn to to understand a new layer type.\n","\n","We walk you through creating instances of various layer types in the code below. We directly use the instances to operate input data."]},{"cell_type":"markdown","metadata":{"id":"CX5xxzB1vXwi"},"source":["#### Linear Layers\n","\n","Let us begin with the most basic layer in deep learning, the Linear layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dUDE6jjQRQW"},"outputs":[],"source":["# construct a linear layer that takes a tensor of size (3,) and produces a\n","#  tensor of size (5,)\n","linear_layer = torch.nn.Linear(3, 5)\n","\n","# pass [1, 2, 3] through the layer\n","example_input = torch.tensor([ 1, 2, 3 ], dtype=torch.float)\n","linear_layer(example_input)"]},{"cell_type":"markdown","metadata":{"id":"sqrHdPdHvehw"},"source":["Notice that as promised in the call to `nn.Linear(3, 5)`, the output tensor has 5 entries. Its output values are the result of an internal state (the layer *weights*) that has been initialised with random weights."]},{"cell_type":"markdown","metadata":{"id":"3SQI0UMFwxj2"},"source":["#### Activation Layers\n","\n","Several types of activation layers exist, most notably the logistic sigmoid, rectified linear unit (ReLU), and the hyperbolic tangent. Each of these has a layer in `torch.nn`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7tTJxVGu1wS"},"outputs":[],"source":["# construct a ReLU layer\n","relu_layer = torch.nn.ReLU()\n","\n","# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the ReLU layer\n","example_input = torch.tensor([ -3, -2, -1, 0, +1, +2, +3 ], dtype=torch.float)\n","relu_layer(example_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"st3FTSinzry1"},"outputs":[],"source":["# construct a sigmoid layer\n","sigmoid_layer = torch.nn.Sigmoid()\n","\n","# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the sigmoid layer\n","sigmoid_layer(example_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHPmBuUdz4xL"},"outputs":[],"source":["# construct a tanh layer\n","tanh_layer = torch.nn.Tanh()\n","\n","# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the tanh layer\n","tanh_layer(example_input)"]},{"cell_type":"markdown","metadata":{"id":"tvrPbIhrz7J1"},"source":["As you can see, going from minus infinity towards infinity around 0, the ReLU transits from constant 0 to linear behaviour at 0, the logistic sigmoid proceeds to climb from 0 towards 1, and tanh climbs from -1 towards +1."]},{"cell_type":"markdown","metadata":{"id":"qLC-6JYK0Oji"},"source":["#### Dropout Layers\n","\n","It is sometimes to the advantage of model training to \"drop out\" some of the incoming values at random. To this end, `torch.nn` provides the `Dropout` layer, which can be parameterised at construction with the probability of an input value being dropped out."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8PLHSgNbz54s"},"outputs":[],"source":["# construct a dropout layer with probability 0.5\n","dropout_layer = torch.nn.Dropout(p=0.5)\n","\n","# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the dropout layer\n","dropout_layer(example_input)\n","\n","# with p=0.5, roughly half of the inputs should be dropped out on average,\n","#  and the remaining outputs are scaled up by 1/(1-p) == 2\n","# run this snippet multiple times to observe the effects of random dropout"]},{"cell_type":"markdown","metadata":{"id":"uri42H-24L80"},"source":["#### Softmax\n","Sometimes we wish to interpret an $n$-dimensional vector of real values as scores in favour of a single one of $n$ discrete elements possessing a certain property. To this end, we often use the \"softmax\" layer.\n","\n","The softmax layer takes the $n$-dimensional vector of real values and produces an $n$-dimensional vector of values between $0$ and $1$, whose individual entries sum up to $1$.\n","\n","The bigger an entry of the input vector is relative to other entries, the closer its corresponding value in the output vector is to $1$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lv4tVwYK3WZc"},"outputs":[],"source":["# construct a softmax layer\n","softmax_layer = torch.nn.Softmax(dim=0)\n","\n","# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the softmax layer\n","softmax_layer(example_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rvmT75wPoiYq"},"outputs":[],"source":["# pass [ -1, 2, 5, 100 ] through the softmax layer\n","softmax_layer(torch.tensor([ -1, 2, 5, 100 ], dtype=torch.float))"]},{"cell_type":"markdown","metadata":{"id":"3hF_z6JC4M_V"},"source":["### Putting the Layers Together -- Implementing a DNN\n"]},{"cell_type":"markdown","metadata":{"id":"CW_XVRYLuLJG"},"source":["Neural networks are graphs of layers. In PyTorch, we generally tend to implement our neural networks as classes whose constructors construct the constituent parts of the network, and whose `forward` function passes the data through these parts.\n","\n","Below is an example implementation of a shallow neural network. This network is *shallow* as it contains only one hidden trainable layer (=layer that is not an input or output layer).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8boLlg2HwJhg"},"outputs":[],"source":["class ShallowNeuralNet(nn.Module):\n","    def __init__(self, input_width: int, hidden_layer_width: int, output_width):\n","        super().__init__()\n","        self.hidden_layer = nn.Linear(input_width, hidden_layer_width)\n","        self.hidden_relu = nn.ReLU()\n","        self.output_layer = nn.Linear(hidden_layer_width, output_width)\n","\n","    def forward(self, input):\n","        hidden_trainable_output = self.hidden_layer(input)\n","        hidden_relu_output = self.hidden_relu(hidden_trainable_output)\n","        output = self.output_layer(hidden_relu_output)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"Le9mkI0Lwz0i"},"source":["Once the network behaviour has been described in this fashion, we can create an instance of the entire network at once and use it to process data in exactly the same way as we would use layers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjQx_ztbwzGG"},"outputs":[],"source":["shallow_nn_instance = ShallowNeuralNet(5, 10, 2)\n","\n","example_input = torch.ones(5)\n","shallow_nn_instance(example_input)"]},{"cell_type":"markdown","metadata":{"id":"AxG85zD10uCD"},"source":["**Exercise.** Complete the forward pass below to arrive at an implementation of a deep ReLU neural networks with layer profile given by a list of integers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIP3GTxM0tZF"},"outputs":[],"source":["class DeepNeuralNet(nn.Module):\n","  def __init__(self, input_width, hidden_layer_profile, output_width, output_activation=None):\n","    super().__init__()\n","    self.layers = nn.ModuleList()\n","\n","    # create the first hidden layer\n","    self.layers.append(nn.Linear(input_width, hidden_layer_profile[0]))\n","    self.layers.append(nn.ReLU())\n","\n","    # create the internal hidden layers\n","    for in_width, out_width in zip(hidden_layer_profile[0:-1], hidden_layer_profile[1:]):\n","      self.layers.append(nn.Linear(in_width, out_width))\n","      self.layers.append(nn.ReLU())\n","\n","    self.layers = nn.Sequential(*self.layers)\n","\n","    # create the output layer\n","    self.output_layer = nn.Linear(hidden_layer_profile[-1], output_width)\n","    self.output_activation = nn.Identity() if not output_activation else output_activation\n","\n","  def forward(self, input):\n","    x = input\n","\n","    # loop through the layers to produce the output of the hidden network\n","    # for layer in self.layers:\n","      # TODO: pass the intermediate output of the previous layer through the current layer\n","    x = None\n","\n","    # TODO: produce the output of the network from the intermediate output of the last hidden layer\n","    output_before_activation = None\n","\n","    # TODO: engage the optional activation in self.output_activation on the output_before_activation\n","    output = None\n","\n","    return output"]},{"cell_type":"markdown","metadata":{"id":"5ctV1s9_2lw_"},"source":["**Exercise.** Test the class for generic deep neural networks below. Does everything work as expected?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApoMITVs2wLd"},"outputs":[],"source":["# try passing a tensor of random numbers through the network\n","input1 = torch.rand((10,))\n","deep_nn_instance = DeepNeuralNet(input_width=10, hidden_layer_profile=[10, 7, 5], output_width=1)\n","\n","deep_nn_instance(input1)"]},{"cell_type":"markdown","metadata":{"id":"HqrY3Z77-TYq"},"source":["### A Training Loop for Neural Networks\n","In a previous section concerning differentiable functions, we introduced the intuition for using the gradient information due to a choice of loss function to find optimal parameters of a differentiable function.\n","\n","This is exactly what we do for neural networks as well in order to train them to have the behaviour we desire of them.\n","\n","We give code for optimisation of a neural network `net` with particular loss function `loss` on dataset loaded by a `dataloader` below, and comment on it step by step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GWPK-epiM3Q"},"outputs":[],"source":["# Use the gpu if you are connected to a runtime with one.\n","# This is highly recommended, as it makes computations much faster\n","# (especially later on for the larger models)\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","def training_loop(\n","        dataloader: torch.utils.data.DataLoader,\n","        net: nn.Module,\n","        loss_fn: nn.Module,\n","        optimiser: torch.optim.Optimizer,\n","        verbosity: int=3,\n","        device = DEVICE):\n","    size = len(dataloader.dataset)\n","    last_print_point = 0\n","    current = 0\n","\n","    acc_loss = 0\n","    acc_count = 0\n","    net.train()\n","    # for every slice (X, y) of the training dataset\n","    for batch, (X, y) in enumerate(dataloader):\n","        X = X.to(device)\n","        y = y.to(device)\n","\n","        # perform a forward pass to compute the outputs of the net\n","        pred = net(X)\n","\n","        # calculate the loss between the outputs of the net and the desired outputs\n","        loss_val = loss_fn(pred, y)\n","        acc_loss += loss_val.item()\n","        acc_count += 1\n","\n","        # zero the gradients computed in the previous step\n","        optimiser.zero_grad()\n","\n","        # calculate the gradients of the parameters of the net\n","        loss_val.backward()\n","\n","        # use the gradients to update the weights of the network\n","        optimiser.step()\n","\n","        # compute how many datapoints have already been used for training\n","        current = batch * len(X)\n","\n","        # report on the training progress roughly every 10% of the progress\n","        if verbosity >= 3 and (current - last_print_point) / size >= 0.1:\n","            loss_val = loss_val.item()\n","            last_print_point = current\n","            print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n","    return acc_loss / acc_count"]},{"cell_type":"markdown","metadata":{"id":"o84sL9H2pNR2"},"source":["We now possess all the tools necessary for constructing simple neural networks and for training them towards some particular behaviour by gradient descent loss minimisation. Let us put these tools to good use."]},{"cell_type":"markdown","metadata":{"id":"7ogK_KsxYZDN"},"source":["### Learning a Boolean Function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MAPqtEvW3Kzj"},"source":["Let us consider the problem of learning a random Boolean function $f: \\left\\{ 0,1 \\right\\}^n \\to \\left\\{ 0,1 \\right\\}^n$. It might sound a bit boring at first, but bear with us, as it is a very natural and tractable example for the examination of the representational power of various types of neural networks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVoL-jQFYnDi"},"outputs":[],"source":["def make_binary_array(number: int, length: int) -> list:\n","\treturn [ (number>>k)&1 for k in range(0, length) ]\n","\n","data_x_list = []\n","\n","for i in range(0, 256):\n","  data_x_list.append(make_binary_array(i, 8))\n","\n","data_x = torch.tensor(data_x_list, dtype=torch.float)\n","data_y = torch.randint(low=0, high=2, size=(256, 8)).type(torch.float)\n","# Move data to compute device\n","data_x = data_x.to(DEVICE)\n","data_y = data_y.to(DEVICE)\n","\n","dataset = torch.utils.data.TensorDataset(data_x, data_y)"]},{"cell_type":"markdown","metadata":{"id":"5FqdC15d3gkg"},"source":["The above code generates a dataset of $(x, y)$ pairs where $x$ is any $n=8$-bit signal and $y = f(x)$, with $f$ chosen uniformly at random."]},{"cell_type":"markdown","metadata":{"id":"ZTVEVnlMBrCt"},"source":["Given these examples, can we learn a neural network that performs the function of $f$? Yes! Just run the code below"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JAUZSwWBLyd4"},"outputs":[],"source":["def testing_loop(dataloader, net, device = DEVICE):\n","  size = len(dataloader.dataset)\n","  last_print_point = 0\n","  current = 0\n","\n","  acc_correct = 0\n","  acc_count = 0\n","\n","  net.eval()\n","  # for every slice (X, y) of the training dataset\n","  with torch.no_grad():\n","    for batch, (X, y) in enumerate(dataloader):\n","        X = X.to(device)\n","        y = y.to(device)\n","\n","        # perform a forward pass to compute the outputs of the net\n","        pred = net(X)\n","\n","        # round the predictions (0 - 0.5 towards zero, >0.5 towards one)\n","        pred_rounded = torch.round(pred)\n","\n","        # compute the number of correct entries\n","        acc_correct += torch.count_nonzero(pred_rounded == y).item()\n","        acc_count += y.numel()\n","\n","\n","  return acc_correct / acc_count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgKbUBdWMAm1"},"outputs":[],"source":["from tqdm import tqdm\n","\n","def train(dataloader, net, loss_fn, optimiser, epochs, epoch_frequency=50, device=DEVICE, verbosity=3):\n","  least_loss = None\n","  if verbosity < 2:\n","    for t in tqdm(range(epochs)):\n","      mean_loss = training_loop(dataloader, net, loss_fn, optimiser, verbosity=verbosity)\n","      accuracy = testing_loop(dataloader, net)\n","      if not least_loss or mean_loss < least_loss:\n","        least_loss = mean_loss\n","  else:\n","    for t in range(epochs):\n","        if verbosity >= 3:\n","          print(f\"Epoch {t+1}\\n-------------------------------\")\n","\n","        mean_loss = training_loop(dataloader, net, loss_fn, optimiser, verbosity=verbosity)\n","        accuracy = testing_loop(dataloader, net)\n","        if not least_loss or mean_loss < least_loss:\n","          least_loss = mean_loss\n","\n","        if verbosity >= 2 and t%epoch_frequency == 0:\n","          print(f\"Epoch {t:4}: mean loss {mean_loss:.5f}, validation accuracy {accuracy:7.2%}\")\n","        if verbosity >= 3:\n","          print(\"\\n\")\n","\n","  if verbosity >= 1:\n","    print(f\"\\nTraining complete, least loss {least_loss}, final validation accuracy {accuracy:.2%}\")\n","\n","  return least_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qv36ZZ4aXkmA"},"outputs":[],"source":["net = DeepNeuralNet(input_width=8, hidden_layer_profile=[256], output_width=8, output_activation=nn.Sigmoid()).to(DEVICE)\n","\n","training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n","loss_fn = nn.BCELoss()\n","optimiser = torch.optim.Adam(net.parameters(), lr=2e-2)\n","\n","least_loss = train(training_dataloader, net, loss_fn, optimiser, epochs=400, verbosity=1)\n"]},{"cell_type":"markdown","metadata":{"id":"kkVWsgZU36Bg"},"source":["Okay, this is encouraging. We are getting a loss in the order of $10^{-3}$ (which is relatively little for mean binary cross-entropy) and 100% accuracy. You will notice that as the training progresses, the loss tends to decrease and the accuracy increases. You will also notice that our neural network has only one hidden layer of 256 neurons. But are all those neurons really necessary?\n","\n","Let's push things to an extremum and consider a network that has exactly one neuron in its hidden layer. In other words, all of the information about the input the output neurons have must be contained in exactly one activated number, and the hidden layer of such a network is an information bottleneck. All other parameters constant, what sort of loss values and accuracies will we be getting under such circumstances?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_nIrvnj98dN5"},"outputs":[],"source":["slender_net = DeepNeuralNet(input_width=8, hidden_layer_profile=[ 1 ], output_width=8, output_activation=nn.Sigmoid()).to(DEVICE)\n","\n","training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n","loss_fn = nn.BCELoss()\n","optimiser = torch.optim.Adam(slender_net.parameters(), lr=2e-2)\n","\n","least_slender_loss = train(training_dataloader, slender_net, loss_fn, optimiser, epochs=400, verbosity=1)"]},{"cell_type":"markdown","metadata":{"id":"zFLICSAFATyE"},"source":["We see that with one hidden neuron, we learn to predict the values of $f(x)$ only marginally better than a coin flip, and that this corresponds to a relatively large value of the binary cross-entropy loss.\n","\n","Okay. So there is a number of hidden neurons (256) that is sufficient for learning $f$ with 100% accuracy, and there is a number of hidden neurons (1) that is clearly insufficient to learn anything but some rough indication of the correct output.\n","\n","*   With one hidden neuron, we have starved the network of representational power to the extent that it is only slightly better than tossing a fair coin at predicting $f$.\n","*   With 256 hidden neurons, we have given the network enough representational power to learn $f$. Perhaps even too much.\n","\n","What happens in between these two extrema? And, is there a number of neurons beyond which the network fails to learn $f$ correctly but for which $f$ can still be learned?"]},{"cell_type":"markdown","metadata":{"id":"o0wKjLNG4Yrn"},"source":["**Exercise.** Find the least number of neurons $w_1$ such that the training of a shallow neural network with $w$ hidden neurons can still learn to execute $f$ with loss of at most $0.001$. Remember that you can adjust the learning rate and the number of epochs to get finer and more resource-efficient training. You can also set `verbosity=1` to avoid long listings of losses, though verbosity of above `1` might help with the investigation of whether the training losses plateau out.\n","\n","You can also play with the number of epochs and learning rate to ensure the model has finished training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATMk4IcY4vqb"},"outputs":[],"source":["w_1 = None\n","net = DeepNeuralNet(input_width=8, hidden_layer_profile=[ w_1 ], output_width=8, output_activation=nn.Sigmoid()).to(DEVICE)\n","\n","training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n","loss_fn = nn.BCELoss()\n","optimiser = torch.optim.Adam(net.parameters(), lr=2e-2)\n","least_loss = train(training_dataloader, net, loss_fn, optimiser, epochs=400, verbosity=1)"]},{"cell_type":"markdown","metadata":{"id":"j2XD3gXi5GdP"},"source":["**Exercise.** It has been theoretically proven that deep neural networks (that is, networks with more than one hidden layer) can learn the same functions as shallow neural nets while using comparatively fewer neurons and trainable weights. Can you find a `w_2`, being a minimal number of neurons sufficient to learn the function $f$ with loss of at most $0.001$, such that the `w_2` neurons can be distributed in multiple hidden layers? The number of layers you end up using is up to you.\n","\n","Hint: Decrease the learning rate and increase the number of epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uT-6dQcE6UQw"},"outputs":[],"source":["# for example, distribute over three layers\n","w_2_1 = None\n","w_2_2 = None\n","w_2_3 = None\n","\n","w_2 = w_2_1 + w_2_2 + w_2_3\n","print(f\"Using {w_2} neurons\")\n","net = DeepNeuralNet(input_width=8, hidden_layer_profile=[ w_2_1, w_2_2, w_2_3, ], output_width=8, output_activation=nn.Sigmoid()).to(DEVICE)\n","\n","training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n","loss_fn = nn.BCELoss()\n","optimiser = torch.optim.Adam(net.parameters(), lr=2e-2)\n","least_loss = train(training_dataloader, net, loss_fn, optimiser, epochs=400, verbosity=2)"]},{"cell_type":"markdown","metadata":{"id":"DtlSN3BgDObv"},"source":["### Section Takeaway\n","\n","We have seen that neural networks can be constructed as directed graphs of more elementary building blocks (shallow and deep nets).\n","\n","We have also introduced the training loop for a neural network that uses much of PyTorch machinery to perform gradient descent.\n","\n","We have used the above to train networks that learn a Boolean function. We observed that not all networks can learn all functions, and that the number of neurons and their arrangement (or, more precisely, trainable weights and their role within the network) influence the ability of a network to learn to approximate a function. Experimenting, we got the intuitive feel of the notion of *representational power*.\n","\n","Using all that has been learned, we can now go and train networks that are perhaps more suitable for real-world applications."]},{"cell_type":"markdown","metadata":{"id":"CgPbCBVh-X4A"},"source":["## Image Classification with DNNs\n"]},{"cell_type":"markdown","metadata":{"id":"e-XRALwj3OQO"},"source":["Most machine learning workflows involve working with data, creating models, optimizing model parameters, and saving the trained models. This section introduces you to a complete ML workflow implemented in PyTorch, with links to learn more about each of these concepts.\n","\n","We will use the FashionMNIST dataset to train a neural network that predicts if an input image belongs to one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, or Ankle boot."]},{"cell_type":"markdown","metadata":{"id":"3QE2rLR_3UYN"},"source":["### Working with Data\n","\n","PyTorch has two primitives to work with data: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrJOdfWg3fyO"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import Compose, ToTensor, Lambda\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"mXhX0V6x3fOv"},"source":["PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. For this tutorial, we will be using a TorchVision dataset.\n","\n","The `torchvision.datasets` module contains `Dataset` objects for many real-world vision data like CIFAR, COCO. In this tutorial, we use the `FashionMNIST` dataset. Every TorchVision `Dataset` includes two arguments: `transform` and `target_transform`, to modify the samples and labels respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlvqVUin3sJK"},"outputs":[],"source":["# Download training data from open datasets.\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=Compose([\n","      ToTensor(),\n","      Lambda(lambda x: torch.flatten(x, start_dim=0))\n","    ]),\n",")\n","\n","# Download test data from open datasets.\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=Compose([\n","      ToTensor(),\n","      Lambda(lambda x: torch.flatten(x, start_dim=0))\n","    ]),\n",")"]},{"cell_type":"markdown","metadata":{"id":"1oPKr9xq30BL"},"source":["We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJWNfm3t4wDC"},"outputs":[],"source":["batch_size = 64\n","\n","# Create data loaders.\n","train_dataloader = DataLoader(training_data, batch_size=batch_size, pin_memory=True)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, pin_memory=True)\n","\n","for X, y in test_dataloader:\n","    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n","    print(f\"Shape of y: {y.shape} {y.dtype}\")\n","    break"]},{"cell_type":"markdown","metadata":{"id":"TcuLF_FcYtQS"},"source":["We can also have a quick peek at the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"klZSCJp2YsdR"},"outputs":[],"source":["images, labels = next(iter(train_dataloader))\n","print('Shape of input tensor:', list(images.shape))\n","ii = torch.reshape(images[0],(28,28))\n","plt.imshow(ii, cmap='gray')\n","plt.show()\n","print('Label: ', int(labels[0]))"]},{"cell_type":"markdown","metadata":{"id":"q1sUIuMb37Is"},"source":["### Creating Models\n","As we have in previous sections when learning Boolean functions, to define a neural network in PyTorch we create a class that inherits from `nn.Module`. We define the layers of the network in the `__init__` function (the constructor) and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU if available."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMpr9W3b34sp"},"outputs":[],"source":["# Get cpu or gpu device for training.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")\n","\n","net = DeepNeuralNet(input_width=28 * 28, hidden_layer_profile=[512, 512], output_width=10).to(device)\n","print(net)"]},{"cell_type":"markdown","metadata":{"id":"OuUe4bYz4sKm"},"source":["### Optimising Model Parameters"]},{"cell_type":"markdown","metadata":{"id":"z5KVrwn14vwB"},"source":["As illustrated before, to train a model, we need a loss function and an optimiser."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDYCYpIM4t2n"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","optimiser = torch.optim.SGD(net.parameters(), lr=1e-3)"]},{"cell_type":"markdown","metadata":{"id":"ks-kMVbH45MH"},"source":["The training loop from above will serve us well even in our current tasks. We also check the model’s performance against the test dataset to ensure it is learning -- in order to do so, we re-define the `testing_loop` function we used to learn Boolean functions in the new context of image classification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_HRTMXS42fX"},"outputs":[],"source":["def testing_loop(dataloader, net,):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    net.eval()\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            X, y = X.to(device), y.to(device)\n","            pred = net(X)\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    return correct / size"]},{"cell_type":"markdown","metadata":{"id":"riWKYtev5MGt"},"source":["The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model's accuracy and loss at each epoch; we would like to see the accuracy increase and the loss decrease with every epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sdns1QKu5LK4"},"outputs":[],"source":["epochs = 5\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    training_loop(train_dataloader, net, loss_fn, optimiser)\n","    validation_accuracy = testing_loop(train_dataloader, net)\n","    print(f\"Validation Accuracy: {validation_accuracy:.2%}\\n\")\n","print(\"Training Done!\")\n","\n","testing_accuracy = testing_loop(test_dataloader, net)\n","print(f\"\\nTest Accuracy: {testing_accuracy:.2%}\")"]},{"cell_type":"markdown","metadata":{"id":"Owb56aZqSk4x"},"source":["We get an accuracy around 65 % which is much better than the 10 % we would get from random guessing."]},{"cell_type":"markdown","metadata":{"id":"ErIgSMFqLw_p"},"source":["**Exercise.** Play around with different learning setups. Modifying just the learning rate and the number of epochs, how high can you take the validation accuracy? While doing so, do you also observe similar improvements in the test accuracy? (Remember that you can adjust the verbosity level not to have to read through long listings)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IlxBrCs6L3tb"},"outputs":[],"source":["net = DeepNeuralNet(input_width=28 * 28, hidden_layer_profile=[512, 512], output_width=10).to(device)\n","optimiser = torch.optim.SGD(net.parameters(), lr=1e-3)\n","train(train_dataloader, net, loss_fn, optimiser, epochs=5, epoch_frequency=1, verbosity=2)"]},{"cell_type":"markdown","metadata":{"id":"Hdz2qeSpQll8"},"source":["**Exercise.** Play around with the architecture of the neural network that you train. Adding more layers and more neurons, can you take the test performance even higher than in the previous exercise?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZbTBJlKRJQu"},"outputs":[],"source":["# define the architecture of your neural network\n","best_net = DeepNeuralNet(input_width=28 * 28, hidden_layer_profile=[ None ], output_width=10).to(device)\n","optimiser = torch.optim.SGD(best_net.parameters(), lr=1e-3)\n","print(best_net)\n","\n","# train it\n","train(train_dataloader, best_net, loss_fn, optimiser, epochs, verbosity=3)\n","\n","# test it\n","testing_accuracy = testing_loop(test_dataloader, best_net)\n","print(f\"\\nTest Accuracy: {testing_accuracy:.2%}\")"]},{"cell_type":"markdown","metadata":{"id":"LPpFsu8F59xI"},"source":["### Saving and Loading Models\n","Quite often you want to save your model, either to be later deployed in practice (on a website or in a mobile device, for example), or to be able to evaluate it later, in a different workflow. A common way to save a model is to serialise the internal state dictionary (containing the model parameters)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iH2lJVkk5ROQ"},"outputs":[],"source":["torch.save(net.state_dict(), \"model.pth\")\n","print(\"Saved PyTorch Model State to model.pth\")"]},{"cell_type":"markdown","metadata":{"id":"_aei7Jih6FyU"},"source":["The process for loading a model includes re-creating the model structure and loading the state dictionary into it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pa519J6y6JR8"},"outputs":[],"source":["model = DeepNeuralNet(28 * 28, [512, 512], 10)\n","model.load_state_dict(torch.load(\"model.pth\"))"]},{"cell_type":"markdown","metadata":{"id":"aMSKxvms6ROa"},"source":["This model can now be used to make predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQFBDr7V6QOm"},"outputs":[],"source":["classes = [\n","    \"T-shirt/top\",\n","    \"Trouser\",\n","    \"Pullover\",\n","    \"Dress\",\n","    \"Coat\",\n","    \"Sandal\",\n","    \"Shirt\",\n","    \"Sneaker\",\n","    \"Bag\",\n","    \"Ankle boot\",\n","]\n","\n","model.eval()\n","x, y = test_data[0][0], test_data[0][1]\n","with torch.no_grad():\n","    pred = model(x)\n","    predicted, actual = classes[pred.argmax(0)], classes[y]\n","    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["DtlSN3BgDObv"],"gpuType":"T4","provenance":[{"file_id":"1llzSc-etNFgl0XLScm_7qtBjrlacHopW","timestamp":1706282296711}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}