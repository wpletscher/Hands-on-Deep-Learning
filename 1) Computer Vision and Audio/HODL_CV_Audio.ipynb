{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yNFz9gCvae5"
   },
   "source": [
    "Notebook created by [Frédéric Berdoz](https://disco.ethz.ch/members/fberdoz)\n",
    "\n",
    "Computer vision part originally created by [Peter Belcák](https://disco.ethz.ch/members/pbelcak)\n",
    "\n",
    "Audio part originally created by [Luca Lanzendörfer](https://disco.ethz.ch/members/lucala)\n",
    "\n",
    "Other contributor(s): [Andreas Plesner](https://disco.ethz.ch/members/aplesner), [Till Aczel](https://disco.ethz.ch/members/taczel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-g7f7xDP3Rn"
   },
   "outputs": [],
   "source": [
    "# Uncomment for Colab\n",
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSGm8ZM9Z01t"
   },
   "source": [
    "Today's session will first revisit some of the basics from the session #1 in more detail. There, we will show some tricks that may increase the speed at which you prototype deep neural networks.\n",
    "\n",
    "The overreaching aim of this session is, however, to introduce you to computer vision and audio processing, mainly through convolutional neural networks. The main benefit of these networks is that they can learn far more about pictures (and also videos or sound waves) while requiring far less representational power.\n",
    "\n",
    "If you are using Colab make sure to switch to a GPU runtime before running this notebook.\n",
    "You can do this by going to Runtime > Change runtime type > Hardware accelerator > GPU.\n",
    "\n",
    "Dig in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw1sttF5HumL"
   },
   "source": [
    "# Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-rJS3vBFEM0"
   },
   "source": [
    "Computer Vision (CV) is a field that studies how computers can gain some degree of understanding from digital images and/or video. *Understanding* in this definition has a rather broad meaning -- it can range from being able to distinguish between a cat and a dog on the picture, to more complex tasks such as describing the image in natural language.\n",
    "\n",
    "There are several common types of computer vision tasks:\n",
    "\n",
    "* **Image Classification** is the simplest kind, when we need to classify an image into one of many pre-defined categories, for example, distinguish a cat from a dog on a photograph, or recognise a handwritten digit.\n",
    "\n",
    "* **Object Detection** is a slightly more difficult task, in which we need to find known objects on the picture and localise them, that is, return the **bounding box** for each of recognised objects.\n",
    "\n",
    "* **Segmentation** is similar to object detection, but instead of giving bounding box we need to return an exact pixel map outlining each of the recognised objects.  \n",
    "\n",
    "![An image showing how computer vision object detection can be performed with cats, dogs, and ducks.](https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/images/2-image-data-1.png)\n",
    "\n",
    "Image taken from [CS224d Stanford Course](https://cs224d.stanford.edu/index.html).\n",
    "\n",
    "Here we'll focus on the **image classification** task, and on how neural networks can be used to solve it. As with any other machine learning tasks, to train a model for classifying images we’ll need a labeled dataset, that is, a large number of images for each of the classes.\n",
    "\n",
    "First, we will look at how images are represented as tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VN_s2u6ma0kR"
   },
   "source": [
    "## Images as Tensors\n",
    "\n",
    "Computer Vision works with Images. As you probably know, images consist of pixels, so they can be thought of as a rectangular collection (array) of pixels.\n",
    "\n",
    "In the first part of this tutorial, we will deal with handwritten digit recognition. We will use the MNIST dataset, which consists of grayscale images of handwritten digits, 28x28 pixels. Each image can be represented as 28x28 array, and elements of this array would denote intensity of corresponding pixel - either in the scale of range 0 to 1 (in which case floating point numbers are used), or 0 to 255 (integers). The popular python library `NumPy` is often used with computer vision tasks, because it allows to operate with multidimensional arrays (tensors) effectively.\n",
    "\n",
    "To deal with color images, we need some way to represent colors. In most cases, we represent each pixel by 3 intensity values, corresponding to Red (R), Green (G) and Blue (B) components. This color encoding is called RGB, and thus color image of size $H\\times W$ will be represented as an array of size $3\\times H\\times W$ (sometimes the order of components might be different, but the idea is the same).\n",
    "\n",
    "![Grayscale Image](https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/images/2-image-data-2.png) | ![RGB Image](https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/images/2-image-data-3.png)\n",
    "------|------\n",
    "5x5 Grayscale Image | 5x5 Color (RGB) Image\n",
    "\n",
    "Using multi-dimensional arrays to represent images also has an advantage, because we can use an extra dimension to store a sequence of images. For example, to represent a video fragment consisting of 200 colour frames/images with 800x600 dimension, we may use the tensor of size 200x3x600x800."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd8P5ueNF9dU"
   },
   "source": [
    "The following bits will remind you of the first session, in which we used FashionMNIST and MNIST datasets. Since we're familiar with these already, we'll just fly through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDpfh5yRGR4r"
   },
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWw_sidKF2jT"
   },
   "outputs": [],
   "source": [
    "# import the packages needed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgyVz-aWF8VU"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "data_train = torchvision.datasets.MNIST('./data',\n",
    "        download=True, train=True, transform=ToTensor())\n",
    "data_test = torchvision.datasets.MNIST('./data',\n",
    "        download=True, train=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoWRA7UJGObu"
   },
   "source": [
    "### Visualising the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "LslUFjXYGU2b"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,7)\n",
    "for i in range(7):\n",
    "    ax[i].imshow(data_train[i][0].view(28,28), cmap='gray')\n",
    "    ax[i].set_title(data_train[i][1])\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUQuCICTGbvU"
   },
   "source": [
    "### Dataset Structure\n",
    "\n",
    "We have a total of 60,000 training images and 10,000 testing images in the MNIST dataset. It's important to split out the data for training and testing. We shall do some data exploration to get a better idea of what our data looks like:\n",
    "\n",
    "Each sample is a tuple in the following structure:\n",
    " * First element is the actual image of a digit, represented by a tensor of shape 1x28x28\n",
    " * Second element is a **label** that specifies which digit is represented by the tensor. It is a tensor that contains a number from 0 to 9.\n",
    "\n",
    "`data_train` is a training dataset that we will use to train our model on. `data_test` is a smaller test dataset that we can use to verify our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDx0uhzqGVPd"
   },
   "outputs": [],
   "source": [
    "print('Training samples:', len(data_train))\n",
    "print('Test samples:', len(data_test))\n",
    "\n",
    "print('Tensor size:', data_train[0][0].size())\n",
    "print('First 10 digits are:', [data_train[i][1] for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETAwRijqGiXV"
   },
   "source": [
    "All pixel intensities of the images are represented by floating-point values in between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie_eiainGgoz"
   },
   "outputs": [],
   "source": [
    "print('Min intensity value: ', data_train[0][0].min().item())\n",
    "print('Max intensity value: ', data_train[0][0].max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_dCdQpVGuML"
   },
   "source": [
    "### Sidenote: Loading Your Own Images\n",
    "\n",
    "In most of the practical applications, you would have your own images located on disk that you want to use to train your neural network. In this case, you need to load them into PyTorch tensors.\n",
    "\n",
    "One of the ways to do that is to use one of the Python libraries for image manipulation, such as *Open CV*, or *PIL/Pillow*, or *imageio*. Once you load your image into numpy array, you can easily convert it to tensors.\n",
    "\n",
    "> It is important to make sure that all values are scaled to the range [0..1] before you pass them to a neural network - it is the usual convention for data preparation, and all default weight initialisations in neural networks are designed to work with this range. `ToTensor` transform that we have seen above automatically scales PIL/numpy images with integer pixel values into [0..1] range.\n",
    "\n",
    "An even easier approach is to use functionality in **torchvision** library, namely `ImageFolder`. It does all the preprocessing steps automatically, and also assigns labels to images according to the directory structure. We will see the example of using `ImageFolder` later in this notebook, once we start classifying our own cats and dogs images.\n",
    "\n",
    "> It is important to note that for training all images should be scaled to the same size. If your original images have different aspect ratios, you need to decide how to handle this scaling - either by cropping images, or by padding extra space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpV67gVjSkKP"
   },
   "source": [
    "## Revisiting Dense Neural Networks\n",
    "\n",
    "For completeness and as a baseline, we will also quickly run through some of the things you've already seen in the introductory session, just with a slightly different toolkit.\n",
    "\n",
    "Let's focus on the problem of handwritten digit recognition. It is a classification problem, because for each input image we need to specify the class -- which digit it is.\n",
    "\n",
    "In this section, we start with the simplest possible approach for image classification; a fully-connected neural network (which is also called a *perceptrons*). We will recap the way neural networks are defined in PyTorch, and how the training algorithm works. If you are already confident about those concepts, feel free to run through the cells towards the next section, where we introduce Convolutional Neural Networks (CNNs).\n",
    "\n",
    "We use `pytorchcv` helper from Microsoft to load all data we have talked about above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0PPLUMdGjkt"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9W-52-cLTscw"
   },
   "source": [
    "### Dense Neural Networks (Again)\n",
    "\n",
    "A basic **neural network** in PyTorch consists of a number of **layers**. The simplest network would include just one fully-connected layer, which is called **Linear** layer, with 784 inputs (one input for each pixel of the input image) and 10 outputs (one output for each class).\n",
    "\n",
    "<img alt=\"A graph showing how an image is broken into layers based on the pixels.\" src=\"https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/images/3-train-dense-neural-networks-1.png\" width=\"60%\"/>\n",
    "\n",
    "As we discussed above, the dimension of our digit images is $1\\times28\\times28$, i.e. each image contains $28\\times28=784$ different pixels. Because linear layer expects its input as one-dimensional vector, we need to insert another layer into the network, called **Flatten**, to change input tensor shape from $1\\times28\\times28$ to $784$.\n",
    "\n",
    "After `Flatten`, there is a main linear layer (called `Dense` in PyTorch terminology) that converts 784 inputs to 10 outputs -- one per class. We want the $n$-th output of the network to return the probability of the input digit being equal to $n$.\n",
    "\n",
    "Because the output of a fully-connected layer is not normalised to be between 0 and 1, it cannot be thought of as probability. Moreover, if want outputs to be probabilities of different digits, they all need to add up to 1. To turn output vectors into probability vector, a function called **Softmax** is often used as the last activation function in a classification neural network. For example, $\\mathrm{softmax}([-1,1,2]) = [0.035,0.25,0.705]$.\n",
    "\n",
    "> In PyTorch, we often prefer to use **LogSoftmax** function, which will also compute logarithms of output probabilities. To turn the output vector into the actual probabilities, we need to take **torch.exp** of the output.\n",
    "\n",
    "Thus, the architecture of our network can be represented by the following sequence of layers:\n",
    "\n",
    "<img alt=\"An image showing the architecture of the network broken into a sequence of layers.\" src=\"https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/images/3-train-dense-neural-networks-3.png\" width=\"90%\"/>\n",
    "\n",
    "It can be defined in PyTorch in the following way, using `Sequential` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIdMUV1cTJxQ"
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "  nn.Flatten(),\n",
    "  nn.Linear(784, 10), # 784 inputs, 10 outputs\n",
    "  nn.LogSoftmax(dim=1)\n",
    ")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fG-VA_GLT2QR"
   },
   "source": [
    "This sequence of layers is shown below in more detail. For all vectors in this diagram we also indicate tensor size.\n",
    "\n",
    "<img alt=\"An image showing the architecture of the network broken into a sequence of layers.\" src=\"https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/images/3-train-dense-neural-networks-2.png\" width=\"90%\"/>\n",
    "\n",
    "On the right hand side of this diagram we also have expected network output, represented as one-hot encoded vector. Expected output is compared with the actual output of our network using **loss function**, that gives one number -- loss -- as an output. Our goal during network training is to minimise this loss by adjusting model parameters - layer weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-fXHyubT_Hs"
   },
   "source": [
    "### Training the Network\n",
    "\n",
    "A network defined in this way can take any digit as input and produce a vector of probabilities as an output. Let's see how this network performs by giving it a digit from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aA35IhwnTrYo"
   },
   "outputs": [],
   "source": [
    "print('Digit to be predicted: ', data_train[0][1])\n",
    "torch.exp(net(data_train[0][0].to(device))).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TTMAOFbUET2"
   },
   "source": [
    "> Because we use `LogSoftmax` as final activation of our network, we pass network output through `torch.exp` to get probabilities.\n",
    "\n",
    "As you can see the network predicts similar probabilities for each digit. This is because it has not been trained on how to recognise the digits. We need to give it our training data to train it on our dataset.\n",
    "\n",
    "To train the model we will need to create **batches** from our dataset of a certain size, let's say 64. PyTorch has an object called **DataLoader** that can create batches of our data for us automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-itrEsmUBLI"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=64) # we can also use larger batch sizes for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGn6dJQXUJ8h"
   },
   "outputs": [],
   "source": [
    "def train_epoch(net, dataloader, device, lr=0.01, optimiser=None, loss_fn = nn.NLLLoss()):\n",
    "    optimiser = optimiser or torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    # Ensure the network is in training mode\n",
    "    net.train()\n",
    "    total_loss, acc, count = 0,0,0\n",
    "    for features,labels in dataloader:\n",
    "        # Ensure features/images and labels are on the correct device\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        # Predict outputs for features and compute the loss between predictions\n",
    "        # and labels\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out, labels)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Zero gradients from earlier computations\n",
    "        optimiser.zero_grad()\n",
    "        # Compute gradients for weights\n",
    "        loss.backward()\n",
    "        # Do optimiser step\n",
    "        optimiser.step()\n",
    "\n",
    "        # Get predicted classes as the entry with the highest probability\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        # Compute accuracy\n",
    "        acc += (predicted == labels).sum().cpu()\n",
    "        count += len(labels)\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "train_epoch(net, train_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LODQy_TeUQnJ"
   },
   "source": [
    "Since this function is pretty generic we will be able to use it later in our other examples. The function takes the following parameters:\n",
    "* **Neural network**\n",
    "* **DataLoader**, which defines the data to train on\n",
    "* **Loss Function**, which is a function that measures the difference between the expected result and the one produced by the network. In most of the classification tasks `NLLLoss` is used, so we will make it a default.\n",
    "* **Optimiser**, which defined an *optimisation algorithm*. The most traditional algorithm is *stochastic gradient descent*, but we will use a more advanced version called **Adam** by default.\n",
    "* **Learning rate** defines the speed at which the network learns. During learning, we show the same data multiple times, and each time weights are adjusted. If the learning rate is too high, new values will overwrite the knowledge from the old ones, and the network would perform badly. If the learning rate is too small it results in a very slow learning process.\n",
    "\n",
    "Here is what we do when training:\n",
    "* Switch the network to training mode (`net.train()`). This ensures that modules (like dropout, batch norm) that behave differently in training and inference use the correct mode.\n",
    "* Go over all batches in the dataset, and for each batch do the following:\n",
    "   - compute predictions made by the network on this batch (`out`)\n",
    "   - compute `loss`, which is the discrepancy between predicted and expected values\n",
    "   - `loss.backward()` computes the gradient\n",
    "   - try to minimise the loss by taking a step in the opposite gradient direction, therefore adjusting weights of the network (`optimiser.step()`)\n",
    "   - compute the number of correctly predicted cases (**accuracy**)\n",
    "\n",
    "The function calculates and returns the average loss per data item, and training accuracy (percentage of cases guessed correctly). By observing this loss during training we can see whether the network is improving and learning from the data provided.\n",
    "\n",
    "It is also important to control the accuracy on the test dataset (also called **validation accuracy**). A good neural network with a lot of parameters can predict with decent accuracy on any training dataset, but it may poorly generalise to other data. That's why in most cases we set aside part of our data, and then periodically check how well the model performs on them. Here is the function to evaluate the network on test dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JNkoYAgUYYO"
   },
   "outputs": [],
   "source": [
    "def validate_epoch(net, dataloader, device, loss_fn=nn.NLLLoss()):\n",
    "    # Ensure network is in evaluation mode\n",
    "    net.eval()\n",
    "    count,acc,loss = 0,0,0\n",
    "    # We use torch.no_grad() to remove gradient computations\n",
    "    # This is not necessary, but can be much faster\n",
    "    with torch.no_grad():\n",
    "        for features,labels in dataloader:\n",
    "            # Ensure features and labels are on the correct device\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Make predictions\n",
    "            out = net(features)\n",
    "            # Compute loss and accuracy of the model\n",
    "            loss += loss_fn(out,labels)\n",
    "            pred = torch.max(out,1)[1]\n",
    "            acc += (pred==labels).sum()\n",
    "            count += len(labels)\n",
    "    return loss.item()/count, acc.item()/count\n",
    "\n",
    "validate_epoch(net, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tME8w9pWUdEl"
   },
   "source": [
    "Similarly to `train` function, we return average loss and accuracy on test dataset.\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Normally when training a neural network, we train the model for several epochs observing training and validation accuracy. In the beginning, both training and validation accuracy should increase (quickly), as the network picks up the patterns in the dataset. However, at some point it can happen that training accuracy increases while validation accuracy starts to decrease. That would be an indication of **overfitting**, that is model does well on your training dataset, but not on new data.\n",
    "\n",
    "Below is the training function that can be used to perform both training and validation. It prints the training and validation accuracy for each epoch, and also returns the history that can be used to plot the loss and accuracy on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaHXVBeKUYt3"
   },
   "outputs": [],
   "source": [
    "def train(net, train_loader, test_loader, device=torch.device('cpu'), optimiser=None, lr=0.01, epochs=10, loss_fn=nn.NLLLoss()):\n",
    "    optimiser = optimiser or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    res = { 'train_loss' : [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    for ep in range(epochs):\n",
    "        tl,ta = train_epoch(\n",
    "            net=net,\n",
    "            dataloader=train_loader,\n",
    "            device=device,\n",
    "            optimiser=optimiser, lr=lr, loss_fn=loss_fn)\n",
    "        vl,va = validate_epoch(\n",
    "            net=net,\n",
    "            dataloader=test_loader,\n",
    "            device=device,\n",
    "            loss_fn=loss_fn)\n",
    "        print(f\"Epoch {ep:2}, Train acc={ta:.3f}, Val acc={va:.3f}, Train loss={tl:.3f}, Val loss={vl:.3f}\")\n",
    "        res['train_loss'].append(tl)\n",
    "        res['train_acc'].append(ta)\n",
    "        res['val_loss'].append(vl)\n",
    "        res['val_acc'].append(va)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvG5FyY1UhdA"
   },
   "outputs": [],
   "source": [
    "# Re-initialize the network to start from scratch\n",
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ").to(device)\n",
    "\n",
    "# This will take a minute or two\n",
    "hist = train(\n",
    "    net=net,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq1PbX5zUyao"
   },
   "source": [
    "This function logs messages with the accuracy on training and validation data from each epoch. It also returns this data as a dictionary (called **history**). We can then visualize this data to better understand our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwujrUjEU2FX"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(hist['train_acc'], label='Training acc')\n",
    "plt.plot(hist['val_acc'], label='Validation acc')\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "plt.plot(hist['train_loss'], label='Training loss')\n",
    "plt.plot(hist['val_loss'], label='Validation loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nv97wgawaG3Z"
   },
   "source": [
    "Discuss overfitting with yourself or your partner for two minutes.\n",
    "\n",
    "When does it happen?\n",
    "\n",
    "Why does it happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZ-O4jL6YAj7"
   },
   "source": [
    "### Visualising Network Weights\n",
    "\n",
    "The `Dense` layer in our network is also called *linear*, because it performs linear transformation of its input, which can be defined as $y=Wx+b$, where $W$ is a matrix of weights, and $b$ is bias. Weights matrix $W$ is in fact responsible for what our network can do, i.e. for recognising digits. In our case, it has size of $784\\times10$, because it produces 10 outputs (one output per digit) for an input image.\n",
    "\n",
    "Let's visualise our weights of our neural network and see what they look like. When the network is more complex than just one layer it can be a difficult to visualise the results like this, because in complex network weights do not make much sense when visualised. However, in our case each of 10 dimensions of weight matrix $W$ correspond to individual digits, and thus can be visualised to see how the digit recognition takes place. For example, if we want to see if our number is 0 or not, we will multiply input digit by $W[0]$ and pass the result through a softmax normalisation to get the answer.\n",
    "\n",
    "In the code below, we will first get the matrix $W$ into `weight_tensor` variable. It can be obtained by calling the `net.parameters()` method (which returns both $W$ and $b$), and then calling `next` to get the first of two parameters. Then we will go over each dimension, reshape it to $28\\times28$ size, and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Adcw9RIkYG9X"
   },
   "outputs": [],
   "source": [
    "weight_tensor = next(net.parameters())\n",
    "fig, axes = plt.subplots(2,5,figsize=(12,5))\n",
    "axes = axes.flatten()\n",
    "for i,x in enumerate(weight_tensor):\n",
    "    axes[i].imshow(x.view(28,28).cpu().detach())\n",
    "    axes[i].set_title(f\"Weight tensor {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4ZBq1PvYJoI"
   },
   "source": [
    "#### **Exercise 1**\n",
    "Stop for a second and try to interpret the pictures of the weight. Can you say anything about the functions of individual linear neurons? I.e., can you say anything about what sort of information they attempt to capture from the inputs? Discuss among yourselves and talk to TAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evpla-86VXCS"
   },
   "source": [
    "### Deeper Networks\n",
    "\n",
    "As you might remember, having more neurons and multiple layers of neurons relying on each other together with non-linear activations grants neural networks further *representational power* -- that is, the ability to learn more, or to learn about its input more precisely. These layers beyond the output layer are called *hidden layers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B49ZM4iAYgBh"
   },
   "source": [
    "<img alt=\"An image showing a multi-layer network with a hidden layer between the input layer and the output layer\" src=\"https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/images/3-train-dense-neural-networks-4.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y33N0rW9YvRS"
   },
   "source": [
    "Let's investigate an example deep neural network, depicted below:\n",
    "\n",
    "<img alt=\"An image showing the network layer structure as it's broken down into layers\" src=\"https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/images/3-train-dense-neural-networks-5.png\" width=\"70%\"/>\n",
    "\n",
    "This network can be quickly defined in PyTorch with the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFIBYQovU2Zw"
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(784,100),     # 784 inputs, 100 outputs\n",
    "        nn.ReLU(),              # Activation Function\n",
    "        nn.Linear(100,10),      # 100 inputs, 10 outputs\n",
    "        nn.LogSoftmax(dim=0))\n",
    "\n",
    "summary(net,input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7G9rXn0ZhoW"
   },
   "source": [
    "Here we use `summary()` function to display a detailed layer-by-layer structure of a network with some other useful information. In particular, we can see:\n",
    "\n",
    "* Layer-by-layer structure of a network, and output size of each layer\n",
    "* Number of parameters of each layer, as well as for the whole network. As an approximate rule of thumb that is also a gross oversimplification, the more parameters the network has, the more data samples it needs to be trained on without **overfitting**.\n",
    "\n",
    "Let's see how the number of parameters is calculated. First linear layer has 784 inputs and 100 outputs. The layer is defined by $W_1\\times x+b_1$, where $W_1$ has size $784\\times100$, and $b_1$ - $100$. Thus total number of parameters for this layer is $784\\times100+100 = 78500$. Similarly, number of parameters for the second layer is $100\\times10+10 = 1010$. Activation functions, as well as `Flatten` layers do not have parameters.\n",
    "\n",
    "There is another syntax that we already used in the first seession to define the same network by using Python classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zpjLuOhZhCu"
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import relu, log_softmax\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden = nn.Linear(784,100)\n",
    "        self.out = nn.Linear(100,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = self.flatten(x)\n",
    "        # Apply the hidden linear layer\n",
    "        x = self.hidden(x)\n",
    "        # Apply relu activation function\n",
    "        x = relu(x)\n",
    "        # Apply the output linear layer\n",
    "        x = self.out(x)\n",
    "        # Apply the log_softmax function to get log probabilities\n",
    "        x = log_softmax(x,dim=0)\n",
    "        return x\n",
    "\n",
    "net = MyNet()\n",
    "\n",
    "summary(net, input_size=(1,28,28),device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLKN7K2DZqzb"
   },
   "source": [
    "You can see that the structure of a neural network is the same as with the `Sequential`-defined network, but the definition is more explicit. Our custom neural network is represented by a class inherited from ``torch.nn.Module`` class.\n",
    "\n",
    "Class definition consists of two parts:\n",
    "* In the constructor (`__init__`) we define all layers that our network will have. Those layers are stored as internal variables of the class, and PyTorch will automatically know that parameters of those layers should be optimised when training. Internally, PyTorch uses `parameters()` method to look for all trainable parameters, and `nn.Module` will automatically collect all trainable parameters from all sub-modules.\n",
    "* We define the `forward` method that does the forward pass computation of our neural network. In our case, we start with a parameter tensor `x`, and explicitly pass it through all the layers and activation functions, starting from `flatten`, up to final linear layer `out`. When we apply our neural network to some input data `x` by writing `out = net(x)`, the `forward` method is called.\n",
    "\n",
    "In fact, `Sequential` networks are represented in a very similar manner, they just store a list of layers and apply them sequentially during the forward pass. Here we have a chance to represent this process more explicitly, which eventually gives us more flexibility. That is one of the reasons that using classes for neural network definition is a recommended and preferred practice.\n",
    "`Sequential` can also be used in the class definition which can make the code simpler.\n",
    "\n",
    "You can now try to train this network using exactly the same `train` function that we've defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ln-df-_nP3Rz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(hist):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(hist['train_acc'], label='Training acc')\n",
    "    plt.plot(hist['val_acc'], label='Validation acc')\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(hist['train_loss'], label='Training loss')\n",
    "    plt.plot(hist['val_loss'], label='Validation loss')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxT-y7w0Znxe"
   },
   "outputs": [],
   "source": [
    "hist = train(net, train_loader, test_loader, device=torch.device('cpu'), epochs=5)\n",
    "plot_results(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbCUN4yVvz-3"
   },
   "source": [
    "#### **Exercise 2**\n",
    "How do the results above compare to the earlier results using only a single linear layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10q1ArltI6ww"
   },
   "source": [
    "The validation accuracy increases with multiple iterations instead of decreasing. Also the validation loss does not significantly increase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UFnKEdVbK-C"
   },
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07GsEiRlbTAA"
   },
   "source": [
    "In the previous section, we have learned how to define a multi-layered neural network using class definition, but those networks were generic, and not specialised for computer vision tasks. In this section, we shall learn about **Convolutional Neural Networks (CNNs)**, which are specifically designed for computer vision.\n",
    "\n",
    "Computer vision classification problems can be viewed as a class of special cases of general classification, because when trying to find a certain object in the picture, one is scanning the image looking for some specific **patterns** and their combinations. For example, when looking for a cat, one may first look for horizontal lines, which can form whiskers, and then certain combination of whiskers can tell them that it is actually a picture of a cat. Relative position and presence of certain patterns is what plays the decisive role in this example, not their exact position on the image.\n",
    "\n",
    "To understand CNNs we need to understand convolutional layers which consist of convolutional filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLMfGSPqbfpk"
   },
   "source": [
    "### Convolutional Filters\n",
    "\n",
    "Convolutional filters are small windows that run over each pixel of the image and compute weighted sum of the neighboring pixels.\n",
    "\n",
    "<img alt=\"Sliging window over 28x28 digit image\" src=\"https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/images/4-convolutional-networks-1.png\" width=\"50%\"/>\n",
    "\n",
    "It can be understood as for each pixel in the picture you take the element-wise product between the pixel's neighbourhood and the filter and compute the sum.\n",
    "\n",
    "They are defined by matrices of weight coefficients. Let's see the examples of applying two different convolutional filters over our MNIST handwritten digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UC4cVS7rP3Rz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "def plot_convolution(t,title=''):\n",
    "    with torch.no_grad():\n",
    "        c = nn.Conv2d(kernel_size=(3,3),out_channels=1,in_channels=1)\n",
    "        c.weight.copy_(t)\n",
    "        fig, ax = plt.subplots(2,6,figsize=(8,3))\n",
    "        fig.suptitle(title,fontsize=16)\n",
    "        for i in range(5):\n",
    "            im = data_train[i][0]\n",
    "            ax[0][i].imshow(im[0])\n",
    "            ax[1][i].imshow(c(im.unsqueeze(0))[0][0])\n",
    "            ax[0][i].axis('off')\n",
    "            ax[1][i].axis('off')\n",
    "        ax[0,5].imshow(t)\n",
    "        ax[0,5].axis('off')\n",
    "        ax[1,5].axis('off')\n",
    "        #plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcXOWN-Rblt0"
   },
   "outputs": [],
   "source": [
    "plot_convolution(torch.tensor([[-1., 0., 1.], [-1., 0., 1.], [-1., 0., 1.]]), 'Vertical edge filter')\n",
    "plot_convolution(torch.tensor([[-1., -1., -1.], [0., 0., 0.], [1., 1., 1.]]), 'Horizontal edge filter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycNVVuWlbxb4"
   },
   "source": [
    "First filter is called a **vertical edge filter**, and it is defined by the following matrix:\n",
    "$$\n",
    "\\left(\n",
    "    \\begin{matrix}\n",
    "     -1 & 0 & 1 \\cr\n",
    "     -1 & 0 & 1 \\cr\n",
    "     -1 & 0 & 1 \\cr\n",
    "    \\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "When this filter goes over relatively uniform pixel field, all values add up to 0. However, when it encounters a vertical edge in the image, high spike value is generated. That's why in the images above you can see vertical edges represented by high and low values, while horizontal edges are averaged out.\n",
    "\n",
    "An opposite thing happens when we apply horizontal edge filter - horizontal lines are amplified, and vertical are averaged out.\n",
    "\n",
    "> If we apply $3\\times3$ filter to an image of size $28\\times28$ - the size of the image will become $26\\times26$, because the filter does not go over the image boundaries. In some cases, however, we may want to keep the size of the image the same, in which case image is padded with, e.g., zeros on each side.\n",
    "\n",
    "In classical computer vision, multiple filters were applied to the image to generate features, which then were used by machine learning algorithm to build a classifier. However, in deep learning we construct networks that **learn** best convolutional filters to solve classification problems.\n",
    "\n",
    "To do that, we introduce **convolutional layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10nMPZ7Mb21R"
   },
   "source": [
    "### Covolutional Layers\n",
    "\n",
    "\n",
    "\n",
    "Convolutional layers are defined using `nn.Conv2d` construction. We need to specify the following:\n",
    "* `in_channels` - number of input channels. In our case we are dealing with a grayscale image, thus number of input channels is 1. Color image has 3 channels (RGB).\n",
    "* `out_channels` - number of filters to use. We will use 9 different filters, which will give the network plenty of opportunities to explore which filters work best for our scenario.\n",
    "* `kernel_size` is the size of the sliding window. Usually 3x3 or 5x5 filters are used. The choice of filter size is usually chosen by experiment, that is by trying out different filter sizes and comparing resulting accuracy.\n",
    "\n",
    "> For convolutional layers, if you have data of shape $C \\times W \\times H$ then it is $W$ by $H$ images with $C$ channels. Colour images have three channels while gray scale images have one channel. In the hidden layers the image might have many channels.\n",
    "\n",
    "The simplest CNN will contain one convolutional layer. Given the input size 28x28, after applying nine 5x5 filters we will end up with a tensor of 9x24x24 (the spatial size is smaller, because there are only 24 positions where a sliding interval of length 5 can fit into 28 pixels). Here the result of each filter is represented by a different channel in the image (thus the first dimension 9 corresponds to the number of filters).\n",
    "\n",
    "After convolution, we flatten 9x24x24 tensor into one vector of size 5184, and then add linear layer, to produce 10 classes. We also use `relu` activation function in between layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KoIR9M7ZvxH"
   },
   "outputs": [],
   "source": [
    "class OneConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneConv, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=9, kernel_size=(5,5))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(5184, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv(x))\n",
    "        x = self.flatten(x)\n",
    "        x = nn.functional.log_softmax(self.fc(x), dim=1)\n",
    "        return x\n",
    "\n",
    "net = OneConv()\n",
    "\n",
    "summary(net, input_size=(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsZvjNajdtII"
   },
   "source": [
    "You can see that this network contains around 50k trainable parameters, compared to around 80k in fully-connected multi-layered networks. This allows us to achieve good results even on smaller datasets, because convolutional networks generalise considerably better.\n",
    "\n",
    "> Note that the number of parameters of convolutional layer is quite small, and it *does not depend on the resolution of the image*! In our case, we were using 9 filters of dimension $5\\times5$, thus the number of parameters is $9\\times5\\times5+9=234$. We omitted this in our discussion above, but convolutional filters also have a bias. Most of the parameters of our network comes from the final `Dense` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tqltkmgdrbM"
   },
   "outputs": [],
   "source": [
    "hist = train(net, train_loader, test_loader, epochs=5, device=device)\n",
    "plot_results(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE4mybD_8hMM"
   },
   "source": [
    "#### **Exercise 3**\n",
    "How does the CNN compare to the previous models in terms of overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZc49nKGKbHW"
   },
   "source": [
    "It is similar to the model with hidden layers. Overfitting is not very bad since the training acc and the validation acc increase at approximately the same rate. Only the validation loss increases tremendously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6F6lulod2uE"
   },
   "source": [
    "As you can see, we are able to achieve higher accuracy, and much faster, compared to the fully-connected networks from previous unit.\n",
    "\n",
    "We can also visualise the weights of our trained convolutional layers, to try and make some more sense of what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0coq2m4d1HF"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(3, 3, figsize=(6,6))\n",
    "ax = ax.flatten()\n",
    "with torch.no_grad():\n",
    "    p = next(net.conv.parameters())\n",
    "    for i,x in enumerate(p):\n",
    "        ax[i].imshow(x.detach().cpu()[0,...])\n",
    "        ax[i].axis('off')\n",
    "        ax[i].set_title(f'Filter {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LngUadO18zWr"
   },
   "source": [
    "#### **Exercise 4**\n",
    "What do you get out from the plot above? Discuss with yourself or your partner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4d1js_qKmhr"
   },
   "source": [
    "We can vaguely recognize that the different filters extract different patterns of the images. For example filter 7 looks for vertical lines and filter 4 for horizontal lines. But overall the filters do not make much sense to the human eye.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nTXvxQFfiLd"
   },
   "source": [
    "### Multi-Layered CNNs\n",
    "\n",
    "The section above has introduced the convolutional filters; filters that can extract patterns from images. For our MNIST classifier we used 9 5x5 filters, resulting in 9x24x24 tensor.\n",
    "\n",
    "We can use the same idea of convolution to extract higher-level patterns in the image. For example, rounded edges of digits such as 8 and 9 can be composed from a number of smaller strokes. To recognise those patterns, we can build another layer of convolution filters on top of the result of the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x29cIwCgfvde"
   },
   "source": [
    "### Pooling Layers\n",
    "\n",
    "First convolutional layers looks for primitive patterns, such as horizontal or vertical lines. Next level of convolutional layers on top of them look for higher-level patterns, such as primitive shapes. More convolutional layers can combine those shapes into some parts of the picture, up to the final object that we are trying to classify. This creates a hierarchy of extracted patterns.\n",
    "\n",
    "When doing so, we also need to apply one trick: reducing the spatial size of the image. Once we have detected there is a horizontal stoke within a sliding window, it is not so important at which exact pixel it occurred. Thus we can \"scale down\" the size of the image, which is done using one of the **pooling layers**:\n",
    "\n",
    " * **Average Pooling** takes a sliding window (for example, 2x2 pixels) and computes an average of values within the window\n",
    " * **Max Pooling** replaces the window with the maximum value. The idea behind max pooling is to detect a presence of a certain pattern within the sliding window.\n",
    "\n",
    "<img alt=\"Max Pooling\" src=\"https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/images/5-multilayer-convolutions-1.png\" width=\"50%\"/>\n",
    "\n",
    "Thus, in a typical CNN there would be several convolutional layers, with pooling layers in between them to decrease the dimensions of the images. We would also increase the number of filters, because as patterns become more advanced; there are more possible interesting combinations that we need to be looking for.\n",
    "\n",
    "![An image showing several convolutional layers with pooling layers.](https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/images/5-multilayer-convolutions-2.png)\n",
    "\n",
    "Because of decreasing spatial dimensions and increasing feature/filters dimensions, this architecture is also called **pyramid architecture**.\n",
    "\n",
    "In the next example, we will use a two-layered CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QySK67dSd4e0"
   },
   "outputs": [],
   "source": [
    "class MultiLayerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLayerCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 320)\n",
    "        x = nn.functional.log_softmax(self.fc(x), dim=1)\n",
    "        return x\n",
    "\n",
    "net = MultiLayerCNN()\n",
    "summary(net, input_size=(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JS2KzZKgCLQ"
   },
   "source": [
    "Note a few things about the definition:\n",
    "* Instead of using `Flatten` layer, we are flattening the tensor inside `forward` function using `view` function, which is similar to `reshape` function in numpy. Since flattening does not have trainable weights, it is not required that we create a separate layer instance within our class - we can just use a function from `torch.nn.functional` namespace.\n",
    "* We use just one instance of a pooling layer in our model, since it does not contain any trainable parameters, and thus one instance can be effectively reused.\n",
    "* The number of trainable parameters (~8.5K) is dramatically smaller than in previous cases (80K in Perceptron, 50K in one-layer CNN). This happens because convolutional layers in general have few parameters, independent of the input image size. Also, due to pooling, dimensionality of the image is significantly reduced before applying the final dense layer. Small number of parameters have positive impact on our models, because it helps to prevent overfitting even on smaller dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dp0Kr_RCf--1"
   },
   "outputs": [],
   "source": [
    "hist = train(net, train_loader, test_loader, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWhyyL0AgSup"
   },
   "source": [
    "#### **Exercise 5**\n",
    "Modify the above code to run the example for 10 epochs and plot the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wxkn4HyqgRMk"
   },
   "outputs": [],
   "source": [
    "hist = train(net, train_loader, test_loader, epochs=10, device=device)\n",
    "plot_results(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pW79e8tgeLR"
   },
   "source": [
    "What you should probably observe is that we are able to achieve higher accuracy, and much faster, just with 1 or 2 epochs. It means that a more sophisticated network architecture may need much less data to figure out what is going on, and to extract generic patterns from our images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pTWlXt3gjMo"
   },
   "source": [
    "### Playing with Real Images from the CIFAR-10 Dataset\n",
    "\n",
    "While our handwritten digit recognition problem may seem like a toy problem, we are now ready to do something more serious. Let's explore more advanced dataset of pictures of different objects, called [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html). It contains 60k 32x32 color images, divided into 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZNZg2xoggcW"
   },
   "outputs": [],
   "source": [
    "# We load in the image and apply this transform to them. Firstly, convert to a tensor.\n",
    "# Secondly, normalize the three channels by substracting 0.5 and dividing by 0.5.\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=14, shuffle=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=14, shuffle=False)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "av8u04ZegqM6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def display_dataset(dataset, n=10,classes=None):\n",
    "    fig,ax = plt.subplots(1,n,figsize=(15,3))\n",
    "    mn = min([dataset[i][0].min() for i in range(n)])\n",
    "    mx = max([dataset[i][0].max() for i in range(n)])\n",
    "    for i in range(n):\n",
    "        ax[i].imshow(np.transpose((dataset[i][0]-mn)/(mx-mn),(1,2,0)))\n",
    "        ax[i].axis('off')\n",
    "        if classes:\n",
    "            ax[i].set_title(classes[dataset[i][1]])\n",
    "\n",
    "display_dataset(trainset,classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUkabkYPguY9"
   },
   "source": [
    "A well-known architecture for CIFAR-10 is called [LeNet](https://en.wikipedia.org/wiki/LeNet), and has been proposed by *Yann LeCun*. It follows the same principles as we have outlined above. However, since all images are color, input tensor size is $3\\times32\\times32$, and the $5\\times5$ convolutional filter is applied across color dimension as well - meaning that the size of convolution kernel matrix is $3\\times5\\times5$.\n",
    "\n",
    "We also do one more simplification to this model - we do not use `log_softmax` as output activation function, and just return the output of last fully-connected layer. In this case we can just use `CrossEntropyLoss` loss, which is a composition of log-sfotmax and negative log lieklyhood loss function to optimise the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxozTxGogq-s"
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 120, 5)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(120, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = self.flat(x)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = LeNet().to(device)\n",
    "\n",
    "summary(net,input_size=(1,3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rT6aZWyhFlw"
   },
   "source": [
    "Training this network properly will take significant amount of time, and should preferably be done on GPUs.\n",
    "\n",
    "> In order to achieve better training results, we may need to experiment with some training parameters, such as learning rate. Thus, we explicitly define a *stochastic gradient descent* (SGD) optimiser with momentum here, and pass training parameters. You can adjust those parameters and observe how they affect training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evfr821rhD00"
   },
   "outputs": [],
   "source": [
    "# Create an optimiser using SGD with a learning rate of 1e-3 and momentum of 0.9\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "hist = train(net, trainloader, testloader, device=device, epochs=3, optimiser=opt, loss_fn=nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UkFpkwIhK_B"
   },
   "source": [
    "The accuracy that we have been able to achieve with 3 epochs of training does not seem great. However, remember that blind guessing would only give us 10% accuracy, and that our problem is actually significantly more difficult than MNIST digit classification. Getting above 50% accuracy in such a short training time seems like a good accomplishment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFmN17ithPUM"
   },
   "source": [
    "#### **Exercise 6**\n",
    "Below is a diagram comparing `LeNet` and `AlexNet` ([paper](https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)). Implement a class `AlexNet` as a subclass of `nn.Module`. The cell must result in a successful output of the network summary. Discuss your implementation with the TAs at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNZU_OLSjZm-"
   },
   "source": [
    "![LeNet vs AlexNet.](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Comparison_image_neural_networks.svg/480px-Comparison_image_neural_networks.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INDIQglfje8y"
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "  def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 96, 11, stride=4)\n",
    "        self.pool = nn.MaxPool2d(3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(96, 256, 5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(256, 384, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 384, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(384, 256, 3)                  \n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(4096, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 1000)\n",
    "  \n",
    "  def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = nn.functional.relu(self.conv4(x))\n",
    "        x = self.pool(nn.functional.relu(self.conv5(x)))\n",
    "        x = self.flat(x)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "alex_net_instance = AlexNet()\n",
    "summary(alex_net_instance, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClzYvjaXhvAi"
   },
   "source": [
    "## Pre-Trained Modules and Transfer Learning\n",
    "\n",
    "Training CNNs can take a lot of time, and a lot of data is required for that task. However, much of the time is spent to learn the best low-level filters that a network is using to extract patterns from images. A natural question arises - can we use a neural network trained on one dataset and adapt it to classifying different images without full training process?\n",
    "\n",
    "This approach is called transfer learning, because we transfer some knowledge from one neural network model to another. In transfer learning, we typically start with a pre-trained model, which has been trained on some large image dataset, such as ImageNet. Those models can already do a good job extracting different features from generic images, and in many cases just building a classifier on top of those extracted features can yield a good result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHgL4yVXhjwC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpLaaYWTh6QT"
   },
   "source": [
    "### Cats vs. Dogs Dataset\n",
    "\n",
    "In this unit, we will solve a real-life problem of classifying images of cats and dogs. For this reason, we will use [Kaggle Cats vs. Dogs Dataset](https://www.kaggle.com/c/dogs-vs-cats), which can also be downloaded [from Microsoft](https://www.microsoft.com/en-us/download/details.aspx?id=54765).\n",
    "\n",
    "Let's download this dataset and extract it into `data` directory (this process may take some time (minutes not seconds)!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9obu9Zrh89G"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import glob\n",
    "from PIL import Image  # Add this import to handle image operations\n",
    "\n",
    "# Download and extract if not present\n",
    "zip_path = 'data/kagglecatsanddogs_5340.zip'\n",
    "extract_path = 'data/PetImages'\n",
    "if not os.path.exists(zip_path):\n",
    "    urllib.request.urlretrieve('https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip', zip_path)\n",
    "if not os.path.exists(extract_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "\n",
    "# Check image files\n",
    "def check_image(fn):\n",
    "    try:\n",
    "        im = Image.open(fn)\n",
    "        im.verify()  # Verify the integrity of the image\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_image_dir(path):\n",
    "    for fn in glob.glob(path):\n",
    "        if not check_image(fn):\n",
    "            print(f\"Corrupt image: {fn}\")\n",
    "            os.remove(fn)\n",
    "\n",
    "# Check Cat and Dog images\n",
    "check_image_dir('data/PetImages/Cat/*.jpg')\n",
    "check_image_dir('data/PetImages/Dog/*.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q-SQM3EiLHU"
   },
   "source": [
    "Next, let's load the images into PyTorch dataset, converting them to tensors and doing some normalisation. We define image transformation pipeline by composing several primitive transformations using `Compose`:\n",
    "* `Resize` resizes our image to 256x256 dimensions\n",
    "* `CenterCrop` gets the central part of the image with size 224x224. Pre-trained VGG network has been trained on 224x224 images, thus we need to bring our dataset to this size.\n",
    "* `ToTensor` normalises pixel intensities to be in `[0, 1]` range, and convert images to PyTorch tensors\n",
    "* `std_normalise` transform is additional normalisation step specific for VGG network. When training a VGG network, original images from ImageNet were transformed by subtracting dataset mean intensity by color and dividing by standard deviation (also by color). Thus, we need to apply the same transformation to our dataset, so that all images are processed correctly.\n",
    "\n",
    "There are a few reason why we resized images to size 256, and then cropped to 224 pixels:\n",
    "* We wanted to demonstrate more possible transformations.\n",
    "* Pets are usually somewhere in the central part of the image, so we can improve classification by focusing more on the central part\n",
    "* Since some of the images are not square, we end up having padded parts of the image that do not contain any useful picture data, and cropping the image a bit reduces the padded part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rH-if9kViC57"
   },
   "outputs": [],
   "source": [
    "std_normalise = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "trans = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        std_normalise\n",
    "])\n",
    "dataset = torchvision.datasets.ImageFolder('data/PetImages', transform=trans)\n",
    "trainset_size = int(0.8*len(dataset))\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [trainset_size, len(dataset) - trainset_size])\n",
    "\n",
    "display_dataset(testset, n=15, classes=[\"Cat\", \"Dog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nbL8Hseiyi1"
   },
   "source": [
    "### Pre-Trained Models\n",
    "\n",
    "There are many different pre-trained models available inside `torchvision` module, and even more models can be found on the Internet. Let's see how simplest VGG-16 model can be loaded and used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqykOp_DinD0"
   },
   "outputs": [],
   "source": [
    "vgg = torchvision.models.vgg16(pretrained=True)\n",
    "sample_image = dataset[0][0].unsqueeze(0)\n",
    "res = vgg(sample_image)\n",
    "print(res[0].argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvBZEI7tjBkW"
   },
   "source": [
    "The result that we have received is a number of an `ImageNet` class, which can be looked up [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). We can use the following code to automatically load this class table and return the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVztOzqbixGr"
   },
   "outputs": [],
   "source": [
    "import json, requests\n",
    "class_map = json.loads(requests.get(\"https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/computer-vision-pytorch/imagenet_class_index.json\").text)\n",
    "class_map = { int(k) : v for k,v in class_map.items() }\n",
    "\n",
    "class_map[res[0].argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOPXHNyIjFbE"
   },
   "outputs": [],
   "source": [
    "summary(vgg, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wj5SwFMRjJHg"
   },
   "source": [
    "#### **Exercise 7**\n",
    "Compare your implementation of `AlexNet` with `VGG-16`. What are three main differences that you can spot? Talk to the TAs about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWqx7gVaK-FQ"
   },
   "source": [
    "1. Way more convolutional layers\n",
    "2. Larger filter sizes\n",
    "3. More parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9AOb76Pjk5Z"
   },
   "source": [
    "### GPU Computations\n",
    "\n",
    "Deep neural networks, such as VGG-16 and other more modern architectures require quite a lot of computational power to run. It makes sense to use GPU acceleration, if it is available. In order to do so, we need to explicitly move all tensors involved in the computation to GPU.\n",
    "\n",
    "The way it is normally done is to check the availability of GPU in the code, and define `device` variable that points to the computational device - either GPU or CPU.\n",
    "\n",
    "First, make sure that you have GPUs/TPUs enabled in your Colab though. You can do so under `Runtime > Change runtime type > Hardware accelerator` in the top bar menus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLk49kDKjG0j"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Doing computations on device = {}'.format(device))\n",
    "\n",
    "vgg.to(device)\n",
    "sample_image = sample_image.to(device)\n",
    "\n",
    "vgg(sample_image).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnG7bOKvkCfx"
   },
   "source": [
    "### Extracting VGG features\n",
    "\n",
    "If we want to use VGG-16 to extract features from our images, we need the model without final classification layers. In fact, this \"feature extractor\" can be obtained using `vgg.features` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sAcg10ykBEr"
   },
   "outputs": [],
   "source": [
    "res = vgg.features(sample_image).cpu()\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.imshow(res.detach().view(-1, 512))\n",
    "print(res.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYIpA-jik5Ab"
   },
   "source": [
    "The dimension of feature tensor is 512x7x7, but in order to visualise it we had to reshape it to 2D form.\n",
    "\n",
    "Now let's try to see if those features can be used to classify images. Let's manually take some portion of images (800 in our case), and pre-compute their feature vectors. We will store the result in one big tensor called `feature_tensor`, and also labels into `label_tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qo7W3LRbkWOT"
   },
   "outputs": [],
   "source": [
    "bs = 8\n",
    "dl = torch.utils.data.DataLoader(dataset,batch_size=bs,shuffle=True)\n",
    "num = bs*100\n",
    "feature_tensor = torch.zeros(num,512*7*7).to(device)\n",
    "label_tensor = torch.zeros(num).to(device)\n",
    "i = 0\n",
    "for x,l in dl:\n",
    "    with torch.no_grad():\n",
    "        f = vgg.features(x.to(device))\n",
    "        feature_tensor[i:i+bs] = f.view(bs,-1)\n",
    "        label_tensor[i:i+bs] = l\n",
    "        i+=bs\n",
    "        print('.',end='')\n",
    "        if i>=num:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsRAvazak9vi"
   },
   "source": [
    "Now we can define `vgg_dataset` that takes data from this tensor, split it into training and test sets using `random_split` function, and train a small one-layer dense classifier network on top of extracted features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EW87qiIDk6kE"
   },
   "outputs": [],
   "source": [
    "vgg_dataset = torch.utils.data.TensorDataset(feature_tensor, label_tensor.to(torch.long))\n",
    "train_ds, test_ds = torch.utils.data.random_split(vgg_dataset, [700, 100])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(512*7*7, 2),\n",
    "    torch.nn.LogSoftmax(dim=1)\n",
    ").to(device)\n",
    "\n",
    "history = train(net, train_loader, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRsR31YvlDnm"
   },
   "source": [
    "The results are great, we can distinguish between a cat and a dog with 99% probability! However, we have only tested this approach on a small subset of all images, because manual feature extraction seems to take a lot of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq_pXhoHlKTS"
   },
   "source": [
    "### Transfer Learning Using One VGG Network\n",
    "\n",
    "We can also avoid manually pre-computing the features by using the original VGG-16 network as a whole during training. Let's look at the VGG-16 object structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLD3WCvVlAMK"
   },
   "outputs": [],
   "source": [
    "print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kd3n45E7lPFR"
   },
   "source": [
    "You can see that the network contains:\n",
    "* feature extractor (`features`), comprised of a number of convolutional and pooling layers\n",
    "* average pooling layer (`avgpool`)\n",
    "* final `classifier`, consisting of several dense layers, which turns 25088 input features into 1000 classes (which is the number of classes in ImageNet)\n",
    "\n",
    "To train the end-to-end model that will classify our dataset, we need to:\n",
    "* **replace the final classifier** with the one that will produce required number of classes. In our case, we can use one `Linear` layer with 25088 inputs and 2 output neurons.\n",
    "* **freeze weights of convolutional feature extractor**, so that they are not trained. It is recommended to initially do this freezing, because otherwise untrained classifier layer can destroy the original pre-trained weights of convolutional extractor. Freezing weights can be accomplished by setting `requires_grad` property of all parameters to `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlFmpykflNra"
   },
   "outputs": [],
   "source": [
    "vgg.classifier = torch.nn.Linear(25088,2).to(device)\n",
    "\n",
    "for x in vgg.features.parameters():\n",
    "    x.requires_grad = False\n",
    "\n",
    "summary(vgg,(1, 3,244,244))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUITx_OZlUpC"
   },
   "source": [
    "As you can see from the summary, this model contain around 15 million total parameters, but only 50k of them are trainable - those are the weights of classification layer. That is good, because we are able to fine-tune smaller number of parameters with smaller number of examples.\n",
    "\n",
    "Now let's train the model using our original dataset. This process will take a long time, so we will use the `train_long` function that will print some intermediate results without waiting for the end of epoch. It is highly recommended to run this training on GPU-enabled compute!\n",
    "\n",
    "> **Note:** If you are interested in the implementation of the `train_long` function, refer to the `pytorchcv.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5dc4uYJP3R6"
   },
   "outputs": [],
   "source": [
    "def validate(net, dataloader,loss_fn=nn.NLLLoss()):\n",
    "    net.eval()\n",
    "    count,acc,loss = 0,0,0\n",
    "    with torch.no_grad():\n",
    "        for features,labels in dataloader:\n",
    "            lbls = labels.to(device)\n",
    "            out = net(features.to(device))\n",
    "            loss += loss_fn(out,lbls)\n",
    "            pred = torch.max(out,1)[1]\n",
    "            acc += (pred==lbls).sum()\n",
    "            count += len(labels)\n",
    "    return loss.item()/count, acc.item()/count\n",
    "\n",
    "\n",
    "def train_long(net,train_loader,test_loader,epochs=5,lr=0.01,optimizer=None,loss_fn = nn.NLLLoss(),print_freq=10):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        total_loss,acc,count = 0,0,0\n",
    "        for i, (features,labels) in enumerate(train_loader):\n",
    "            lbls = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = net(features.to(device))\n",
    "            loss = loss_fn(out,lbls)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss+=loss\n",
    "            _,predicted = torch.max(out,1)\n",
    "            acc+=(predicted==lbls).sum()\n",
    "            count+=len(labels)\n",
    "            if i%print_freq==0:\n",
    "                print(\"Epoch {}, minibatch {}: train acc = {}, train loss = {}\".format(epoch,i,acc.item()/count,total_loss.item()/count))\n",
    "        vl,va = validate(net,test_loader,loss_fn)\n",
    "        print(\"Epoch {} done, validation acc = {}, validation loss = {}\".format(epoch,va,vl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsAv3WA2lRd7"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset,batch_size=16)\n",
    "test_loader = torch.utils.data.DataLoader(testset,batch_size=16)\n",
    "\n",
    "train_long(vgg,train_loader,test_loader,loss_fn=torch.nn.CrossEntropyLoss(),epochs=1,print_freq=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCZr6NsclaXx"
   },
   "source": [
    "It looks like we have obtained reasonably accurate cats vs. dogs classifier! Let's save it for future use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7CbbFVElZQ7"
   },
   "outputs": [],
   "source": [
    "torch.save(vgg,'data/cats_dogs.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdvvB7DKlddq"
   },
   "source": [
    "We can then load the model from file at any time. You may find it useful in case the next experiment destroys the model - you would not have to re-start from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJ3B38ckld6C"
   },
   "outputs": [],
   "source": [
    "vgg = torch.load('data/cats_dogs.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p8tvXY1loRw"
   },
   "source": [
    "### Fine-Tuning Transfer Learning\n",
    "\n",
    "In the previous section, we have trained the final classifier layer to classify images in our own dataset. However, we did not re-train the feature extractor, and our model relied on the features that the model has learned on ImageNet data. If your objects visually differ from ordinary ImageNet images, this combination of features might not work best. Thus it makes sense to start training convolutional layers as well.\n",
    "\n",
    "To do that, we can unfreeze the convolutional filter parameters that we have previously frozen.\n",
    "\n",
    "> **Note:** It is important that you freeze parameters first and perform several epochs of training in order to stabilize weights in the classification layer. If you immediately start training end-to-end network with unfrozen parameters, large errors are likely to destroy the pre-trained weights in the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1vIkqHBlovr"
   },
   "outputs": [],
   "source": [
    "for x in vgg.features.parameters():\n",
    "    x.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx8UU_JRlsY0"
   },
   "source": [
    "After unfreezing, we can do a few more epochs of training. You can also select lower learning rate, in order to minimise the impact on the pre-trained weights. However, even with low learning rate, you can expect the accuracy to drop in the beginning of the training, until finally reaching slightly higher level than in the case of fixed weights.\n",
    "\n",
    "> **Note:** This training happens much slower, because we need to propagate gradients back through many layers of the network! You may want to watch the first few minibatches to see the tendency, and then stop the computation.\n",
    "\n",
    "> **Note:** The learning rate is now much lower to not destroy the earlier solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWUN4ka-lzyA"
   },
   "outputs": [],
   "source": [
    "train_long(vgg,train_loader,test_loader,loss_fn=torch.nn.CrossEntropyLoss(),epochs=1,print_freq=90,lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhOiV4kwlyJr"
   },
   "source": [
    "### Other Computer Vision Models\n",
    "\n",
    "VGG-16 is one of the simplest computer vision architectures. `torchvision` package provides many more [pre-trained networks](https://pytorch.org/vision/stable/models.html). The most frequently used ones among those are **ResNet** architectures, developed by Microsoft, and **Inception** by Google. For example, let's explore the architecture of the simplest ResNet-18 model (ResNet is a family of models with different depth, you can try experimenting with ResNet-151 if you want to see what a really deep model looks like):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhfiN8qkl1cr"
   },
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet18()\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZYJpPZ2mNk8"
   },
   "source": [
    "As you can see, the model contains the same building blocks: feature extractor and fully-connected layer(`fc`). This allows us to use this model in exactly the same manner as we have been using VGG-16 for transfer learning. You can try experimenting with the code above, using different ResNet models as the base model, and see how accuracy changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shK_lQF5mPvk"
   },
   "source": [
    "This network also contains yet another type of layer: **Batch Normalisation**. The idea of batch normalisation is to bring values that flow through the neural network to right interval. Usually neural networks work best when all values are in the range of [-1,1] or [0,1], and that is the reason that we scale/normalise our input data accordingly. However, during training of a deep network, it can happen that values get significantly out of this range, which makes training problematic. Batch normalisation layer computes average and standard deviation for all values of the current minibatch, and uses them to normalise the signal before passing it through a neural network layer. This considerably improves the stability of deep networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezmaWJn_mYLJ"
   },
   "source": [
    "### The Transfer Learning Takeaway\n",
    "\n",
    "Using transfer learning, we were able to quickly put together a classifier for our custom object classification task, and achieve high accuracy. However, this example was not completely fair, because original VGG-16 network was pre-trained to recognise cats and dogs, and thus we were just reusing most of the patterns that were already present in the network. You can expect lower accuracy on more exotic, domain-specific objects, such as details on production line in a plant, or different tree leaves.\n",
    "\n",
    "You can see that more complex tasks that we are solving now require higher computational power, and cannot be easily solved on the CPU. Below, we will try to use more lightweight implementation to train the same model using lower compute resources, which results in just slightly lower accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y0hqVzU83yc"
   },
   "source": [
    "# Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61KOd_2wITG2"
   },
   "source": [
    "The second part of this session will focus on the combination of audio and machine learning. We will first cover different aspects of audio preprocessing and later on classify musical genres. We will use some of the concepts seen in the computer vision part of the notebook, in particular convolution.\n",
    "\n",
    "The goal of this session is to get hands-on experience working with audio data in machine learning and to obtain a glimpse into the applications of machine learning within the audio domain.\n",
    "\n",
    "Parts of this notebook were taken from [Valerio Velardo's ML for Audio Youtube playlist](https://www.youtube.com/watch?v=iCwMQJnKk2c&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0), [the tutorial on supervised music classification](https://music-classification.github.io/tutorial/part3_supervised/tutorial.html) and [torchaudio's feature extraction tutorial](https://pytorch.org/audio/main/tutorials/audio_feature_extractions_tutorial.html#sphx-glr-tutorials-audio-feature-extractions-tutorial-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYa-7cQ8ITG3"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnyJFYbVITG3"
   },
   "source": [
    "Before we dig into the machine learning aspects of audio processing let's first revisit some fundamentals of audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urh9RytUITG3"
   },
   "source": [
    "We will demonstrate this using the `librosa` library and a built-in demo of a five second trumpet snippet. Let's import the necessary libraries and load the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIyPyBQ-ITG3"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import Audio\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZostGyLRITG4"
   },
   "outputs": [],
   "source": [
    "filename = librosa.ex('trumpet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwBfpq4rITG4"
   },
   "source": [
    "We can load an audio file using `librosa.load`, here we need to pass two arguments, the sampling rate and whether the sound should be mono.\n",
    "\n",
    "By specifying `sr=None` we ensure that the file is loaded with the native sampling rate. `mono=True` is usually used as it makes everything easier. Just like in Computer Vision, where grayscale makes life easier, mono channel audio allows us to work with a single channel. If the audio is multi channel, it is usually averaged into a single channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42dD-SK6ITG4"
   },
   "source": [
    "> Note: Sampling rate is the number of samples per second from a signal. The higher the sampling rate, the more information is captured. However, the higher the sampling rate, the more memory is required to store the audio file. The sampling rate is usually 44.1 kHz or 48 kHz for CD quality audio. This value is also used for the Nyquist frequency, which is the highest frequency that can be represented in a signal. For example, if the sampling rate is 44.1 kHz, the Nyquist frequency is 22.05 kHz. This means that the highest frequency that can be represented in the signal without aliasing is 22.05 kHz. Aliasing occurs when the signal is distorted due to the sampling rate being too low. The Nyquist frequency is half the sampling rate and strategically chosen to include the highest frequencies audible to humans, which is around 20 kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Y3ZGhYKITG4"
   },
   "outputs": [],
   "source": [
    "trumpet, sr = librosa.load(filename, sr=None, mono=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13GrckB9ITG4"
   },
   "source": [
    "We are now ready to work on the soundwave contained in `trumpet`, the `sr` lets us check what sampling rate we are working with. We are expecting `trumpet` to contain a single array of values, the length of the array is determined by the length of the audio snippet multiplied by the sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DICdm8TITG4"
   },
   "outputs": [],
   "source": [
    "print(trumpet.shape)\n",
    "print(sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTaTjyEWITG4"
   },
   "source": [
    "Nice! According to the formula this snippet should contain $117601 / 22050$ seconds of audio. This turns out to be 5.33 seconds.\n",
    "\n",
    "Let's load this array into `Audio` and give it a listen. `Audio` is provided by `IPython.display`. A handy package that allows various features directly within a jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAhaA0tAITG5"
   },
   "outputs": [],
   "source": [
    "Audio(trumpet, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYQ6FWmQITG5"
   },
   "source": [
    "Exquisite. The length of the snippet checks out as well, very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbfR5c8e_FGw"
   },
   "source": [
    "## Audio visualization\n",
    "Let's visualize this snippet. First, we will write a little helper function to plot a waveform, in case we need it again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZlxUKKDITG5"
   },
   "outputs": [],
   "source": [
    "def plot_waveform(waveform, sr, title=\"Waveform\"):\n",
    "    if torch.is_tensor(waveform):\n",
    "        waveform = waveform.numpy()\n",
    "        _, num_frames = waveform.shape\n",
    "        waveform = waveform[0]\n",
    "    else:\n",
    "        num_frames = len(waveform)\n",
    "    time_axis = np.arange(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplots(1, 1)\n",
    "    axes.plot(time_axis, waveform, linewidth=1)\n",
    "    axes.grid(True)\n",
    "    axes.set_xlabel(\"Time (s)\")\n",
    "    axes.set_ylabel(\"Amplitude\")\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iefiexWITG5"
   },
   "source": [
    "The `plot_waveform` function is a bit more complicated than it needs to be. This allows us to plot both numpy and torch arrays in one function. It will come in handy later on.\n",
    "\n",
    "Let's now plot the soundwave of the trumpet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rteZ6CfITG5"
   },
   "outputs": [],
   "source": [
    "plot_waveform(trumpet, sr, title=\"Trumpet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e864ygDITG5"
   },
   "source": [
    "The soundwave is a representation of the amplitude of the sound over time. The amplitude is the strength of the sound wave. The higher the amplitude, the louder the sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI6wG-mIITG5"
   },
   "source": [
    "In audio processing for ML we usually work with spectrograms. A spectrogram is a two-dimensional representation of the signal, where the x-axis is time and the y-axis is frequency. The color of the spectrogram represents the intensity of the signal at a given time and frequency. The spectrogram is a very useful tool for analyzing audio signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJtID2r2ITG5"
   },
   "source": [
    "It turns out that spectrograms containing frequencies and temporal information can be generated using a variation of the Fourier Transform: Short-Time Fourier Transform!\n",
    "\n",
    "Short-Time Fourier Transform is basically the same as a normal discrete Fourier Transform but applied only to short segments of the signal. We need this because, applying a discrete Fourier Transform to the entire soundwave would average out all the frequencies over time and essentially remove the temporal component of the signal.\n",
    "\n",
    "STFT has a couple of parameters that are usually set:\n",
    "- window length: Width of the window that masks the signal at time $t$, a small window will improve temporal resolution, but at the cost of spatial resolution. This is known as the time-frequency localization trade-off. Therefore, what values are best depend on the signal and the application, there is no free lunch!\n",
    "- hop length: Distance between adjacent segments.\n",
    "- n_fft: Length of the windowed signal after padding. Usually the window length and n_fft are set to be the same value. Depending on the application different values are recommended, for speech 512 and for music 2048."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjLfcuUwITG5"
   },
   "source": [
    "![Alt text](https://www.researchgate.net/publication/346243843/figure/fig1/AS:961807523000322@1606324191138/Short-time-Fourier-transform-STFT-overview.png)\n",
    "\n",
    "Image taken from [Area-Efficient Short-Time Fourier Transform Processor for Time–Frequency Analysis of Non-Stationary Signals](https://www.mdpi.com/2076-3417/10/20/7208)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxjnQifnITG5"
   },
   "source": [
    "Let's start by using `librosa.stft`, the built-in function for Short-Time Fourier Transform, to obtain a complex-valued matrix for our audio snippet. We define the parameters, so that we can easily reuse them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mHOIVvAITG6"
   },
   "outputs": [],
   "source": [
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "WIN_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3HJ5h_hITG6"
   },
   "outputs": [],
   "source": [
    "stft_trumpet = librosa.stft(trumpet, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JGkqVWFITG6"
   },
   "source": [
    "`stft_trumpet` is a matrix with dimension `[1+n_fft/2, len(signal)/hop_length]`. It contains (for our chosen parameters) 1025 frequency bins and 230 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16CRT3r-ITG6"
   },
   "outputs": [],
   "source": [
    "print(stft_trumpet.shape)\n",
    "print(stft_trumpet.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsgzCj5kITG6"
   },
   "source": [
    "However, because each entry is a complex value, we cannot yet visualize our trumpet matrix. The usual approach to solve this, is to compute the squared magnitude for each value. This is also known as the power spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x78f6YuhITG6"
   },
   "outputs": [],
   "source": [
    "power_trumpet = np.abs(stft_trumpet) ** 2\n",
    "print(power_trumpet.shape)\n",
    "print(power_trumpet.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzVy-GkkITG6"
   },
   "source": [
    "So, now that we have float values we can plot our matrix. We will define a `plot_spectrogram` function for easy reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUC-JF_HITG6"
   },
   "outputs": [],
   "source": [
    "def plot_spectrogram(specgram, title=None, yscale=\"linear\", ylabel=\"Hz\"):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    plt.title(title or \"Spectrogram (db)\")\n",
    "    librosa.display.specshow(specgram, y_axis=yscale, x_axis='time')\n",
    "    if \"dB\" in title or \"Mel\" in title:\n",
    "        plt.colorbar(ax=axs, format=\"%+2.f dB\")\n",
    "    else:\n",
    "        plt.colorbar(ax=axs)\n",
    "    fig.gca().set_ylabel(ylabel)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PI2DCB7nITG6"
   },
   "outputs": [],
   "source": [
    "plot_spectrogram(power_trumpet, \"Power Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc5GmNjAITG6"
   },
   "source": [
    "We can faintly make out a signal in the first two hundred frequency bins. Let's modify this. Converting the power spectrogram to decibel, which is a simple log transformation is done using the built in function `power_to_db` in `librosa`. This makes sense since the human hearing perceives frequencies not linearly, but logarithmically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vevr4P7PITG6"
   },
   "source": [
    "> Note: The decibel scale is used to measure sound pressure levels. The decibel scale is a logarithmic scale, which means that a 10 dB increase is not twice as loud, but 10 times as loud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIAbwl8wITG6"
   },
   "outputs": [],
   "source": [
    "plot_spectrogram(librosa.power_to_db(power_trumpet), \"dB Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRlDS64mITG7"
   },
   "source": [
    "This is already better, we can observe the energy of the different frequencies over time.\n",
    "\n",
    "We have already changed the values of our matrix to better reflect human hearing, but the frequency bins are still displayed linearly. A logarithmic scale on the y-axis would be closer to how we perceive the various frequencies. Let's change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fH9HMxiyITG7"
   },
   "outputs": [],
   "source": [
    "plot_spectrogram(librosa.power_to_db(power_trumpet), \"Log-Frequency dB Spectrogram\", \"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZZ7o2hN_s38"
   },
   "source": [
    "Nice! The fundamental frequency and the harmonic components are now clearly visible.\n",
    "\n",
    "> Note: The fundamental frequency is the lowest frequency in a sound. The harmonics are the frequencies that are integer multiples of the fundamental frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_mjyhy9ITG7"
   },
   "source": [
    "## Mel-Spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yi_0UDV0_zED"
   },
   "source": [
    "We have almost made it to the final form of spectrogram that we want for our audio processing. There is one more step to go: Mel-Spectrogram.\n",
    "\n",
    "Every spectrogram we looked at so far used frequencies measured in Hertz. This is a bit problematic for human hearing. Frequencies are spaced linearly but our hearing has a fundamentally different perception. Our hearing can distinguish lower frequencies better compared to higher frequencies, this relationship is also logarithmic.\n",
    "\n",
    "Mel-Spectrograms solve this problem by converting frequencies into Mels. The Mel-scale is a perceptual scale of pitches judged by listeners to be equal in distance from one another. It is a nonlinear transformation of the frequency scale. The Mel-scale is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yZjizjJITG7"
   },
   "source": [
    "![Alt text](https://www.sfu.ca/sonic-studio-webdav/handbook/Graphics/Mel.gif)\n",
    "\n",
    "Image taken from [sfu.ca](https://www.sfu.ca/sonic-studio-webdav/handbook/Mel.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUx-c87tITG7"
   },
   "source": [
    "The conversion is given by the following equations:\n",
    "- $m = 2595 * log(1+f/500)$\n",
    "- $f = 700*(10^{m/2595}-1)$\n",
    "\n",
    "To convert frequencies to Mel-scale we need one parameter: Mel bands.\n",
    "\n",
    "Mel bands are the equivalent to the frequency bins in a frequency spectrogram but for the Mel-Spectrogram. So how many mel bands should we use? That depends on the application, general numbers are between 40 and 128.\n",
    "\n",
    "Let's now generate our trumpet Mel-Spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSaZI-61ITG7"
   },
   "source": [
    "> Note: Next to Mel bands we also need Mel Filter Banks. MFBs are a set of triangular filters that are used to convert the frequency spectrogram into a Mel-Spectrogram. The Mel-Spectrogram is then obtained by multiplying the Mel Filter Banks with the frequency spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4z_Xx2kzITG7"
   },
   "outputs": [],
   "source": [
    "N_MELS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skcl9In_ITG7"
   },
   "outputs": [],
   "source": [
    "mel_trumpet = librosa.feature.melspectrogram(y=trumpet, sr=sr, n_fft=N_FFT, n_mels=N_MELS, hop_length=HOP_LENGTH, win_length=WIN_LENGTH)\n",
    "print(mel_trumpet.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2Sa3HTuITG7"
   },
   "source": [
    "`mel_trumpet` is a matrix with dimension `[n_mels, len(signal)/hop_length]`. It contains (for our chosen parameters) 128 mel bins and 230 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekjair2UITG7"
   },
   "outputs": [],
   "source": [
    "plot_spectrogram(librosa.power_to_db(mel_trumpet), \"Mel-Spectrogram\", ylabel=\"mel bins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AwOz21eITG7"
   },
   "source": [
    "So now that we have a Mel-Spectrogram, we have covered all points of an ideal audio feature:\n",
    "- Time-Frequency representation: Spectrogram.\n",
    "- Perceptually-relevant amplitude representation: Power to dB conversion.\n",
    "- Perceptually-relevant frequency representation: Mel-Spectrogram\n",
    "\n",
    "Finally, we can start training some models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMWXVa9NITG7"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "licocS3lITG7"
   },
   "source": [
    "There are many different applications for machine learning in audio processing, some of them are:\n",
    "- Speech: Speech recognition, speech enhancement, speech separation, speech-to-speech translation, speech-to-text, text-to-speech, speaker recognition, speaker diarization, speaker verification, voice activity detection, voice emotion recognition.\n",
    "- Music: Music generation, music recommendation, music transcription, music information retrieval, music source separation, music style transfer, music mood recognition, music genre recognition.\n",
    "\n",
    "Today we will focus on music. We will train a classifier to recognize a music genre based on a Mel-Spectrogram of a music snippet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNfB1RDfITG8"
   },
   "source": [
    "For this, we will first need to download GTZAN, a dataset from 2002 containing 100 music tracks for 10 different genres. The recordings vary in quality. This dataset is basically the MNIST of audio.\n",
    "\n",
    "These famous datasets are usually supported directly in the built-in downloader from pytorch, but the website hosting the dataset has been down since October 2022, so we will have to download it from Kaggle. Fortunately for you, we have prepared it in a polybox drive which can be easily downloaded using `wget`.\n",
    "\n",
    "From the Kaggle link below: *The GTZAN dataset is the most-used public dataset for evaluation in machine listening research for music genre recognition (MGR). The files were collected in 2000-2001 from a variety of sources including personal CDs, radio, microphone recordings, in order to represent a variety of recording conditions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIXtWsqhITG8"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://polybox.ethz.ch/index.php/s/Xx84jJJYGUt60oT/download\"\n",
    "output_path = \"gtzan.zip\"\n",
    "urllib.request.urlretrieve(url, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg4V5burITG8"
   },
   "source": [
    "After downloading we will unzip it and move the relevant parts. We will also clean up a bit by removing parts that are no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-n0TDbYITG8"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Check if the zip file is already extracted\n",
    "if not os.path.exists('data/genres'):\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile('gtzan.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "\n",
    "    # Rename the folder if it exists\n",
    "    if os.path.exists('Data/genres_original'):\n",
    "        os.rename('Data/genres_original', 'data/genres')\n",
    "\n",
    "    # Remove the original 'Data' folder and the zip file\n",
    "    shutil.rmtree('Data', ignore_errors=True)\n",
    "    os.remove('gtzan.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkeSjY8eITG8"
   },
   "source": [
    "We will also download three textfiles containing the names of the tracks for each genre. These are used to split the dataset into train, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbowNnSWITG8"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# URLs for the text files\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/train_filtered.txt\",\n",
    "    \"https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/valid_filtered.txt\",\n",
    "    \"https://raw.githubusercontent.com/coreyker/dnn-mgr/master/gtzan/test_filtered.txt\"\n",
    "]\n",
    "\n",
    "# Download each file\n",
    "for url in urls:\n",
    "    output_path = url.split(\"/\")[-1]  # Extract filename from URL\n",
    "    urllib.request.urlretrieve(url, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfKL3C36ITG8"
   },
   "source": [
    "The 54th jazz music file is corrupted, so we will remove it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKw2f4eVITG8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Remove the file\n",
    "file_path = 'data/genres/jazz/jazz.00054.wav'\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "\n",
    "# Remove the line from the text file\n",
    "train_file_path = './train_filtered.txt'\n",
    "with open(train_file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Remove the 253rd line (index 252, because indices are 0-based)\n",
    "del lines[252]\n",
    "\n",
    "# Write the modified lines back to the file\n",
    "with open(train_file_path, 'w') as file:\n",
    "    file.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvQgH7gtITG8"
   },
   "source": [
    "We are going to need various packages, let's import them.\n",
    "\n",
    "Some important packages are:\n",
    "- `torch`: The main package for deep learning.\n",
    "- `torchaudio`: The package for ML audio processing, compatible with torch and gpu acceleration.\n",
    "- `soundfile`: The package for reading and writing audio files.\n",
    "- `sklearn.metrics`: The package for computing metrics.\n",
    "- `seaborn`: The package for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6UZLyWuITG8"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANWvKzteITG8"
   },
   "source": [
    "Let's now define a constant for the genres we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcHX7I8jITG8"
   },
   "outputs": [],
   "source": [
    "GTZAN_GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop',\n",
    "                'jazz', 'metal', 'pop', 'reggae', 'rock']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkVuVobZITG9"
   },
   "source": [
    "We will now define the `GTZANDataset` class. This class will be used to load the audio files and their labels. To improve throughput performance, and because we have enough memory, we will load all the audio files into memory.\n",
    "\n",
    "We will convert the audio files to Mel-Spectrograms on the fly as part of the first step of our neural network. We could also process the audio into Mel-Spectrograms beforehand and save them as numpy arrays, but this would require more space and we would not be able to take a quick listen straight from the dataloader.\n",
    "\n",
    "We will be training on the entire 29 second audio file, but the dataset implementation allows us to train on smaller chunks of audio. While training we can crop random chunks of audio from the loaded files. This is done to increase the amount of data we have available for training. In the testing and validation phase we would then run the model on different chunks of the same audio file and aggregate the outputs to get a final prediction. This approach is useful for training on longer audio but not necessarily needed for our short snippets.\n",
    "\n",
    "However, even if we train on the entire audio snippet we still need to use this cropping approach to ensure that all samples have exactly the same length, otherwise we would not be able to stack them into a batch. All audio snippets are somewhere close to 30 seconds in duration, so we will crop them to exactly 29 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBe7wnWGITG9"
   },
   "outputs": [],
   "source": [
    "class GTZANDataset(data.Dataset):\n",
    "    def __init__(self, data_path, split, num_samples, num_chunks):\n",
    "\n",
    "        # path where data is stored\n",
    "        self.data_path =  data_path if data_path else ''\n",
    "\n",
    "        # split of data to use: train, valid, test\n",
    "        self.split = split\n",
    "\n",
    "        # length of audio clip in seconds, samples refers to sampling rate * seconds\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # number of chunks to split audio clip into for testing/validation\n",
    "        self.num_chunks = num_chunks\n",
    "\n",
    "        self.genres = GTZAN_GENRES\n",
    "        self._get_song_list()\n",
    "\n",
    "    def _get_song_list(self):\n",
    "        list_filename = os.path.join(self.data_path, f'{self.split}_filtered.txt')\n",
    "        with open(list_filename) as f:\n",
    "            lines = f.readlines()\n",
    "        self.song_list = [line.strip() for line in lines]\n",
    "\n",
    "    def _adjust_audio_length(self, wav):\n",
    "        if self.split == 'train':\n",
    "            random_index = random.randint(0, len(wav) - self.num_samples - 1)\n",
    "            wav = wav[random_index: random_index + self.num_samples]\n",
    "        else:\n",
    "            hop = (len(wav) - self.num_samples) // self.num_chunks\n",
    "            wav = np.array([wav[i * hop: i * hop + self.num_samples] for i in range(self.num_chunks)])\n",
    "        return wav\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        song_path = self.song_list[index]\n",
    "        genre_name = song_path.split('/')[0]\n",
    "        genre_index = self.genres.index(genre_name)\n",
    "\n",
    "        # Path to the audio file\n",
    "        audio_filename = os.path.join(self.data_path, 'data/genres', song_path)\n",
    "\n",
    "        # Load the audio file when required\n",
    "        wav, _ = sf.read(audio_filename)\n",
    "        wav = self._adjust_audio_length(wav).astype('float32')\n",
    "\n",
    "        return wav, genre_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.song_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gLmuzxbITG9"
   },
   "source": [
    "We will load the dataset through a `DataLoader` object. This object will take care of batching and shuffling the data.\n",
    "\n",
    "The `get_dataloader` function expects a set of parameters:\n",
    "- `data_path`: The path to the dataset. This can remain `None` since our data is in the same directory as the notebook.\n",
    "- `split`: The split of the dataset to use. This can be either `train`, `valid` or `test`.\n",
    "- `num_samples`: The length of the audio snippets to use. As explained above, this will be 29 seconds of audio, where each second is 22050 samples.\n",
    "- `batch_size`: The size of the batches to use.\n",
    "- `num_workers`: The number of workers to use for loading the data. We will set it to 0 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OC-tKqszITG9"
   },
   "outputs": [],
   "source": [
    "def get_dataloader(data_path=None,\n",
    "                   split='train',\n",
    "                   num_samples=22050 * 29,\n",
    "                   num_chunks=1,\n",
    "                   batch_size=32,\n",
    "                   num_workers=0):\n",
    "    is_shuffle = True if (split == 'train') else False\n",
    "    batch_size = batch_size if (split == 'train') else (batch_size // num_chunks)\n",
    "    data_loader = data.DataLoader(dataset=GTZANDataset(data_path,\n",
    "                                                       split,\n",
    "                                                       num_samples,\n",
    "                                                       num_chunks),\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=is_shuffle,\n",
    "                                  drop_last=False,\n",
    "                                  num_workers=num_workers,\n",
    "                                  worker_init_fn=np.random.seed(42))\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha9a2duEITG9"
   },
   "source": [
    "Let's instantiate a dataloader for every split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_MQ8gC-ITG9"
   },
   "outputs": [],
   "source": [
    "train_loader = get_dataloader(split='train')\n",
    "valid_loader = get_dataloader(split='valid')\n",
    "test_loader = get_dataloader(split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUlfaYdtITG9"
   },
   "source": [
    "Now that we have our dataloaders, let's have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xynrx4gAITG9"
   },
   "outputs": [],
   "source": [
    "iter_train_loader = iter(train_loader)\n",
    "train_wav, train_genre = next(iter_train_loader)\n",
    "print('training data shape:', str(train_wav.shape))\n",
    "print(train_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aIiTob0ITG9"
   },
   "source": [
    "Since we set the batch size to 32, we get 32 audio snippets of length `num_samples` which is 22050 Hz multiplied by 29 seconds.\n",
    "\n",
    "Next to the size of the audio snippets, we also get a tensor with 32 entries containing the labels of our batch. The labels are encoded as integers from zero to ten, so we will need to decode them to get the actual genre names.\n",
    "\n",
    "Let's analyze the first audio snippet in our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxOvSwv5ITG9"
   },
   "outputs": [],
   "source": [
    "audio = train_wav[0].numpy()\n",
    "print('genre:', GTZAN_GENRES[train_genre[0]])\n",
    "plot_waveform(audio, sr=22050, title=\"Waveform\")\n",
    "mel_spec = librosa.feature.melspectrogram(y=audio, sr=22050, n_fft=N_FFT, n_mels=N_MELS, hop_length=HOP_LENGTH, win_length=WIN_LENGTH)\n",
    "plot_spectrogram(librosa.power_to_db(mel_spec), \"Mel-Spectrogram\", ylabel=\"mel bins\")\n",
    "Audio(audio, rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YPOZMrgITG9"
   },
   "source": [
    "Let's have a look at the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEOJOSDqITG9"
   },
   "outputs": [],
   "source": [
    "iter_test_loader = iter(test_loader)\n",
    "test_wav, test_genre = next(iter_test_loader)\n",
    "print('validation/test data shape:', str(test_wav.shape))\n",
    "print(test_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z68AouQmITG-"
   },
   "source": [
    "#### **Exercise 8**\n",
    "Compare the print statement outputs between the train and test loader. Note the differences and why they occur in the cell below and discuss with the TAs at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyFUaleLC8T5"
   },
   "source": [
    "In the train loader, the data is shuffled to remain bias-free, therefore the values in the batch are randomized. In the test loader the values are sorted (the first 31 are blues) to get a better picture of the situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lUjKVauA4MO"
   },
   "source": [
    "## Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnPK-QJ-ITG-"
   },
   "source": [
    "Now that we have the data in place, let's define our model.\n",
    "\n",
    "Since we will be working with Mel-Spectrograms, which in a way are just images, we will be using a convolutional neural network. The input to the network will be a Mel-Spectrogram of shape `[n_mels, num_samples]`. The output will be a vector of length 10, where each element corresponds to the probability of the audio snippet belonging to a specific genre.\n",
    "\n",
    "We will start by defining our convolutional block. This block will be used to extract features from the spectrogram. It consists of a convolutional layer, a batch normalization layer, a ReLU activation function, a max pooling layer and a dropout layer.\n",
    "\n",
    "> Note: A batch normalization layer is a layer that normalizes the output of the previous layer by subtracting the mean and dividing by the standard deviation. This is done to ensure that the output of the previous layer has a mean of zero and a standard deviation of one. This generally improves the convergence of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPhy2cdxITG-"
   },
   "outputs": [],
   "source": [
    "class Conv_2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, shape=3, pooling=2, dropout=0.1):\n",
    "        super(Conv_2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, shape, padding=shape//2)\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(pooling)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, wav):\n",
    "        out = self.conv(wav)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRQnbhwsITG-"
   },
   "source": [
    "Let's define the rest of the model. We will use two convolutional blocks and one fully connected layer.\n",
    "\n",
    "Pytorch models always have a `__init__` and `forward` function:\n",
    "- `__init__` defines the layers and transformations of the model. It is called when we instantiate the model. Here we will define necessary transformations that we learned in the previous section on Mel-Spectrograms as well as the layers of our model.\n",
    "- `forward` defines the forward pass of the model. This method is called when we pass a batch of data through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPuddfMCITG-"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_channels=16,\n",
    "                       sample_rate=22050,\n",
    "                       n_fft=1024,\n",
    "                       f_min=0.0,\n",
    "                       f_max=11025.0,\n",
    "                       num_mels=128,\n",
    "                       num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # mel spectrogram\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                                            n_fft=n_fft,\n",
    "                                                            f_min=f_min,\n",
    "                                                            f_max=f_max,\n",
    "                                                            n_mels=num_mels)\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        self.input_bn = nn.BatchNorm2d(1)\n",
    "\n",
    "        # convolutional layers\n",
    "        self.layer1 = Conv_2d(1, num_channels, pooling=(2, 3))\n",
    "        self.layer2 = Conv_2d(num_channels, num_channels * 4, pooling=(3, 4))\n",
    "\n",
    "        # dense layers\n",
    "        self.dense = nn.Linear(139776, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, wav):\n",
    "        # input Preprocessing\n",
    "        out = self.melspec(wav)\n",
    "        out = self.amplitude_to_db(out)\n",
    "\n",
    "        # conv expects image with channel dimension [batch, channel, width, height]\n",
    "        out = out.unsqueeze(1)\n",
    "\n",
    "        # convolutional layers\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        # reshape to flatten the output of the convolutional layers\n",
    "        out = out.reshape(len(out), -1)\n",
    "\n",
    "        # dense layers\n",
    "        out = self.dense(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OJ9N4EbITG-"
   },
   "source": [
    "Let's instantiate our model and move it to the GPU, if one is available. The print statement will tell us more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-k8gsjOITG-"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "cnn = CNN().to(device)\n",
    "print('Running on', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4-lcF5UITG-"
   },
   "source": [
    "We will be using the Adam optimizer with a learning rate of 0.001. We will also use the cross entropy loss function. We will train the model for 40 epochs.\n",
    "\n",
    "> Note: A simple explanation of the cross entropy loss function is that it measures the distance (or difference) between the predicted probability distribution and the true probability distribution. The lower the loss, the better the model is at predicting the correct class. This is not to be confused with the KL divergence, which is another measure of the difference between two probability distributions and is often used as a regularization term.\n",
    "\n",
    "The Pytorch implementation of cross entropy loss expects the inputs to be logits, which are the outputs of the last layer of the model before the softmax activation function. This is also the reason why our model did not have an activation function in the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDvk8j0uBVS0"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKIRVZebITG-"
   },
   "source": [
    "Let's finally define the training loop. We will train the model for `num_epochs`. For each epoch we will iterate over the training data and update the model parameters. We will also evaluate the model on the validation data and print the loss and accuracy. The model parameters will be saved if the validation loss is lower than the previous best loss.\n",
    "\n",
    "Let's train the model. Make sure you are connected to a GPU environment, it shouldn't take too much time to train (a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDysJDDSITG-"
   },
   "outputs": [],
   "source": [
    "def train_model(model, model_name, device=device):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    valid_losses = []\n",
    "    num_epochs = 40\n",
    "    for epoch in range(num_epochs):\n",
    "        train_losses = []\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        for (wav, genre_index) in tqdm(train_loader):\n",
    "            wav = wav.to(device)\n",
    "            genre_index = genre_index.to(device)\n",
    "\n",
    "            # Forward\n",
    "            out = model(wav)\n",
    "            loss = loss_function(out, genre_index)\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        losses = []\n",
    "        for wav, genre_index in valid_loader:\n",
    "            wav = wav.to(device)\n",
    "            genre_index = genre_index.to(device)\n",
    "\n",
    "            # reshape and aggregate chunk-level predictions\n",
    "            b, c, t = wav.size()\n",
    "            logits = model(wav.view(-1, t))\n",
    "            logits = logits.view(b, c, -1).mean(dim=1)\n",
    "            loss = loss_function(logits, genre_index)\n",
    "            losses.append(loss.item())\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "\n",
    "            # append labels and predictions\n",
    "            y_true.extend(genre_index.tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        valid_loss = np.mean(losses)\n",
    "\n",
    "        print('Epoch: [%d/%d], Train loss: %.4f, Valid loss: %.4f, Valid accuracy: %.4f' % (epoch+1, num_epochs, np.mean(train_losses), valid_loss, accuracy))\n",
    "\n",
    "        # Save model\n",
    "        valid_losses.append(valid_loss.item())\n",
    "        if np.argmin(valid_losses) == epoch:\n",
    "            print('Saving the best model at %d epochs!' % epoch)\n",
    "            torch.save(model.state_dict(), f'{model_name}_best_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlyOFw0uITG_"
   },
   "outputs": [],
   "source": [
    "train_model(cnn, 'cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hl-EeHaXITG_"
   },
   "source": [
    "Our model is now trained. As we can see, the model's training and validation loss decreased, and the validation accuracy increased. A random classifier would give us a 10% accuracy because we have 10 classes. Our model is already doing much better than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ1tfA_qBmk6"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPa5Z6IXBved"
   },
   "source": [
    "\n",
    "Let's now evaluate the model on the test set. We will load the best model parameters and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfYmneUlITG_"
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "S = torch.load('cnn_best_model.ckpt')\n",
    "cnn.load_state_dict(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNDoWjW1ITG_"
   },
   "source": [
    "Our CNN now contains the best model parameters that we found during training. We can now use this model to make predictions on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNwIqjAyITG_"
   },
   "source": [
    "Since we will only be evaluating we will not need to compute gradients. We will also not need to update the model parameters, so we can set `torch.no_grad()` while we iterate through the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1sGnQBwITG_"
   },
   "outputs": [],
   "source": [
    "def predict_testset(model):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for wav, genre_index in tqdm(test_loader):\n",
    "            wav = wav.to(device)\n",
    "            genre_index = genre_index.to(device)\n",
    "\n",
    "            # predict\n",
    "            b, c, t = wav.size()\n",
    "            logits = model(wav.view(-1, t))\n",
    "            logits = logits.view(b, c, -1).mean(dim=1)\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "\n",
    "            # append labels and predictions\n",
    "            y_true.extend(genre_index.tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HYhvfC6ITG_"
   },
   "outputs": [],
   "source": [
    "y_true, y_pred = predict_testset(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYwVaS9QITG_"
   },
   "source": [
    "Now that our lists contain the predictions and the labels, we can compute the accuracy and the confusion matrix using sklearn.\n",
    "\n",
    "The confusion matrix shows us how often the model predicted a specific class. The rows represent the true labels and the columns represent the predicted labels. The diagonal shows us how often the model predicted the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BN6Sq1BrITG_"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, genres):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # add row and column names\n",
    "    s = sns.heatmap(cm, annot=True, xticklabels=GTZAN_GENRES, yticklabels=GTZAN_GENRES, cmap='YlGnBu')\n",
    "    s.set(xlabel='Predicted label', ylabel='True label')\n",
    "    print('Accuracy: %.4f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIQIcS12ITG_"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true, y_pred, GTZAN_GENRES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjJa2d7YB9vQ"
   },
   "source": [
    "#### **Exercise 9**\n",
    "What can you observe from the confusion matrix? Write your observations in the cell below and discuss with the TAs at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zRuP-KRC3Da"
   },
   "source": [
    "There are some genres that get recognized more easily than others. Also some genres get often mispredicted as other genres, like country gets often misclassified as rock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tyj0f40JITHA"
   },
   "source": [
    "#### **Exercise 10**\n",
    "Improve the performance of the CNN MGR model, you are free to use any techniques that you have learned so far. The model should work with the same dataloader as defined above and predict ten classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1UkcKmHITHA"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_channels=16,\n",
    "                       sample_rate=22050,\n",
    "                       n_fft=1024,\n",
    "                       f_min=0.0,\n",
    "                       f_max=11025.0,\n",
    "                       num_mels=128,\n",
    "                       num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # Mel spectrogram transformation\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            f_min=f_min,\n",
    "            f_max=f_max,\n",
    "            n_mels=num_mels\n",
    "        )\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 120, 5)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(120, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, wav):\n",
    "        # input Preprocessing\n",
    "        out = self.melspec(wav)\n",
    "        out = self.amplitude_to_db(out)\n",
    "        # out has shape [32, 128, 1249]\n",
    "\n",
    "        # TODO define your forward pass here\n",
    "        out = self.pool(nn.functional.relu(self.conv1(out)))\n",
    "        out = self.pool(nn.functional.relu(self.conv2(out)))\n",
    "        out= nn.functional.relu(self.conv3(out))\n",
    "        out = self.flat(out)\n",
    "        out = nn.functional.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU3tx14mITHA"
   },
   "source": [
    "Once the model is defined we can train it with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jxkqdtp1ITHA"
   },
   "outputs": [],
   "source": [
    "chkpt_name = 'MyModel'\n",
    "num_epochs = 40\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "mymodel = MyModel().to(device)\n",
    "train_model(mymodel, chkpt_name, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytrRoVyOITHA"
   },
   "source": [
    "Let's now have a look at the predictions of our new (and hopefully) improved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0qxY9KkITHA"
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "S = torch.load(chkpt_name+'_best_model.ckpt')\n",
    "mymodel.load_state_dict(S)\n",
    "y_true_mymodel, y_pred_mymodel = predict_testset(mymodel)\n",
    "plot_confusion_matrix(y_true_mymodel, y_pred_mymodel, GTZAN_GENRES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xo5WuayaITHB"
   },
   "source": [
    "#### **Exercise 11**\n",
    "Briefly describe what you did to improve the performance of the model and discuss the results in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UGyYn-lITHB"
   },
   "source": [
    "We used more hidden layers and therefore expanded the cnn and increased the amount of learnable parameters  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTuvG0JzP3SF"
   },
   "source": [
    "### Autoencoders\n",
    "\n",
    "So you might be thinking: Cool, we have represented audio in a meaningful way as images, but can we do better than such a hand-engineered feature? How about we let a neural network learn the features itself?\n",
    "\n",
    "Autoencoders are a type of neural network that are used for unsupervised learning. They are composed of an encoder and a decoder. The encoder compresses the input into a latent space representation, and the decoder reconstructs the input from the latent space representation. The latent space is usually a lower-dimensional space compared to the input space. The loss we're optimizing is the reconstruction of the signal (the original signal and the reconstructed signal should be the same).\n",
    "\n",
    "In the following exercise, we will be using a special form of an autoencoder, also called U-Net. There are many reasons why we might want to do this. In our example we will be using the Autoencoder to remove noise from speech samples (denoising). A U-Net is a neural network that connects each layer of the encoder via skip connection with the corresponding layer of the decoder. U-Nets are also commonly used for diffusion models and originally developed for images. You can see below the architecture of an image U-Net, we will adopt this for audio:\n",
    "\n",
    "![An image showing how computer vision object detection can be performed with cats, dogs, and ducks.](https://media.geeksforgeeks.org/wp-content/uploads/20220614121231/Group14.jpg)\n",
    "\n",
    "We will use the amazing pytorch lightning library to train our autoencoder. It's a wrapper around Pytorch that makes training easier and more efficient. It's also super easy to use for multi-gpu training, if that's ever a requirement.\n",
    "\n",
    "We will define a `LightningModule` that contains the model, the optimizer, the loss function and the training loop.\n",
    "\n",
    "Furthermore, we will use 1D convolutional layers and the `Snake` activation function, which is a periodic activation function making it easier for the network to learn oscillatory patterns. The `tanh` activation function is used for the output, as the audio waveform contains values between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frWThkkuP3SF"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "\n",
    "# snake activation function\n",
    "class Snake(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        # Make alpha a learnable parameter\n",
    "        self.alpha = nn.Parameter(torch.tensor(alpha))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + (1.0/self.alpha) * torch.pow(torch.sin(self.alpha * x), 2)\n",
    "\n",
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=7, stride=3, padding=1),\n",
    "            Snake()\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, kernel_size=3, stride=2, padding=0),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder layers with skip connections\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 64, kernel_size=3, stride=2, padding=0),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(128, 1, kernel_size=7, stride=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path with stored intermediate outputs\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        d1 = self.dec1(e4)\n",
    "        d1 = torch.cat([d1, e3], dim=1)  # Skip connection 1\n",
    "\n",
    "        d2 = self.dec2(d1)\n",
    "        d2 = torch.cat([d2, e2], dim=1)  # Skip connection 2\n",
    "\n",
    "        d3 = self.dec3(d2)\n",
    "        d3 = torch.cat([d3, e1], dim=1)  # Skip connection 3\n",
    "\n",
    "        x_hat = self.dec4(d3)\n",
    "\n",
    "        return x_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        noisy_sample, ground_truth = batch\n",
    "\n",
    "        # Forward pass\n",
    "        x_hat = self(noisy_sample)\n",
    "\n",
    "        # Pad x_hat to match x if lengths differ\n",
    "        if x_hat.size(-1) != ground_truth.size(-1):\n",
    "            padding = ground_truth.size(-1) - x_hat.size(-1)\n",
    "            x_hat = torch.nn.functional.pad(x_hat, (0, padding))\n",
    "\n",
    "        # Reconstruction loss\n",
    "        recon_loss = nn.MSELoss()(ground_truth, x_hat)\n",
    "\n",
    "        self.log('recon_loss', recon_loss, prog_bar=True)\n",
    "\n",
    "        return recon_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_00zV8jNP3SF"
   },
   "source": [
    "For the Speech dataset we will be using the `torchaudio` library to load the data. We will also use the pyloudnorm package to normalize all audio snippets to -24 dB loudness.\n",
    "\n",
    "Denoising is an interesting task because we can train a model in a self-supervised manner: we add noise to the original clean signal, and then have the model learn to remove that noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6NjtRkDP3SF"
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPython.display import Audio\n",
    "from IPython import display\n",
    "import pyloudnorm as pyln\n",
    "\n",
    "# dataloader loading CMU arctic dataset\n",
    "cmu_arctic = torchaudio.datasets.CMUARCTIC('.', download=True)\n",
    "\n",
    "def normalize_audio(waveform, sampling_rate=16000):\n",
    "    data = waveform.numpy().transpose(1, 0) # switch channel and data dimension\n",
    "\n",
    "    # measure the loudness first\n",
    "    meter = pyln.Meter(sampling_rate) # create BS.1770 meter\n",
    "    loudness = meter.integrated_loudness(data)\n",
    "\n",
    "    # loudness normalize audio to -24 dB LUFS\n",
    "    loudness_normalized_audio = torch.from_numpy(pyln.normalize.loudness(data, loudness, -24.0)).transpose(1, 0) # switch channel and data dimension back\n",
    "\n",
    "    return loudness_normalized_audio.float()\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files):\n",
    "        self.audio_files = audio_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio file\n",
    "        waveform = self.audio_files[idx][0]\n",
    "        sample_rate = 16000\n",
    "\n",
    "        # Set target length (use the maximum length in your dataset or a fixed length)\n",
    "        target_length = sample_rate*2  # Adjust this value based on your needs\n",
    "\n",
    "        # If shorter than target length, pad with zeros\n",
    "        if waveform.size(1) < target_length:\n",
    "            padding = target_length - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "\n",
    "        # If longer than target length, take random snippet\n",
    "        if waveform.size(1) > target_length:\n",
    "            start_idx = torch.randint(0, waveform.size(1) - target_length + 1, (1,)).item()\n",
    "            waveform = waveform[:, start_idx:start_idx + target_length]\n",
    "\n",
    "        noised_waveform = waveform + torch.randn_like(waveform) * 0.1\n",
    "\n",
    "        noised_waveform = normalize_audio(noised_waveform)\n",
    "        waveform = normalize_audio(waveform)\n",
    "\n",
    "        return noised_waveform, waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IULSDF9WP3SF"
   },
   "outputs": [],
   "source": [
    "train_arctic, test_arctic = torch.utils.data.random_split(cmu_arctic, [int(len(cmu_arctic)*0.8), len(cmu_arctic)-int(len(cmu_arctic)*0.8)])\n",
    "train_loader = torch.utils.data.DataLoader(AudioDataset(train_arctic), batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(AudioDataset(test_arctic), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQXNO1jdP3SF"
   },
   "outputs": [],
   "source": [
    "noisy, gt = next(iter(train_loader))\n",
    "noisy = noisy[0].unsqueeze(0)\n",
    "\n",
    "Audio(noisy[0].detach().cpu().numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Liub-wmYP3SF"
   },
   "outputs": [],
   "source": [
    "Audio(gt[0].detach().cpu().numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYbnMvJsP3SF"
   },
   "outputs": [],
   "source": [
    "# instantiate and train the autoencoder\n",
    "autoencoder = Autoencoder()\n",
    "trainer = pl.Trainer(max_epochs=10, log_every_n_steps=10)\n",
    "trainer.fit(autoencoder, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jb7_WIjKP3SF"
   },
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    x_hat = autoencoder(noisy[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF2Qxq6_P3SF"
   },
   "source": [
    "Let's also listen to the original and reconstructed audio snippets, can you tell the difference? (Best to listen with headphones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cL4nrOj2P3SG"
   },
   "outputs": [],
   "source": [
    "# noisy audio\n",
    "Audio(noisy[0].numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6rfGY1SP3SG"
   },
   "outputs": [],
   "source": [
    "# reconstructed audio\n",
    "Audio(x_hat[0].detach().cpu().numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4soYAKDcP3SG"
   },
   "outputs": [],
   "source": [
    "# original audio\n",
    "Audio(gt[0].detach().cpu().numpy(), rate=16000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ezmaWJn_mYLJ"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
