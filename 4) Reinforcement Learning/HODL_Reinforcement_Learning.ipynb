{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook created by [Saku Peltonen](https://disco.ethz.ch/members/speltonen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for Colab\n",
    "# !pip install gymnasium pygame imageio swig\n",
    "# !pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "In this session, we will take a look at reinforcement learning (RL). The first part of the notebook introduces the basics of RL using traditional, table-based methods. In the second part, we apply deep neural networks as function approximators to solve RL tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by installing the required packages and downloading figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import pygame\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import imageio\n",
    "import math\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning in FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://polybox.ethz.ch/index.php/s/RGYpM0ETqPyUD0d/download\" width=\"400\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FrozenLake is a toy environment for training and testing RL agents. The goal of the player is to get to the target square (marked by the gift box) without falling into the holes in the ice. The player moves orthogonally. However, the ice is slippery, so the player does not always go in the intended direction! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the environment. The next cell contains utility functions. You do not need to go through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_arrow = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
    "\n",
    "def get_board_dims(frozenlake_env):\n",
    "    height, width = frozenlake_env.unwrapped.desc.shape\n",
    "    observation_size = height * width\n",
    "    return observation_size, height, width\n",
    "\n",
    "\n",
    "class NoWindowWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # Set up pygame with a dummy video driver to prevent window creation\n",
    "        os.environ['SDL_VIDEODRIVER'] = 'dummy'\n",
    "        pygame.init()\n",
    "    def observation(self, *args):\n",
    "        return self.env.observation(*args)\n",
    "\n",
    "def interactive_play(env, evaluation=None, display_arrows=True, cell_size=1, use_keys=False):\n",
    "    height, width = env.unwrapped.desc.shape\n",
    "    plot_width = width * cell_size\n",
    "    plot_height = height * cell_size\n",
    "\n",
    "    def on_action(action):\n",
    "        nonlocal move_count\n",
    "        move_count += 1\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        update_view(state, reward=reward, terminated=terminated)\n",
    "\n",
    "        return terminated\n",
    "\n",
    "    def draw_arrows(ax, evaluation):\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                state = y * width + x\n",
    "                try: \n",
    "                    # In wrapped environments, we need the modified observation\n",
    "                    observation = env.observation(state).to(device)\n",
    "                except AttributeError:\n",
    "                    observation = state\n",
    "                    \n",
    "                q_values = [evaluation(observation, action) for action in range(4)]\n",
    "\n",
    "                text_rows = [f\"{action_to_arrow[action]}: {str(round(q_values[action], 2))}\" for action in range(4)]\n",
    "                text = \"\\n\".join(text_rows)\n",
    "                plt.text(x / width + 1/(2*width), \n",
    "                         1 - (y / height + 1/(2*height)),  # NOTE: y-coordinates are flipped\n",
    "                         text, fontsize=10, \n",
    "                         ha='center', va='center', \n",
    "                         color='black', \n",
    "                         alpha=1,\n",
    "                         transform = ax.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "    def update_view(state, reward=0, terminated=False):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(plot_width, plot_height))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        ax.imshow(env.render())\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        if display_arrows and evaluation is not None:\n",
    "            draw_arrows(ax, evaluation)\n",
    "            \n",
    "        # plt.savefig('frozenlake_frame.png')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Previous reward: \" + str(reward))\n",
    "        if not terminated:\n",
    "            print(\"Move count: \", move_count)\n",
    "            if not use_keys:\n",
    "                display(grid)\n",
    "        else: \n",
    "            if reward == 1:\n",
    "                print(\"Game won!\")\n",
    "            else: \n",
    "                print(\"Game lost!\")\n",
    "\n",
    "\n",
    "    env = NoWindowWrapper(env)\n",
    "    state, _ = env.reset()\n",
    "    move_count = 0\n",
    "    \n",
    "\n",
    "    if not use_keys:\n",
    "        buttons = {}\n",
    "\n",
    "        for action in range(4):\n",
    "            button = widgets.Button(description=action_to_arrow[action])\n",
    "            button.action_key = action\n",
    "            # Attach the click event to the function\n",
    "            button.on_click(lambda b: on_action(b.action_key))\n",
    "\n",
    "            buttons[action] = button\n",
    "\n",
    "        # Display the buttons in a grid layout\n",
    "        grid = widgets.GridBox(\n",
    "            children=[buttons[action] for action in range(4)],\n",
    "            layout=widgets.Layout(\n",
    "                width='700px',\n",
    "                grid_template_columns='25% 25% 25% 25%',\n",
    "                grid_template_rows='auto',\n",
    "                grid_gap='5px'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    update_view(state)\n",
    "\n",
    "    if use_keys:\n",
    "        key_to_action = {'a': 0, 's': 1, 'd': 2, 'w': 3}\n",
    "\n",
    "        while True:\n",
    "            key = input(\"Move (W/A/S/D or Q to quit): \").lower()\n",
    "            if key == 'q':\n",
    "                break\n",
    "            if key in key_to_action:\n",
    "                terminated = on_action(key_to_action[key])\n",
    "                if terminated:\n",
    "                    break\n",
    "    \n",
    "\n",
    "\n",
    "def generate_gif(env, agent, n_frames=100, filename='animation', display_result=True):\n",
    "    \"\"\"Generates a gif of the agent playing. NOTE: this function is not targeted to be used for FrozenLake\"\"\"\n",
    "    frames = []\n",
    "    image_folder = \"frames_temp\"\n",
    "    os.makedirs(image_folder, exist_ok=True)\n",
    "\n",
    "    env = NoWindowWrapper(env)\n",
    "    \n",
    "    observation = env.reset()[0]\n",
    "\n",
    "    total_reward = 0\n",
    "    # Render the environment and save images\n",
    "    for i in range(n_frames):  \n",
    "        action = agent.act(observation, epsilon=0)\n",
    "        next_observation, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        frame = env.render()\n",
    "        observation = next_observation\n",
    "\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Save the frame as an image\n",
    "        image_path = f\"{image_folder}/frame_{i:04d}.png\"\n",
    "        imageio.imwrite(image_path, frame)\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Total reward: {total_reward}\")\n",
    "            observation = env.reset()[0]\n",
    "            total_reward = 0\n",
    "\n",
    "    # Create a GIF from the saved frames\n",
    "    image_files = [f\"{image_folder}/frame_{i:04d}.png\" for i in range(len(frames))]\n",
    "    images = [imageio.imread(image_file) for image_file in image_files]\n",
    "    imageio.mimsave(filename + '.gif', images, duration=0.05)\n",
    "\n",
    "    # Remove the frames folder after creating the GIF\n",
    "    shutil.rmtree(image_folder)\n",
    "\n",
    "    if display_result:\n",
    "        display(Image(filename='animation.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of the environment is specified by the contents of the grid cells:\n",
    "- start (S)\n",
    "- frozen (F)\n",
    "- holes (H)\n",
    "- goal (G)\n",
    "\n",
    "Programmatically we can represent it as a list of strings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = [\n",
    "    'SFFF',\n",
    "    'FHFH',\n",
    "    'FFFH',\n",
    "    'HFFG'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's first try the environment in a safe setting, where the slipperiness of the lake is turned off. \n",
    "\n",
    "Run the below cell to launch an instance of the environment. You can control the agent with the arrow buttons below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, desc=desc, render_mode='rgb_array')\n",
    "interactive_play(env)\n",
    "\n",
    "## Use this version on Snowflake\n",
    "# interactive_play(env, use_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To restart the game, simply rerun the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slippery lake\n",
    "\n",
    "On a slippery lake, the player does not always move as intended, but may slip and move to the sides instead. For example, when trying to move up, the agent only moves up with a probability of 33%, while there is a 33% chance of moving right, and a 33% chance of moving left.  \n",
    "\n",
    "The logic is easiest to understand by trying it out yourself: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = [\n",
    "    \"SFFF\",\n",
    "    \"FFFH\",\n",
    "    \"FFFF\",\n",
    "    \"FFFF\",\n",
    "    \"HFFG\"\n",
    "]\n",
    "env = gym.make('FrozenLake-v1', desc=desc, is_slippery=True, render_mode='rgb_array')\n",
    "\n",
    "interactive_play(env)\n",
    "\n",
    "## Use this version on Snowflake\n",
    "# interactive_play(env, use_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "Before training an agent for the FrozenLake environment, we need to go through a bit of background. The formal model for reinforcement learning is the *Markov Decision Process* (MDP). A Markov decision process is defined by \n",
    "- $S$: a set of *states*. There is a specific *initial state* $s_{\\text{initial}} \\in S$ and a set of *final states* $S_{\\text{final}} \\subseteq S$. \n",
    "- $A$: a set of *actions*\n",
    "- $T$: *transition function*. $T(s', a, s) = P(s' \\mid s, a)$ gives the conditional probability of moving to state $s'$ when taking action $a$ in state $s$. In a *Markov process*, the transition depends only on the current state and action.\n",
    "- $R$: *reward function*. $R(s', a, s)$ gives the reward for moving to state $s'$ by taking action $a$ in state $s$.\n",
    "\n",
    "In *partially observable* environments, the agent does not know the full state of the environment. Instead, it receives an *observation* of the environment. For example, in Minesweeper a player only knows the numbers revealed by their moves, not where the mines are initially placed. \n",
    "\n",
    "### Example: FrozenLake\n",
    "- $S = \\{(x,y): x \\in [\\text{width}], y \\in [\\text{height}]\\}$. This is a set of coordinates, specifying the position of the agent. \n",
    "- $A = \\{$ left, up, right, down $\\}$\n",
    "- $R$: the reward is 1 for moving to the target square. By default, falling into a hole does not give a negative reward (but we could easily change that). \n",
    "\n",
    "The pictures below illustrate how the transition probabilities work: \n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/f08unT640Ux6kDB/download\" width=\"480\" height=\"400\" style=\"vertical-align: top; margin-right: 20px;\"/>\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/3coIl6Tvw0tQsRS/download\" width=\"490\" height=\"440\" style=\"vertical-align: top;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, if a move would take the player outside of the grid, the player remains in the current square. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "We call a player acting in the environment an *agent*. An agent is specified by its *policy* $\\pi: S \\rightarrow A$, a map from states to actions. In a deterministic policy, the agent always picks the same action for a given state. Policies can also be stochastic (for example in the case of $\\epsilon$-greedy exploration).\n",
    "\n",
    "### Objective \n",
    "\n",
    "The goal in reinforcement learning is to find a policy that maximizes the *cumulative expected reward*: \n",
    "$$\\mathbb{E} \\left[\\sum_{t=0}^\\infty \\gamma^t R(s_{t+1}, a_t, s_t) \\right]$$\n",
    "Here $s_t, a_t$ denote the state and action at time step $t$, respectively. The expectation is taken over the transitions: the next state $s_{t+1}$ is sampled according to the distribution $P(s_{t+1} | s_t, a_t)$.\n",
    "\n",
    "$\\gamma \\in [0,1]$ is a *discount factor*. A low value of $\\gamma$ encourages the agent to pay more attention to rewards in the near future. Conversely, when $\\gamma$ is close to 1, distant rewards are almost as valuable. The role of $\\gamma$ is important - the optimal policy usually depends on its value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your skills\n",
    "\n",
    "Once you feel sure on your feet, you can try a more difficult instance of the FrozenLake environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = [\n",
    "    \"FHHH\",\n",
    "    \"SFFG\",\n",
    "    \"FHHF\",\n",
    "    \"FFFF\"\n",
    "]\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=desc, is_slippery=True, render_mode='rgb_array')\n",
    "interactive_play(env)\n",
    "\n",
    "## Use this version on Snowflake\n",
    "# interactive_play(env, use_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/1Yb505EmKgEh1v0/download\" width=\"400\" height=\"400\" />\n",
    "\n",
    "In this exercise, we test your understanding of the environment, policy, and the effect of $\\gamma$. In the environment above, what is the optimal policy when rewards are not discounted, i.e. $\\gamma=1$? Be prepared to give a high level description of the policy. An intuitive explanation for why the policy is optimal is enough (there is no need to actually calculate the expected reward).\n",
    "\n",
    "*Hint:* moving towards the wall is allowed. This is sometimes beneficial to avoid falling into a hole. \n",
    "\n",
    "**Optional challenge question:** What is the optimal policy when $\\gamma$ is very small, e.g. $\\gamma=0.1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When gamma = 1, we simply care that we reach the goal and not how many steps we take. When gamma is very small, we search for the most optimal/fastest way to reach the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the gymnasium library\n",
    "\n",
    "In this notebook, we use the [gymnasium library](https://gymnasium.farama.org/index.html) (gym for short), originally developed by OpenAI for training and testing RL agents. FrozenLake is one of many prebuilt environments in the library. The core benefit of gym is the standardized environment interface. This means that we can easily write agents that can be trained in different environments, without requiring major modifications to the agent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every gym environment has these four methods implemented: \n",
    "- `step`: Takes a given action in the environment. Returns an observation, reward, flag for reaching a final state, and some other information. \n",
    "- `reset`: Puts the environment in its initial state. Returns an observation. \n",
    "- `render`: Displays the state of the environment. \n",
    "- `close`: Closes the environment. This is only needed when we render the environment using external libraries. \n",
    "\n",
    "Please see the [documentation](https://gymnasium.farama.org/api/env/) for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spaces\n",
    "\n",
    "Spaces are used to describe actions and observations. The simplest type of space is given by the `Discrete` class, which describes a single number, usually from 0 to $n-1$. Later in this notebook we will have environments with non-discrete observations. There we use the `Box` space, which is a box in $\\mathbb{R}^n$ (cartesian product of closed intervals). In FrozenLake, the action space consists of the 4 possible actions (left, up, right, down) encoded as integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space is encoded as integers $0, \\dots, \\text{width} \\times \\text{height}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* we can use the term state and observation interchangeably in FrozenLake, because the environment is *fully observable*, that is, observations are equal to the state of the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "*Q-learning* is an RL algorithm that tries to learn the value of taking an action in a particular state. This value, denoted $Q(s,a)$, includes the reward from the next step, as well as the discounted reward from all future steps. \n",
    "\n",
    "In practice, we can think of $Q$ as a two-dimensional list `Q` of numbers with size $|S| \\times |A|$, where `Q[s,a]` contains the respective state-action value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.random.random((env.observation_space.n, env.action_space.n))  # filled with random values for illustration purposes\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is knowing $Q(s,a)$ useful? If we knew the correct state-action values, we could set our policy $\\pi$ to play the action $a = \\text{argmax}_{a \\in A} Q(s,a)$, i.e.  the action that maximizes the expected reward. If the state-action values are correct, this policy is **the best we can do**.\n",
    "\n",
    "### Task 2: Greedy policy\n",
    "In this task, you need to implement a simple policy that picks the action with the best Q-value. Given a state $s$ and the Q-table, return the action $\\text{argmax}_{a \\in A} Q(s,a)$. In case multiple actions have the same Q value, the function can return one of them arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "##### Exercise #####\n",
    "\n",
    "def act_greedy(state, Q):\n",
    "    action = Q[state].argmax()\n",
    "    return action\n",
    "print(act_greedy(0, Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning algorithm\n",
    "Obviously $Q(s,a)$ is not actually known, and computing it explicitly is veeery expensive because we would need to consider all possible state-action sequences. \n",
    "\n",
    "The Q-learning algorithm tries to learn the Q-values. Suppose we have some initial estimate of the $Q$ function (which could even be a zero-initialized array). Suppose we take an action $a$ in state $s$, get some reward $r$, and end up in state $s'$. Observing a reward and the next state gives a new estimate for $Q(s,a)$:\n",
    "\n",
    "$$r + \\gamma \\max_{a' \\in A} Q(s', a')$$\n",
    "\n",
    "where $\\gamma$ is the discount factor for future rewards, and $\\max_{a' \\in A} Q(s', a')$ is the estimated future rewards from state $s'$ onwards, when we take the action $a'$ that seems to give the best rewards. Using this, we can update our original estimate: \n",
    "\n",
    "$$Q^{new}(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha \\cdot \\left(r + \\gamma \\max_{a' \\in A} Q(s', a')\\right)$$\n",
    "Here $\\alpha \\in [0,1]$ is a learning rate parameter that controls how gradually we update the estimate. A higher learning rate gives more weight to the new estimate $r + \\gamma \\max_{a' \\in A} Q(s', a')$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:\n",
    "Implement the Q-learning update as described above. \n",
    "\n",
    "*Note:* If `terminated=True`, the value of $r + \\gamma \\max_{a' \\in A} Q(s', a')$ should only be $r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "##### Exercise #####\n",
    "\n",
    "def learn_from_experience(Q, state, action, reward, next_state, terminated, alpha, gamma):\n",
    "    if terminated == False:\n",
    "        Q[state][action] = (1-alpha) * Q[state][action] + alpha * (reward + gamma * max(Q[next_state]))\n",
    "    else:\n",
    "        Q[state][action] = (1-alpha) * Q[state][action] + reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs exploitation\n",
    "\n",
    "In this section, we look at the tradeoff between *exploring* new strategies and *exploiting* our current knowledge. \n",
    "\n",
    "Recall that selecting $a = \\text{argmax}_{a \\in A} Q(s,a)$ is the best possible policy **if** we know the $Q$-function exactly. However, we are only in the process of learning $Q$. \n",
    "Using the greedy policy by selecting $a = \\text{argmax}_{a \\in A} Q(s,a)$ means taking advantage of what we currently know. However, the $Q$-values may be inaccurate, so there may be undiscovered value in taking some other action $a'$. In other words, using the greedy policy may get our agent stuck in local optima. \n",
    "\n",
    "#### $\\epsilon$-greedy exploration\n",
    "\n",
    "To balance exploitation, we need to do *exploration* to discover new strategies. The simplest way to do this is to simply try random moves. An $\\epsilon$-greedy exploration policy does this by \n",
    "- selecting a random action $a \\in A$ with probability $\\epsilon$\n",
    "- using $a = \\text{argmax}_{a \\in A} Q(s,a)$ with probability $1-\\epsilon$ \n",
    "\n",
    "### Task 4\n",
    "Implement the $\\epsilon$-greedy exploration policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "##### Exercise #####\n",
    "\n",
    "def epsilon_greedy_policy(state, Q, epsilon):\n",
    "    actions = np.arange(Q.shape[1])  # [0, 1, 2, 3]\n",
    "    if random.random() < epsilon:\n",
    "        action = np.random.choice(actions)\n",
    "    else:\n",
    "        action = act_greedy(state, Q)\n",
    "    return action\n",
    "epsilon_greedy_policy(0, Q, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-scheduler\n",
    "\n",
    "The value of the $\\epsilon$ is controlled through the training process. Usually, we start with a higher value to give more priority to exploration, and gradually decrease $\\epsilon$ as training progresses. We use a linear scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(episode, n_episodes, epsilon_start=1, epsilon_end=0):\n",
    "    return epsilon_end + (epsilon_start - epsilon_end) * (1 - episode / n_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an agent\n",
    "\n",
    "We will combine all the tools built above to train an agent. We will use the following FrozenLake instance. \n",
    "\n",
    "To make sure that the agent doesn't get stuck, we limit the maximum number of actions taken by `max_steps`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = [\n",
    "    \"SFFH\",\n",
    "    \"FHFF\",\n",
    "    \"FFFF\",\n",
    "    \"FFFH\",\n",
    "    \"HFFG\"\n",
    "]\n",
    "\n",
    "# Please don't change these values\n",
    "max_steps = 300\n",
    "gamma = 0.99\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=desc, is_slippery=True, render_mode='rgb_array', max_episode_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive_play(env)\n",
    "\n",
    "## Use this version on Snowflake\n",
    "# interactive_play(env, use_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can uncomment the above cell to try the map. There exists a policy that always succeeds, and our agent should be able to find it (can you?). \n",
    "\n",
    "### Run a single episode\n",
    "The below function should run a single game (episode) in the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, Q, gamma, epsilon, alpha):\n",
    "    \"\"\"\n",
    "    Run a single episode of the game using the Q-learning algorithm.\n",
    "\n",
    "    Args:\n",
    "        env: gym environment\n",
    "        Q: numpy array of shape (n_states, n_actions)\n",
    "        gamma: float\n",
    "        max_steps: int\n",
    "        epsilon: float\n",
    "        alpha: learning rate (float)\n",
    "    \n",
    "    Returns:\n",
    "        total_reward: float. For FrozenLake this is 1 for reaching the goal, \n",
    "            0 for falling into a hole or running out of steps)\n",
    "        lost: bool, indicating whether the agent fell into a hole\n",
    "        truncated: bool, indicating whether the episode was ended due to reaching max_steps\n",
    "    \"\"\"\n",
    "\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(state, Q, epsilon)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        learn_from_experience(Q, state, action, reward, new_state, terminated, alpha, gamma)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = new_state\n",
    "\n",
    "        # End the episode if the agent reaches the goal or falls into a hole (terminated=True)\n",
    "        # or if the episode reaches the maximum number of steps (truncated=True)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    lost = total_reward == 0 and (not truncated)  # NOTE: this only holds for FrozenLake\n",
    "    return total_reward, lost, truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "\n",
    "The main loop simply runs the above function `n_episodes` times to train the agent. Additionally, every `test_interval` episodes, we run a few test episodes with $\\epsilon=0$ to see how well the current greedy policy performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Q_agent(env,\n",
    "                  Q,\n",
    "                  n_episodes,  \n",
    "                  test_interval, \n",
    "                  n_test_episodes, \n",
    "                  lr, \n",
    "                  gamma, \n",
    "                  epsilon_start=1, \n",
    "                  epsilon_end=0):\n",
    "\n",
    "    episode = 0\n",
    "    while episode < n_episodes + 1:\n",
    "        if episode % test_interval == 0:\n",
    "            results = [run_episode(env, Q, gamma, 0, lr) for _ in range(n_test_episodes)]\n",
    "            test_rewards = [result[0] for result in results]\n",
    "            lost = [result[1] for result in results]\n",
    "            truncated = [result[2] for result in results]\n",
    "            print(\n",
    "                f\"Episode: {episode}, won: {round(100*np.mean(test_rewards),2)}%\",\n",
    "                f\"lost: {round(100* sum(lost) / n_test_episodes,2)}%\",\n",
    "                f\"truncated: {round(100* sum(truncated) / n_test_episodes,2)}%\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            epsilon = get_epsilon(episode, n_episodes, epsilon_start, epsilon_end)\n",
    "            _ = run_episode(env, Q, gamma, epsilon, lr)\n",
    "        \n",
    "        episode += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Training the agent\n",
    "\n",
    "Below we define hyperparameters and call the main training loop. The default hyperparameters should work, assuming your solutions to the previous tasks are correct. \n",
    "\n",
    "**The agent should find a strategy that works at least 99% of the time**. The optimal policy never falls into holes on this map, but there is a very small chance that it does not reach the goal within `max_steps` steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Exercise #####\n",
    "\n",
    "n_episodes = 30000\n",
    "alpha = 0.01  # learning rate\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0\n",
    "test_interval = n_episodes // 20\n",
    "n_test_episodes = 30\n",
    "\n",
    "observation_size, _, _ = get_board_dims(env)\n",
    "n_actions = 4\n",
    "\n",
    "# Initialize the Q table\n",
    "Q = np.zeros((observation_size, n_actions))\n",
    "\n",
    "# Train the agent\n",
    "train_Q_agent(env, Q, n_episodes, test_interval, n_test_episodes, \n",
    "              alpha, gamma, epsilon_start=epsilon_start, epsilon_end=epsilon_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Q-values\n",
    "After training an agent, you can open the interactive environment and inspect the learned Q-values. \n",
    "\n",
    "*Note:* You can simulate your agent by selecting the action with the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to access state-action values \n",
    "evaluation = lambda state, action: Q[state, action]\n",
    "\n",
    "interactive_play(env, evaluation=evaluation, display_arrows=True)\n",
    "\n",
    "## Use this version on Snowflake\n",
    "# interactive_play(env, evaluation=evaluation, display_arrows=True, use_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging tips (optional)\n",
    "\n",
    "Debugging RL agents can be a bit tricky. Here's a few suggestions to try in case your agent is not doing well:\n",
    "\n",
    "- **Inspect the Q-values (see above):** The values should all be within $[0,1]$, because the largest possible total reward is 1 and there are no negative rewards. \n",
    "- **Training the agent in a simpler environment:** You can try to turn off the slipperiness by setting `slippery=False` or use a simpler map. For this, you may also try changing the value of $\\gamma$ because the Q-values can be easier to interpret when $\\gamma$ is small. Remember to set $\\gamma=0.99$ once you're done with the tests. Here's a few environments you can try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = [\n",
    "    \"SFFG\"\n",
    "]\n",
    "\n",
    "desc = [\n",
    "    \"SFFF\",\n",
    "    \"FFFF\",\n",
    "    \"FFFF\",\n",
    "    \"FFFG\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Printing:** You can also try to print the loss (difference between $Q(s,a)$ and the updated estimate), the value of $\\epsilon$, and the number of steps taken in each episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "Table-based Q-learning works well if the number of observations (states) and actions is limited. This restricts table-based Q-learning to environments where observations are discrete. While there are some workarounds for this (discretization), there is a more fundamental issue in table-based methods:\n",
    "\n",
    "### Curse of dimensionality\n",
    "In complex environments, observations usually contain information in multiple dimensions. For example, consider the [Humanoid environment](https://gymnasium.farama.org/environments/mujoco/humanoid/), where a bipedal robot learns to walk. In this environment, each observation includes coordinates, velocities and angles between various body parts, for a total of 376 dimensions! Even if each dimension is represented by a few discrete values (e.g. 10), we would need a $Q$-table with $10^{376}$ rows!\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/humanoid.gif\" width=\"400\" height=\"400\" />\n",
    "\n",
    "## Deep Q-Networks\n",
    "\n",
    "The idea in deep Q-learning is to replace the Q-table with a deep Q-network (DQN), which is simply a deep neural network for approximating the Q-values. We give the observation as input to the network, and it returns Q-values for each action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "We modify the training loop by storing transitions in the environment as  *experiences* in a replay buffer. An experience is a tuple $e = (\\text{observation}_t, \\text{action}_t, \\text{reward}_t, \\text{observation}_{t+1}, \\text{terminated}_{t+1})$, describing a single step in the environment. To train the model, we randomly sample a minibatch from the replay buffer and run backpropagation on the minibatch. There are two major benefits in using a replay buffer: \n",
    "1. **Stability**: Experiences randomly sampled from a large buffer represent a broad sample, whereas consequtive experiences from the environment are heavily correlated.\n",
    "2. **Cost**: Sometimes generating experiences from the environment is more expensive than training the model with those experiences. Using a replay buffer allows us to reuse experiences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "\n",
    "Experience = namedtuple('Experience',\n",
    "                        ('observation', 'action', 'reward', 'next_observation', 'terminated'))\n",
    "# terminated = won or lost. truncation does not affect this\n",
    "# handling this detail correctly is quite important\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Push new experience to the memory.\n",
    "        As the buffer is a deque with a fixed maxlen, oldest experiences will be automatically discarded\"\"\"\n",
    "        self.buffer.append(Experience(*args))\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Sample a batch of experiences from the memory.\"\"\"\n",
    "        assert len(self.buffer) >= self.batch_size\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        observations, actions, rewards, next_observations, terminateds = zip(*batch)\n",
    "\n",
    "        observations = torch.stack(observations).to(device)  # (batch_size, observation_size)\n",
    "        actions = torch.LongTensor(actions).to(device)  # (batch_size,)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)  # (batch_size,)\n",
    "        next_observations = torch.stack(next_observations).to(device)  # (batch_size, observation_size)\n",
    "        terminateds = torch.BoolTensor(terminateds).to(device)  # (batch_size,)\n",
    "\n",
    "        return observations, actions, rewards, next_observations, terminateds\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function runs a single episode. It looks very similar to what we used before, the only difference being saving the experience to the replay buffer instead of training on it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent, epsilon, train=True, add_to_memory=True):\n",
    "    \"\"\"\n",
    "    Run a single episode.\n",
    "\n",
    "    Args:\n",
    "        env: gym environment\n",
    "        agent: DQNAgent\n",
    "        epsilon: float\n",
    "        train: bool, whether to train the agent\n",
    "    \n",
    "    Returns:\n",
    "        total_reward: float\n",
    "    \"\"\"\n",
    "\n",
    "    observation, info = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    episode_length = 0\n",
    "    losses = []\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(observation, epsilon)\n",
    "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        if add_to_memory:\n",
    "            agent.memory.push(observation, action, reward, new_observation, terminated)\n",
    "            \n",
    "        if train:\n",
    "            loss = agent.optimize_model()\n",
    "            losses.append(loss)\n",
    "\n",
    "        total_reward += reward\n",
    "        observation = new_observation\n",
    "        episode_length +=1\n",
    "        agent.step_count += 1\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "    losses = [loss for loss in losses if loss is not None]\n",
    "    average_loss = sum(losses) / max(len(losses),1)\n",
    "\n",
    "    return total_reward, episode_length, average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN for FrozenLake\n",
    "\n",
    "We will first apply the DQN agent to the FrozenLake environment. This is a good starting point because debugging the agent in this simple environment is relatively easy. \n",
    "\n",
    "\n",
    "### Wrappers\n",
    "\n",
    "Currently observations (states) are given as an integer in $0, \\dots, \\text{width} \\cdot \\text{height}$. This is not the most convenient representation for deep neural networks, which usually work better with vector inputs, so we transform the observation to a one-hot encoded representation. \n",
    "\n",
    "We can do this using a [wrapper](https://gymnasium.farama.org/api/wrappers/). Wrappers are a general tool in the gym library to modify some aspect of the environment, without altering the code of the environment or the agent directly. In fact, we already used a TimeLimit wrapper before to impose a bound on the number of steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(OneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        n = env.observation_space.n\n",
    "        self.n = n\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=1, shape=(n,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        one_hot_obs = np.zeros(self.n)\n",
    "        one_hot_obs[obs] = 1.0\n",
    "        return torch.tensor(one_hot_obs).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this does in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = [\n",
    "    \"SFFH\",\n",
    "    \"HFFG\"\n",
    "]\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=desc, render_mode='rgb_array')\n",
    "env = OneHotWrapper(env)\n",
    "\n",
    "# env.reset returns the initial state / observation\n",
    "obs = env.reset()[0]\n",
    "obs = obs.to(device)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "In principle, we can use any deep neural network (with the appropriate input and output dimensions) as the Q-network. For simplicity, let's not use a deep neural network right away. Rather, we will apply a very shallow network, with no hidden layers at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of squares on the board\n",
    "observation_size, _, _ = get_board_dims(env)\n",
    "\n",
    "# Linear layer with 4 outputs corresponding to the different actions\n",
    "net = nn.Linear(observation_size, 4)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding the observation to the network gives the Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with a single linear layer and a one-hot encoded input, each  state-action pair corresponds to a weight in the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training the Deep Q-network\n",
    "\n",
    "Now that we use a neural network for the Q-values, we need to train it using backpropagation. Recall the Q-value update from before \n",
    "\n",
    "$$Q^{new}(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha \\cdot \\left(r + \\gamma \\max_{a' \\in A} Q(s', a')\\right)$$\n",
    "\n",
    "We think of $Q(s,a)$ as the *current estimate*, and $r + \\gamma \\max_{a' \\in A} Q(s', a')$ as the *target estimate*. The target is analogous to the *target* $y$ in supervised learning.\n",
    "\n",
    "Training involves minimizing the difference between the current and the target estimate using a loss function. Typical choices include the L2 loss $(x-y)^2$ or the L1 loss $|x-y|$. We use [SmoothL1Loss](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html), which is a combination of the L1 and L2 losses. \n",
    "\n",
    "After computing the loss, we run backpropagation to move the current estimate $Q(s,a)$ closer to the target. **Gradients are only applied to the  current estimate and not the target.** It may be helpful to think of this in terms of supervised learning, where we apply gradients to the *model prediction* (current estimate), and not the target $y$ (target estimate). In practice, the target should be computed inside a `torch.no_grad()` block. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the loss function\n",
    "\n",
    "Let's first generate some experiences to see what a minibatch will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ExperienceReplay(capacity=10000, batch_size=4)\n",
    "\n",
    "# We use a random agent for illustration purposes\n",
    "class RandomAgent:\n",
    "    def __init__(self, env, memory):\n",
    "        self.env = env\n",
    "        self.memory = memory\n",
    "        self.step_count = 0\n",
    "\n",
    "    def act(self, observation, epsilon):\n",
    "        return self.env.action_space.sample()\n",
    "\n",
    "agent = RandomAgent(env, memory)\n",
    "\n",
    "env.reset()\n",
    "for _ in range(10):\n",
    "    run_episode(env, agent, 0, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sample a minibatch of experiences. You can use the next cell to see the type of tensors the loss computation will take as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations, actions, rewards, next_observations, terminateds = memory.sample()\n",
    "observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Calculate loss\n",
    "\n",
    "Your task is to implement the loss computation. The comments provide a basic structure. You can assume `batch_size` $\\ge 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Exercise #####\n",
    "\n",
    "def _compute_DQN_loss(observations, actions, rewards, next_observations, terminateds, net, gamma, criterion):\n",
    "    \"\"\"\n",
    "    Compute the loss for a batch of experiences.\n",
    "\n",
    "    Args:\n",
    "        observations: torch tensor of shape (batch_size, observation_size)\n",
    "        actions: torch tensor of shape (batch_size,)\n",
    "        rewards: torch tensor of shape (batch_size,)\n",
    "        next_observations: torch tensor of shape (batch_size, observation_size)\n",
    "        terminateds: torch tensor of shape (batch_size,)\n",
    "        net: DQN\n",
    "        gamma: float\n",
    "        criterion: loss function\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar tensor \n",
    "    \"\"\"\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    # 1. Compute the Q values for the current observations-action pairs\n",
    "    #   - use the net to compute the Q values for the current observations \n",
    "    #   - select the Q values corresponding to the played actions\n",
    "    # Hint: for handling torch.tensors, the gather, unsqueeze and squeeze methods may be useful\n",
    "    \n",
    "    Q = net(observations)\n",
    "    Q = Q.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 2. Compute the target estimate (see the formula above)\n",
    "        #   - compute Q values for the next observations (max over actions)\n",
    "        #   - multiply by gamma\n",
    "        #   - set the Q values for terminated states to 0\n",
    "        #   - add rewards\n",
    "        next_Q = net(next_observations)\n",
    "        next_Q = next_Q.max(dim=1).values * gamma\n",
    "        next_Q[terminateds] = 0\n",
    "        next_Q += rewards\n",
    "        \n",
    "\n",
    "    # 3. Use `criterion` to compute the loss and return it\n",
    "    loss = criterion(Q, next_Q)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell allows you to test the loss computation (mainly to see if there are shape mismatches or type errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_compute_DQN_loss(observations, actions, rewards, next_observations, terminateds, net, 0.9, nn.SmoothL1Loss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class\n",
    "\n",
    "The agent is implemented in a `DQNAgent` class below. The class has two main methods: \n",
    "- `act`: Takes in an observation and epsilon, and acts according to the $\\epsilon$-greedy policy. This is similar to the $\\epsilon$-greedy policy implemented for the table-based agent. Note that we use `torch.no_grad` to not track the gradient here, because we only need the forward pass when generating experiences. \n",
    "- `optimize_model`: This is called in every iteration of the training loop. The method samples a minibatch from the experience replay, computes loss for the minibatch (using `_compute_DQN_loss` implented above) and runs backpropagation to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, net, n_actions, compute_loss, gamma=0.98, lr=0.01, memory_capacity=100000, batch_size=16):\n",
    "        self.net = net\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ExperienceReplay(capacity=memory_capacity, batch_size=batch_size)\n",
    "        self.compute_loss = compute_loss\n",
    "        self.optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def act(self, observation, epsilon):\n",
    "        \"\"\"\n",
    "        Select an action according to an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            observation: torch.tensor of shape (observation_size,)\n",
    "            epsilon: float\n",
    "\n",
    "        Returns:\n",
    "            action: int\n",
    "        \"\"\"\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(range(self.n_actions))\n",
    "        else: \n",
    "            observation = observation.to(device)\n",
    "\n",
    "            # NOTE: torch.no_grad() disables gradient tracking.\n",
    "            # We only need the forward pass, so this saves memory and computation\n",
    "            with torch.no_grad():\n",
    "                return self.net(observation).argmax().item()\n",
    "            \n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        observations, actions, rewards, next_observations, terminateds = self.memory.sample()\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = self.compute_loss(observations, actions, rewards, \n",
    "                                next_observations, terminateds, \n",
    "                                self.net, self.gamma, self.criterion)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.net.parameters(), 100)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DQN_agent(env,\n",
    "                    agent,\n",
    "                    n_episodes,  \n",
    "                    test_interval, \n",
    "                    n_test_episodes, \n",
    "                    epsilon_scheduler):\n",
    "    episode = 0\n",
    "    best_test_reward = -math.inf\n",
    "\n",
    "    while episode < n_episodes:\n",
    "        if episode % test_interval == 0:\n",
    "            test_rewards = [run_episode(env, agent, 0, train=False, add_to_memory=False)[0] for _ in range(n_test_episodes)]\n",
    "            average_test_reward = np.mean(test_rewards)\n",
    "            print(f\"Episode: {episode}, average test reward: {(average_test_reward)}\")\n",
    "\n",
    "            if average_test_reward > best_test_reward:\n",
    "                print('New best test reward. Saving model')\n",
    "                best_test_reward = average_test_reward\n",
    "                torch.save(agent.net.state_dict(), 'policy_net.pth')\n",
    "\n",
    "        else:\n",
    "            epsilon = epsilon_scheduler(episode)\n",
    "            total_reward, episode_length, loss = run_episode(env, agent, epsilon, episode)\n",
    "        \n",
    "        episode += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use the following environment. For debugging, you can also use simpler environments or turn off slipperiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = [\n",
    "    \"SFFF\",\n",
    "    \"FHFF\",\n",
    "    \"FFFF\",\n",
    "    \"FFFH\",\n",
    "    \"HFFG\"\n",
    "]\n",
    "max_steps = 300\n",
    "gamma = 0.99\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=desc, is_slippery=True, render_mode='rgb_array', max_episode_steps=max_steps)\n",
    "env = OneHotWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters should work, but you can also try changing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 2000\n",
    "lr = 0.002\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0\n",
    "test_interval = n_episodes // 10\n",
    "n_test_episodes = 30\n",
    "memory_capacity = 10000\n",
    "\n",
    "n_actions = 4\n",
    "observation_size, _, _ = get_board_dims(env)\n",
    "\n",
    "net = nn.Linear(observation_size, n_actions).to(device)\n",
    "agent = DQNAgent(net, n_actions, _compute_DQN_loss, gamma=gamma, lr=lr, memory_capacity=memory_capacity)\n",
    "# Use a linear epsilon schedule\n",
    "epsilon_scheduler = lambda episode: epsilon_end + (epsilon_start - epsilon_end) * (1 - episode / n_episodes)\n",
    "\n",
    "train_DQN_agent(env, agent, n_episodes, test_interval, n_test_episodes, epsilon_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cell below to inspect the Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_values(obs, action):\n",
    "    \"\"\"Helper function to access state-action values.\"\"\"    \n",
    "    return agent.net(obs)[action].item()\n",
    "\n",
    "interactive_play(env, evaluation=get_q_values, display_arrows=True)\n",
    "\n",
    "## Use this version on Snowflake\n",
    "# interactive_play(env, evaluation=get_q_values, display_arrows=True, use_keys=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Basic DQN\n",
    "\n",
    "In this section, we will build upon the basic DQN model by introducing more advanced techniques, such as implementing a simple Q-network and optimizing it with Double DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Network architecture\n",
    "\n",
    "In order to facilitate parameter tuning, we implement a custom MLP class for the Q-network. The `hidden_size` parameter is a variable-length list of integers, describing the number of units for each hidden layer. The `activation` parameter can be used to control the activation function. \n",
    "\n",
    "**Note**: the last layer should not have an activation; the range of Q-values shouldn't be limited, since they can also be negative for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "##### Exercise #####\n",
    "\n",
    "class QNet(nn.Module): \n",
    "    def __init__(self, observation_size, n_actions, hidden_size=[128,128], activation=nn.ReLU):\n",
    "        \"\"\"\n",
    "        Initialize the network.\n",
    "\n",
    "        Args:\n",
    "            observation_size: int, dimension of the observation vector\n",
    "            n_actions: int, number of actions\n",
    "            hidden_size: [int], the number of units for each hidden layer\n",
    "            activation: \n",
    "        \"\"\"\n",
    "        super(QNet, self).__init__()\n",
    "\n",
    "        ## YOUR CODE HERE\n",
    "        ## Define the layers of the network\n",
    "        ##  - create a list of layers and activations \n",
    "        ##       - use activation() to instantiate the activation function\n",
    "        ##  - use nn.Sequential \n",
    "\n",
    "        layers = []\n",
    "        input_dim = observation_size\n",
    "\n",
    "        for hidden_dim in hidden_size:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(activation())\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, n_actions))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN\n",
    "\n",
    "Training DQN agents can be an unstable process. The instability is often linked to the overestimation of Q-values. To address this issue, we implement a technique known as [Double DQN](https://arxiv.org/abs/1509.06461). \n",
    "\n",
    "Double DQN uses two networks: a *policy network* $Q_{\\text{policy}}$ and a *target network* $Q_{\\text{target}}$. Both networks share the same architecture and are initialized with identical weights. The policy network is what we are actively training. The target net is a slightly delayed version of the policy net. It's weights are updated by copying the weights from the policy net every once in a while. \n",
    "\n",
    "The target net is used to stabilize the update equation (copied from above):\n",
    "\n",
    "$$Q(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha \\cdot \\left(r + \\gamma \\max_{a' \\in A} Q(s', a')\\right)$$\n",
    "\n",
    "In Double DQN, we first select the best action in the next state using the policy net: $a^* = \\arg\\max_{a' \\in A} Q_{\\text{policy}}(s',a')$. The value of this action is then evaluated with the target net. The modified equation looks like this: \n",
    "\n",
    "$$Q_{\\text{policy}}(s,a) \\leftarrow (1-\\alpha) Q_{\\text{policy}}(s,a) + \\alpha \\cdot \\left(r + \\gamma Q_{\\text{target}}(s', a^*)\\right)$$\n",
    "\n",
    "\n",
    "\n",
    "Every once in a while, we sync the two networks by setting $Q_{\\text{target}} \\leftarrow Q_{\\text{policy}}$. More generally, the networks can be gradually synchronized by setting $Q_{\\text{target}} \\leftarrow (1-\\tau) Q_{\\text{target}} + \\tau Q_{\\text{policy}}$ where $\\tau \\in ]0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Double-DQN Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "##### Exercise #####\n",
    "\n",
    "def _compute_DoubleDQN_loss(observations, actions, rewards, next_observations, terminateds, net, target_net, gamma, criterion):\n",
    "    \"\"\"\n",
    "    Compute the loss for a batch of experiences.\n",
    "\n",
    "    Args:\n",
    "        observations: torch tensor of shape (batch_size, observation_size)\n",
    "        actions: torch tensor of shape (batch_size,)\n",
    "        rewards: torch tensor of shape (batch_size,)\n",
    "        next_observations: torch tensor of shape (batch_size, observation_size)\n",
    "        terminateds: torch tensor of shape (batch_size,)\n",
    "        net: nn.Module\n",
    "        target_net: nn.Module\n",
    "        gamma: float\n",
    "        criterion: loss function\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Compute the Q values for the current observations-action pairs\n",
    "    # (You can copy your solution from Task 6, Part 1 here)\n",
    "    Q = net(observations)\n",
    "    Q = Q.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 2. Compute the target estimate (see the formula above)\n",
    "        #   - find actions maximizing Q-values in the next states using the policy net\n",
    "        #   - compute Q-values for these actions using the target net \n",
    "        #   - multiply by gamma\n",
    "        #   - set the Q values for terminated states to 0\n",
    "        #   - add rewards\n",
    "        policy_Q = net(next_observations)\n",
    "        next_actions = policy_Q.argmax(dim=1)\n",
    "        target_Q = target_net(next_observations)\n",
    "        next_Q = target_Q.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "        next_Q = next_Q * gamma\n",
    "        next_Q[terminateds] = 0\n",
    "        next_Q += rewards\n",
    "        \n",
    "\n",
    "    # 3. Use `criterion` to compute the loss and return it\n",
    "    # (You can copy your solution from Task 6, Part 3 here)\n",
    "    loss = criterion(Q, next_Q)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell generates batches of random experiences for you to test your implementation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ExperienceReplay(capacity=10000, batch_size=4)\n",
    "\n",
    "# We use the random agent from before for illustration purposes\n",
    "agent = RandomAgent(env, memory)\n",
    "\n",
    "env.reset()\n",
    "for _ in range(10):\n",
    "    run_episode(env, agent, 0, train=False)\n",
    "\n",
    "# sample a batch\n",
    "observations, actions, rewards, next_observations, terminateds = memory.sample()\n",
    "\n",
    "net = QNet(observation_size, n_actions)\n",
    "target_net = QNet(observation_size, n_actions)\n",
    "\n",
    "_compute_DoubleDQN_loss(observations, actions, rewards, next_observations, terminateds, net, target_net, 0.9, nn.SmoothL1Loss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double-DQN agent class\n",
    "\n",
    "The agent class for Double-DQN can be almost identical to `DQNAgent`. Additionally, the target network should be synced with the policy network every `update_tgt_interval` steps (e.g. in the `compute_loss` method). This can be achieved with the following code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for target_param, param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "#     target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the challenge, it's strongly recommended to apply Double-DQN (if you're using a DQN-based agent). To conclude the notebook, let's shift focus to a fundamentally different approach to RL.\n",
    "\n",
    "## Policy-Based Reinforcement Learning\n",
    "\n",
    "So far, we've explored *value-based* approaches, where the agent learns a value function that estimates expected future rewards for each state-action pair. But does it always make sense to assign precise values to everything? \n",
    "\n",
    "### A self-driving example\n",
    "\n",
    "Suppose we are training a self-driving car. The agent might learn action-values like:\n",
    "- Q(drive straight) = **1**\n",
    "- Q(swerve into a pedestrian) = **-100**\n",
    "\n",
    "The values feel a bit arbitrary. Does the *exact value* of these actions really matter? \n",
    "\n",
    "**The core issue:** Value-based methods assume everything can be compared on a numerical scale. However, in many cases, we only care about *relative preferences* — e.g., \"swerve into a pedestrian is worse than driving straight\" — not the exact numbers.\n",
    "\n",
    "\n",
    "### Understanding Policy-Based Methods\n",
    "\n",
    "Policy-based methods take a different route. Instead of relying on a Q-function, they directly learn a policy – a probability distribution over actions. \n",
    "\n",
    "The policy $\\pi_{\\theta}$ is parameterized by $\\theta$. The goal is to adjust $\\theta$ to increase the expected return $J(\\theta)$, which is done with the following:\n",
    "\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_{\\theta}} \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) \\cdot R_t$$\n",
    "\n",
    "where\n",
    "- the expectation is over *trajectories* $\\tau$ – the sequence of states, actions and rewards in an episode, with actions sampled from $\\pi_\\theta$\n",
    "- $\\pi_\\theta(a_t \\mid s_t)$ is the probability of taking the chosen action $a_t$ at step $t$\n",
    "- $R_t = \\sum_{k=t}^T \\gamma^{k-t} r_{k}$ is the total discounted reward starting from step $t$\n",
    "\n",
    "We use this gradient to update the policy in the direction that increases expected rewards:\n",
    "$$ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$$\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "This is the essence of the REINFORCE algorithm. Let's implement it:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import pygame\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class NoWindowWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # Set up pygame with a dummy video driver to prevent window creation\n",
    "        os.environ['SDL_VIDEODRIVER'] = 'dummy'\n",
    "        pygame.init()\n",
    "    def observation(self, *args):\n",
    "        return self.env.observation(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LunarLander environment\n",
    "\n",
    "Our agent is controlling a spacecraft landing on the moon. Check the [documentation](https://gymnasium.farama.org/environments/box2d/lunar_lander/).\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/lunar_lander.gif\" width=\"400\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LunarLander environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "env = NoWindowWrapper(env)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim, action_dim\n",
    "assert(state_dim == 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Policy Network\n",
    "\n",
    "We need a policy network mapping. The network takes in an observation with `state_dim` dimensions and outputs probabilities for each of the `action_dim` actions. An MLP with a few layers should be enough. The last layer of the network should have a softmax activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "##### Exercise #####\n",
    "\n",
    "# Define the policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        assert(state_dim == 8)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: The Agent\n",
    "\n",
    "Use the policy of the agent to get action probabilities. Choose an action according to the probabilities and return the action, as well as the (natural) logarithm of its probability. \n",
    "\n",
    "**Hint:** use `torch.distributions.Categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "##### Exercise #####\n",
    "\n",
    "class REINFORCE:\n",
    "    \"\"\"REINFORCE (Monte Carlo Policy Gradient) algorithm implementation\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=0.001, gamma=0.99):\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def act(self, state, greedy=False):\n",
    "        \"\"\"Select an action from the policy distribution\n",
    "        If greedy is True, select the action with the highest probability\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).to(device).unsqueeze(0)\n",
    "        \n",
    "        ## YOUR CODE HERE\n",
    "        # Get the action probabilities from the policy network\n",
    "        action_probs = self.policy(state)\n",
    "        if greedy:\n",
    "            action = action_probs.argmax(dim=1).item()\n",
    "            log_prob = torch.log(action_probs[0, action])\n",
    "        else:\n",
    "            # Sample an action from the policy distribution\n",
    "            m = Categorical(action_probs)\n",
    "            action = m.sample().item()\n",
    "            log_prob = m.log_prob(action)\n",
    "        return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Solution #####\n",
    "\n",
    "class REINFORCE:\n",
    "    \"\"\"REINFORCE (Monte Carlo Policy Gradient) algorithm implementation\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=0.001, gamma=0.99):\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def act(self, state, greedy=False):\n",
    "        \"\"\"Select an action from the policy distribution\n",
    "        If greedy is True, select the action with the highest probability\"\"\"\n",
    "        state = torch.FloatTensor(state).to(device).unsqueeze(0)\n",
    "        probs = self.policy(state)\n",
    "        if greedy:\n",
    "            action = torch.argmax(probs, dim=-1)\n",
    "            log_prob = torch.log(probs[0, action])\n",
    "            return action.item(), log_prob\n",
    "        else:\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample()\n",
    "            return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function runs a single episode, storing the sequence of states and actions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent, training=True):\n",
    "    \"\"\"\n",
    "    Run a single episode of the environment with the given agent.\n",
    "    \"\"\"\n",
    "    state, _ = env.reset()\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, log_prob = agent.act(state, greedy=not training)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    if training:\n",
    "        return sum(rewards), log_probs, rewards\n",
    "    else:\n",
    "        return sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Updating the Policy\n",
    "\n",
    "1. Convert rewards to dicounted future returns:\n",
    "    - Given a list of rewards $[r_1, \\dots, r_T]$, compute the list of discounted future returns where $R_t = \\sum_{j=t}^T \\gamma^{j-t} r_j$\n",
    "    - Hint: to do this efficiently, start from the end of the episode. \n",
    "    - Convert the result to a tensor, and normalize it by substracting the mean and dividing by the standard deviation. This improves training stability. \n",
    "\n",
    "2. Calculate policy loss\n",
    "    - The loss at time $t$ is defined as $\\text{loss}_t = - \\log \\pi_\\theta(a_t \\mid s_t) \\cdot R_t$. \n",
    "    - The minus sign appears because we are minimizing the loss, even though the underlying objective is to maximize returns.\n",
    "\n",
    "3. Backpropagate and update policy\n",
    "    - Zero the gradients, perform backpropagation on the total policy loss, and take one optimizer step.\n",
    "    - The function does not return anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "##### Exercise #####\n",
    "\n",
    "def update_policy(agent, log_probs, rewards):\n",
    "    \"\"\"Update policy parameters using policy gradient\"\"\"\n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    # 1. Convert rewards to discounted future returns\n",
    "    rewards = rewards[::-1]\n",
    "    discounted_rewards = []\n",
    "    R = 0\n",
    "    for reward in rewards:\n",
    "        R = reward + agent.gamma * R\n",
    "        discounted_rewards.append(R)\n",
    "    discounted_rewards = discounted_rewards[::-1]\n",
    "    discounted_rewards = torch.FloatTensor(discounted_rewards).to(device)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
    "    \n",
    "    # 2. Calculate policy loss\n",
    "    policy_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "    for i in range(len(log_probs)):\n",
    "        policy_loss = policy_loss - log_probs[i] * discounted_rewards[i]\n",
    "\n",
    "\n",
    "    # 3. Perform backpropagation and update the policy network\n",
    "    agent.optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    agent.optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function is used to train the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, num_episodes=2000, print_every=100, test_every=100, num_tests=10):\n",
    "    \"\"\"Train the agent and periodically evaluate its performance\"\"\"\n",
    "    episode_rewards = []\n",
    "    test_rewards = []\n",
    "    \n",
    "    for episode in range(1, num_episodes+1):\n",
    "        # Training episode\n",
    "        total_reward, log_probs, rewards = run_episode(env, agent, training=True)\n",
    "        update_policy(agent, log_probs, rewards)\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Periodic evaluation\n",
    "        if episode % test_every == 0:\n",
    "            test_reward = np.mean([run_episode(env, agent, training=False) for _ in range(num_tests)])\n",
    "            test_rewards.append((episode, test_reward))\n",
    "            print(f\"Test Reward: {test_reward:.2f}\")\n",
    "        \n",
    "        # Periodic progress display\n",
    "        if episode % print_every == 0:\n",
    "            # Calculate moving average of rewards\n",
    "            window = min(100, len(episode_rewards))\n",
    "            avg_reward = np.mean(episode_rewards[-window:])\n",
    "            \n",
    "            print(f\"Episode {episode}/{num_episodes}, Avg Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_reward = np.mean([run_episode(env, agent, training=False) for _ in range(num_tests)])\n",
    "    print(f\"\\nTraining completed. Final average reward over {num_tests} episodes: {final_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards, test_rewards#, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train the agent. A good solution should achieve an average reward of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a REINFORCE agent\n",
    "agent = REINFORCE(state_dim, action_dim, hidden_dim=128, lr=0.001, gamma=0.99)\n",
    "rewards, test_rewards = train_agent(env, agent, num_episodes=1500, print_every=100, test_every=200, num_tests=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code can be used to generate a gif of your agent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os\n",
    "import shutil\n",
    "from IPython.display import display, Image\n",
    "\n",
    "\n",
    "def generate_gif(env, agent, n_frames=100, filename='animation', display_result=True, fps=50, image_folder='frames_temp'):\n",
    "    frames = []\n",
    "    os.makedirs(image_folder, exist_ok=True)\n",
    "\n",
    "    env = NoWindowWrapper(env)\n",
    "    observation = env.reset()[0]\n",
    "    total_reward = 0\n",
    "    for i in range(n_frames):  \n",
    "        action = agent.act(observation, greedy=True)[0]\n",
    "        next_observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        frame = env.render()\n",
    "        observation = next_observation\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Save the frame as an image\n",
    "        image_path = f\"{image_folder}/frame_{i:04d}.png\"\n",
    "        imageio.imwrite(image_path, frame)\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Total reward: {total_reward}\")\n",
    "            observation = env.reset()[0]\n",
    "            total_reward = 0\n",
    "\n",
    "    # Create a GIF from the saved frames\n",
    "    image_files = [f\"{image_folder}/frame_{i:04d}.png\" for i in range(len(frames))]\n",
    "    images = [imageio.v2.imread(image_file) for image_file in image_files]\n",
    "    duration = 1.0 / fps\n",
    "    imageio.mimsave(filename + '.gif', images, duration=duration)\n",
    "    shutil.rmtree(image_folder)\n",
    "    if display_result:\n",
    "        display(Image(filename='animation.gif'))\n",
    "\n",
    "\n",
    "generate_gif(env, agent, n_frames=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
